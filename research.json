[
    {
        "name": "3D Ken Burns",
        "description": "A reference implementation of 3D Ken Burns Effect from a Single Image using PyTorch - given a single input image, it animates this still image with a virtual camera scan and zoom subject to motion parallax",
        "author": [
            [
                "Manuel Romero",
                "https://mrm8488.github.io/"
            ]
        ],
        "links": [
            [
                "arxiv",
                "https://arxiv.org/abs/1909.05483"
            ],
            [
                "yt",
                "https://www.youtube.com/watch?v=WrajxHHfRBA"
            ],
            [
                "git",
                "https://github.com/sniklaus/3d-ken-burns",
                1523
            ],
            [
                "doi",
                "https://doi.org/10.1145/3355089.3356528",
                134
            ]
        ],
        "colab": "https://colab.research.google.com/github/mrm8488/shared_colab_notebooks/blob/master/3D_Ken_Burns.ipynb",
        "update": 1706130183.0
    },
    {
        "name": "Learning to Paint",
        "description": "Learning to Paint With Model-based Deep Reinforcement Learning",
        "author": [
            [
                "Manuel Romero",
                "https://mrm8488.github.io/"
            ]
        ],
        "links": [
            [
                "arxiv",
                "https://arxiv.org/abs/1903.04411"
            ],
            [
                "yt",
                "https://www.youtube.com/watch?v=YmOgKZ5oipk"
            ],
            [
                "reddit",
                "https://www.reddit.com/r/reinforcementlearning/comments/b5lpfl/learning_to_paint_with_modelbased_deep/"
            ],
            [
                "doi",
                "https://doi.org/10.1109/ICCV.2019.00880",
                89
            ]
        ],
        "colab": "https://colab.research.google.com/github/mrm8488/shared_colab_notebooks/blob/master/custom_learningtopaint.ipynb",
        "update": 1675243948.0
    },
    {
        "name": "StyleGAN 2",
        "description": "Generation of faces, cars, etc.",
        "author": [
            [
                "Mikael Christensen",
                "https://github.com/Syntopia"
            ]
        ],
        "links": [
            [
                "arxiv",
                "http://arxiv.org/abs/1912.04958"
            ],
            [
                "yt",
                "https://youtu.be/c-NJtV9Jvp0"
            ],
            [
                "git",
                "https://github.com/NVlabs/stylegan2",
                11006
            ],
            [
                "git",
                "https://github.com/NVlabs/ffhq-dataset"
            ],
            [
                "doi",
                "https://doi.org/10.1109/CVPR42600.2020.00813",
                3203
            ]
        ],
        "colab": "https://colab.research.google.com/drive/1ShgW6wohEFQtqs_znMna3dzrcVoABKIH",
        "update": 1636142903.17
    },
    {
        "name": "StyleGAN3",
        "description": "Alias-Free Generative Adversarial Networks",
        "author": [
            [
                "Tero Karras",
                "https://research.nvidia.com/person/tero-karras"
            ],
            [
                "Miika Aittala",
                "https://research.nvidia.com/person/Miika-Aittala"
            ],
            [
                "Samuli Laine",
                "https://research.nvidia.com/person/Samuli-Laine"
            ],
            [
                "Erik Härkönen",
                "https://github.com/harskish"
            ],
            [
                "Janne Hellsten",
                "https://research.nvidia.com/person/Janne-Hellsten"
            ],
            [
                "Jaakko Lehtinen",
                "https://users.aalto.fi/~lehtinj7/"
            ],
            [
                "Timo Aila",
                "https://research.nvidia.com/person/timo-aila"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/NVlabs/stylegan3",
                6467
            ],
            [
                "git",
                "https://github.com/NVlabs/stylegan3-detector"
            ],
            [
                "git",
                "https://github.com/NVlabs/ffhq-dataset"
            ],
            [
                "git",
                "https://github.com/NVlabs/metfaces-dataset"
            ],
            [
                "git",
                "https://github.com/NVlabs/stylegan2-ada-pytorch"
            ],
            [
                "git",
                "https://github.com/NVlabs/stylegan2-ada"
            ],
            [
                "project",
                "https://nvlabs.github.io/stylegan3"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2106.12423"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/1706.08500"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/1801.01401"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/1904.06991"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/1812.04948"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/1606.03498"
            ],
            [
                "neurips",
                "https://proceedings.neurips.cc/paper/2021/hash/076ccd93ad68be51f23707988e934906-Abstract.html"
            ]
        ],
        "colab": "https://colab.research.google.com/drive/1BXNHZBai-pXtP-ncliouXo_kUiG1Pq7M",
        "update": 1691893297.85
    },
    {
        "name": "Neuralangelo",
        "description": "Framework for high-fidelity 3D surface reconstruction from RGB video captures",
        "author": [
            [
                "Zhaoshuo Li",
                "https://mli0603.github.io/"
            ],
            [
                "Thomas Müller",
                "https://tom94.net/"
            ],
            [
                "Alex Evans",
                "https://scholar.google.com/citations?user=ToqGImkAAAAJ"
            ],
            [
                "Russell Taylor",
                "https://www.cs.jhu.edu/~rht/"
            ],
            [
                "Mathias Unberath",
                "https://mathiasunberath.github.io/"
            ],
            [
                "Ming-Yu Liu",
                "https://mingyuliu.net/"
            ],
            [
                "Chen-Hsuan Lin",
                "https://chenhsuanlin.bitbucket.io/"
            ]
        ],
        "links": [
            [
                "project",
                "https://research.nvidia.com/labs/dir/neuralangelo/"
            ],
            [
                "git",
                "https://github.com/NVlabs/neuralangelo",
                4411
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2306.03092"
            ],
            [
                "git",
                "https://github.com/mli0603/BlenderNeuralangelo"
            ],
            [
                "yt",
                "https://youtu.be/PQMNCXR-WF8"
            ],
            [
                "yt",
                "https://youtu.be/Qpdw3SW54kI"
            ],
            [
                "yt",
                "https://youtu.be/lC2uPDfaTcE"
            ],
            [
                "blog post",
                "https://blogs.nvidia.com/blog/2023/06/01/neuralangelo-ai-research-3d-reconstruction/"
            ]
        ],
        "colab": "https://colab.research.google.com/drive/1i16s8W_OV0Hd3-PIuo64JKDwwdOesgXQ",
        "update": 1725288096.999
    },
    {
        "name": "StyleGAN-Human",
        "description": "A Data-Centric Odyssey of Human Generation",
        "author": [
            [
                "Jianglin Fu",
                "https://github.com/arleneF"
            ],
            [
                "Shikai Li",
                "https://github.com/leeskyed"
            ],
            [
                "Yuming Jiang",
                "https://yumingj.github.io/"
            ],
            [
                "Kwan-Yee Lin",
                "https://kwanyeelin.github.io/"
            ],
            [
                "Chen Qian",
                "https://scholar.google.com/citations?user=AerkT0YAAAAJ"
            ],
            [
                "Chen Change Loy",
                "https://www.mmlab-ntu.com/person/ccloy/"
            ],
            [
                "Wayne Wu",
                "https://wywu.github.io/"
            ],
            [
                "Ziwei Liu",
                "https://liuziwei7.github.io/"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/stylegan-human/stylegan-human",
                1151
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2204.11823"
            ],
            [
                "project",
                "https://stylegan-human.github.io/"
            ],
            [
                "yt",
                "https://youtu.be/nIrb9hwsdcI"
            ],
            [
                "git",
                "https://github.com/NVlabs/stylegan"
            ],
            [
                "git",
                "https://github.com/NVlabs/stylegan2-ada-pytorch"
            ],
            [
                "git",
                "https://github.com/NVlabs/stylegan3"
            ],
            [
                "pwc",
                "https://paperswithcode.com/dataset/market-1501"
            ],
            [
                "yt",
                "https://youtu.be/86b49sCz0Gg"
            ],
            [
                "yt",
                "https://youtu.be/g3nmM6MdxwY"
            ],
            [
                "yt",
                "https://youtu.be/p2uwqh_SFL8"
            ],
            [
                "doi",
                "https://doi.org/10.1007/978-3-031-19787-1_1",
                52
            ]
        ],
        "colab": "https://colab.research.google.com/drive/1sgxoDM55iM07FS54vz9ALg1XckiYA2On",
        "update": 1660883751.06
    },
    {
        "name": "Text2Human",
        "description": "Text-driven controllable framework for a high-quality and diverse human generation",
        "author": [
            [
                "Yuming Jiang",
                "https://yumingj.github.io/"
            ],
            [
                "Shuai Yang",
                "https://williamyang1991.github.io/"
            ],
            [
                "Haonan Qiu",
                "http://haonanqiu.com/"
            ],
            [
                "Wayne Wu",
                "https://wywu.github.io/"
            ],
            [
                "Chen Change Loy",
                "https://www.mmlab-ntu.com/person/ccloy/"
            ],
            [
                "Ziwei Liu",
                "https://liuziwei7.github.io/"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/yumingj/Text2Human",
                831
            ],
            [
                "project",
                "https://yumingj.github.io/projects/Text2Human.html"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2205.15996"
            ],
            [
                "yt",
                "https://youtu.be/yKh4VORA_E0"
            ],
            [
                "hf",
                "https://huggingface.co/spaces/hysts/Text2Human"
            ],
            [
                "hf",
                "https://huggingface.co/spaces/CVPR/drawings-to-human"
            ],
            [
                "git",
                "https://github.com/yumingj/DeepFashion-MultiModal"
            ],
            [
                "doi",
                "https://doi.org/10.1145/3528223.3530104",
                62
            ],
            [
                "yt",
                "https://youtu.be/RV-g5BlH3Zg"
            ],
            [
                "git",
                "https://github.com/samb-t/unleashing-transformers"
            ]
        ],
        "colab": "https://colab.research.google.com/drive/1AVwbqLwMp_Gz3KTCgBTtnGVtXIlCZDPk",
        "update": 1656968318.321
    },
    {
        "name": "GPT-2",
        "description": "Retrain an advanced text generating neural network on any text dataset using gpt-2-simple!",
        "author": [
            [
                "Max Woolf",
                "https://minimaxir.com/"
            ]
        ],
        "links": [
            [
                "blog post",
                "https://minimaxir.com/2019/09/howto-gpt2/"
            ],
            [
                "git",
                "https://github.com/openai/gpt-2",
                22602
            ],
            [
                "git",
                "https://github.com/minimaxir/gpt-2-simple"
            ],
            [
                "blog post",
                "https://openai.com/research/better-language-models"
            ],
            [
                "reddit",
                "https://www.reddit.com/r/MachineLearning/comments/aqlzde/r_openai_better_language_models_and_their/"
            ]
        ],
        "colab": "https://colab.research.google.com/drive/1VLG8e7YSEwypxU-noRNhsv5dW4NfTGce",
        "update": 1634524182.04
    },
    {
        "name": "AMARETTO",
        "description": "Multiscale and multimodal inference of regulatory networks to identify cell circuits and their drivers shared and distinct within and across biological systems of human disease",
        "author": [
            [
                "Nathalie Pochet",
                "http://portals.broadinstitute.org/pochetlab/"
            ],
            [
                "Olivier Gevaert",
                "https://profiles.stanford.edu/olivier-gevaert"
            ],
            [
                "Mohsen Nabian",
                "https://github.com/monabiyan"
            ],
            [
                "Jayendra Shinde",
                "https://jayendrashinde91.github.io/"
            ],
            [
                "Celine Everaert",
                "http://www.crig.ugent.be/en/node/510"
            ],
            [
                "Thorin Tabor",
                "http://thorin.tabcreations.com/"
            ]
        ],
        "links": [
            [
                "project",
                "http://portals.broadinstitute.org/pochetlab/amaretto.html"
            ],
            [
                "git",
                "https://github.com/gevaertlab/AMARETTO",
                16
            ],
            [
                "bioconductor",
                "https://bioconductor.org/packages/release/bioc/html/AMARETTO.html"
            ]
        ],
        "colab": "https://colab.research.google.com/drive/1JfnRoNgTVX_7VEGAAmjGjwP_yX2tdDxs",
        "update": 1709126899.23
    },
    {
        "name": "Customizing a Transformer Encoder",
        "description": "We will learn how to customize the encoder to employ new network architectures",
        "author": [
            [
                "Chen Chen",
                "https://github.com/chenGitHuber"
            ]
        ],
        "links": [
            [
                "arxiv",
                "https://arxiv.org/abs/1706.03762"
            ],
            [
                "git",
                "https://github.com/tensorflow/models/tree/master/official/nlp/modeling",
                77243
            ],
            [
                "git",
                "https://github.com/tensorflow/models/blob/master/official/nlp/modeling/networks/encoder_scaffold.py"
            ]
        ],
        "colab": "https://colab.research.google.com/github/tensorflow/models/blob/master/official/colab/nlp/customize_encoder.ipynb",
        "update": 1655923595.0
    },
    {
        "name": "Fine-tuning a BERT",
        "description": "We will work through fine-tuning a BERT model using the tensorflow-models PIP package",
        "author": [
            [
                "Chen Chen",
                "https://github.com/chenGitHuber"
            ],
            [
                "Claire Yao",
                "https://github.com/claireyao-fen"
            ]
        ],
        "links": [
            [
                "tf",
                "https://tensorflow.org/hub"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/1810.04805"
            ],
            [
                "doi",
                "https://doi.org/10.18653/v1/N19-1423",
                2550
            ]
        ],
        "colab": "https://colab.research.google.com/github/tensorflow/models/blob/master/official/colab/fine_tuning_bert.ipynb",
        "update": 1621899237.0
    },
    {
        "name": "First Order Motion Model for Image Animation",
        "description": "Transferring facial movements from video to image",
        "author": [
            [
                "Aliaksandr Siarohin",
                "https://aliaksandrsiarohin.github.io/aliaksandr-siarohin-website/"
            ]
        ],
        "links": [
            [
                "neurips",
                "https://papers.nips.cc/paper/2019/hash/31c0b36aef265d9221af80872ceb62f9-Abstract.html"
            ],
            [
                "git",
                "https://github.com/AliaksandrSiarohin/first-order-model",
                14581
            ],
            [
                "project",
                "https://aliaksandrsiarohin.github.io/first-order-model-website/"
            ],
            [
                "yt",
                "https://www.youtube.com/watch?v=u-0cQ-grXBQ"
            ]
        ],
        "colab": "https://colab.research.google.com/github/AliaksandrSiarohin/first-order-model/blob/master/demo.ipynb",
        "update": 1685853413.0
    },
    {
        "name": "Motion Supervised co-part Segmentation",
        "description": "A self-supervised deep learning method for co-part segmentation",
        "author": [
            [
                "Aliaksandr Siarohin",
                "https://aliaksandrsiarohin.github.io/aliaksandr-siarohin-website/"
            ],
            [
                "Subhankar Roy",
                "https://github.com/roysubhankar"
            ]
        ],
        "links": [
            [
                "arxiv",
                "http://arxiv.org/abs/2004.03234"
            ],
            [
                "yt",
                "https://www.youtube.com/watch?v=RJ4Nj1wV5iA"
            ],
            [
                "git",
                "https://github.com/AliaksandrSiarohin/motion-cosegmentation",
                656
            ],
            [
                "git",
                "https://github.com/AliaksandrSiarohin/video-preprocessing"
            ],
            [
                "doi",
                "https://doi.org/10.1109/ICPR48806.2021.9412520",
                9
            ]
        ],
        "colab": "https://colab.research.google.com/github/AliaksandrSiarohin/motion-cosegmentation/blob/master/part_swap.ipynb",
        "update": 1586285732.0
    },
    {
        "name": "Motion Representations for Articulated Animation",
        "description": "Novel motion representations for animating articulated objects consisting of distinct parts",
        "author": [
            [
                "Aliaksandr Siarohin",
                "https://aliaksandrsiarohin.github.io/aliaksandr-siarohin-website/"
            ],
            [
                "Oliver Woodford",
                "https://ojwoodford.github.io/"
            ],
            [
                "Jian Ren",
                "https://alanspike.github.io/"
            ],
            [
                "Menglei Chai",
                "https://mlchai.com/"
            ],
            [
                "Sergey Tulyakov",
                "http://www.stulyakov.com/"
            ]
        ],
        "links": [
            [
                "project",
                "https://snap-research.github.io/articulated-animation/"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2104.11280"
            ],
            [
                "yt",
                "https://www.youtube.com/watch?v=gpBYN8t8_yY"
            ],
            [
                "git",
                "https://github.com/snap-research/articulated-animation",
                1240
            ],
            [
                "doi",
                "https://doi.org/10.1109/CVPR46437.2021.01344",
                110
            ]
        ],
        "colab": "https://colab.research.google.com/github/AliaksandrSiarohin/articulated-animation/blob/master/demo.ipynb",
        "update": 1619720460.0
    },
    {
        "name": "DeOldify (video)",
        "description": "Colorize your own videos!",
        "author": [
            [
                "Jason Antic",
                "https://github.com/jantic"
            ]
        ],
        "links": [
            [
                "arxiv",
                "https://arxiv.org/abs/1805.08318"
            ],
            [
                "git",
                "https://github.com/jantic/DeOldify",
                18053
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/1706.08500"
            ],
            [
                "reddit",
                "https://www.reddit.com/r/Nickelodeons/"
            ],
            [
                "reddit",
                "https://www.reddit.com/r/silentmoviegifs/"
            ],
            [
                "yt",
                "http://www.youtube.com/watch?v=l3UXXid04Ys"
            ],
            [
                "yt",
                "http://www.youtube.com/watch?v=EXn-n2iqEjI"
            ],
            [
                "model",
                "https://data.deepai.org/deoldify/ColorizeVideo_gen.pth"
            ],
            [
                "medium",
                "https://medium.com/element-ai-research-lab/stabilizing-neural-style-transfer-for-video-62675e203e42"
            ],
            [
                "twitter",
                "https://twitter.com/DeOldify"
            ],
            [
                "website",
                "https://deoldify.ai/"
            ]
        ],
        "colab": "https://colab.research.google.com/github/jantic/DeOldify/blob/master/VideoColorizerColab.ipynb",
        "update": 1663580069.0
    },
    {
        "name": "DeOldify (photo)",
        "description": "Colorize your own photos!",
        "author": [
            [
                "Jason Antic",
                "https://github.com/jantic"
            ],
            [
                "Matt Robinson",
                "https://github.com/mc-robinson"
            ],
            [
                "María Benavente",
                "https://github.com/mariabg"
            ]
        ],
        "links": [
            [
                "arxiv",
                "https://arxiv.org/abs/1805.08318"
            ],
            [
                "git",
                "https://github.com/jantic/DeOldify",
                18053
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/1706.08500"
            ],
            [
                "reddit",
                "https://www.reddit.com/r/TheWayWeWere/"
            ],
            [
                "model",
                "https://data.deepai.org/deoldify/ColorizeArtistic_gen.pth"
            ],
            [
                "twitter",
                "https://twitter.com/DeOldify"
            ],
            [
                "website",
                "https://deoldify.ai/"
            ]
        ],
        "colab": "https://colab.research.google.com/github/jantic/DeOldify/blob/master/ImageColorizerColab.ipynb",
        "update": 1663580069.0
    },
    {
        "name": "T5",
        "description": "Text-To-Text Transfer Transformer",
        "author": [
            [
                "Colin Raffel",
                "https://colinraffel.com/"
            ],
            [
                "Noam Shazeer",
                "https://scholar.google.com/citations?user=wsGvgA8AAAAJ"
            ],
            [
                "Adam Roberts",
                "https://github.com/adarob"
            ],
            [
                "Katherine Lee",
                "https://github.com/katelee168"
            ],
            [
                "Sharan Narang",
                "https://github.com/sharannarang"
            ],
            [
                "Michael Matena",
                "https://scholar.google.com/citations?user=rN_9vroAAAAJ"
            ],
            [
                "Yanqi Zhou",
                "https://zhouyanqi.github.io"
            ],
            [
                "Wei Li",
                "https://research.google/people/106528/"
            ],
            [
                "Peter J. Liu",
                "https://scholar.google.com/citations?user=1EPxhywAAAAJ"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/google-research/text-to-text-transfer-transformer",
                6199
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/1910.10683"
            ],
            [
                "tf",
                "https://www.tensorflow.org/datasets"
            ],
            [
                "git",
                "https://github.com/tensorflow/mesh/tree/master/mesh_tensorflow/transformer"
            ]
        ],
        "colab": "https://colab.research.google.com/github/google-research/text-to-text-transfer-transformer/blob/main/notebooks/t5-trivia.ipynb",
        "update": 1652301661.0
    },
    {
        "name": "T5X",
        "description": "Modular, composable, research-friendly framework for high-performance, configurable, self-service training, evaluation, and inference of sequence models at many scales",
        "author": [
            [
                "Adam Roberts",
                "https://github.com/adarob"
            ],
            [
                "Hyung Won Chung",
                "https://github.com/hwchung27"
            ],
            [
                "Anselm Levskaya",
                "https://anselmlevskaya.com/"
            ],
            [
                "Gaurav Mishra",
                "https://research.google/people/GauravMishra/"
            ],
            [
                "James Bradbury",
                "https://github.com/jekbradbury"
            ],
            [
                "Daniel Andor",
                "https://github.com/andorardo"
            ],
            [
                "Sharan Narang",
                "https://github.com/sharannarang"
            ],
            [
                "Brian Lester",
                "https://blester125.com/"
            ],
            [
                "Colin Gaffney",
                "https://github.com/cpgaffney1"
            ],
            [
                "Afroz Mohiuddin",
                "https://github.com/afrozenator"
            ],
            [
                "Curtis Hawthorne",
                "https://github.com/cghawthorne"
            ],
            [
                "Aitor Lewkowycz",
                "https://scholar.google.com/citations?user=Yum1ah0AAAAJ"
            ],
            [
                "Alex Salcianu",
                "https://scholar.google.com/citations?user=HSrT1wsAAAAJ"
            ],
            [
                "Marc van Zee",
                "https://github.com/marcvanzee"
            ],
            [
                "Jacob Austin",
                "https://jacobaustin123.github.io/"
            ],
            [
                "Sebastian Goodman",
                "https://github.com/0x0539"
            ],
            [
                "Livio Baldini Soares",
                "https://liviosoares.github.io/"
            ],
            [
                "Haitang Hu",
                "https://hthu.github.io/"
            ],
            [
                "Sasha Tsvyashchenko",
                "https://endl.ch/"
            ],
            [
                "Aakanksha Chowdhery",
                "http://www.achowdhery.com/"
            ],
            [
                "Jasmijn Bastings",
                "https://jasmijn.ninja/"
            ],
            [
                "Jannis Bulian",
                "http://bulian.org/"
            ],
            [
                "Xavier Garcia",
                "https://scholar.google.com/citations?user=Y2Hio6MAAAAJ"
            ],
            [
                "Jianmo Ni",
                "https://nijianmo.github.io/"
            ],
            [
                "Kathleen Kenealy",
                "https://scholar.google.com/citations?&user=HgRBC5gAAAAJ"
            ],
            [
                "Jonathan Clark",
                "http://www.cs.cmu.edu/~jhclark/"
            ],
            [
                "Dan Garrette",
                "http://www.dhgarrette.com/"
            ],
            [
                "James Lee-Thorp",
                "https://scholar.google.com/citations?user=qsPv098AAAAJ"
            ],
            [
                "Colin Raffel",
                "https://colinraffel.com/"
            ],
            [
                "Noam Shazeer",
                "https://scholar.google.com/citations?user=wsGvgA8AAAAJ"
            ],
            [
                "Marvin Ritter",
                "https://scholar.google.com/citations?user=arcf5FgAAAAJ"
            ],
            [
                "Maarten Bosma",
                "https://scholar.google.com/citations?user=wkeFQPgAAAAJ"
            ],
            [
                "Alexandre Passos",
                "https://www.ic.unicamp.br/~tachard/"
            ],
            [
                "Jeremy Maitin-Shepard",
                "https://research.google/people/JeremyMaitinShepard/"
            ],
            [
                "Noah Fiedel",
                "https://scholar.google.com/citations?user=XWpV9DsAAAAJ"
            ],
            [
                "Brennan Saeta",
                "https://github.com/saeta"
            ],
            [
                "Ryan Sepassi",
                "https://ryansepassi.com/"
            ],
            [
                "Alexander Spiridonov",
                "https://research.google/people/AlexanderSpiridonov/"
            ],
            [
                "Joshua Newlan",
                "https://github.com/joshnewlan"
            ],
            [
                "Andrea Gesmundo",
                "https://github.com/agesmundo"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/google-research/t5x",
                2703
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2203.17189"
            ],
            [
                "docs",
                "https://t5x.readthedocs.io/en/latest/"
            ],
            [
                "git",
                "https://github.com/tensorflow/mesh"
            ],
            [
                "tf",
                "https://www.tensorflow.org/datasets/catalog/wmt_t2t_translate"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/1910.10683"
            ],
            [
                "tf",
                "https://www.tensorflow.org/guide/data"
            ],
            [
                "tf",
                "https://www.tensorflow.org/tensorboard"
            ],
            [
                "git",
                "https://github.com/tensorflow/serving"
            ]
        ],
        "colab": "https://colab.research.google.com/github/google-research/t5x/blob/main/t5x/notebooks/introduction.ipynb",
        "update": 1687830221.0
    },
    {
        "name": "Dream Fields",
        "description": "Zero-Shot Text-Guided Object Generation",
        "author": [
            [
                "Ajay Jain",
                "https://ajayj.com/"
            ],
            [
                "Ben Mildenhall",
                "https://bmild.github.io/"
            ],
            [
                "Jon Barron",
                "https://jonbarron.info/"
            ],
            [
                "Pieter Abbeel",
                "https://people.eecs.berkeley.edu/~pabbeel/"
            ],
            [
                "Ben Poole",
                "https://cs.stanford.edu/~poole/"
            ]
        ],
        "links": [
            [
                "project",
                "https://ajayj.com/dreamfields"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2112.01455"
            ],
            [
                "git",
                "https://github.com/google-research/google-research/tree/master/dreamfields",
                34429
            ],
            [
                "yt",
                "https://youtu.be/1Fke6w46tv4"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2104.00677"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2103.13415"
            ],
            [
                "git",
                "https://github.com/ajayjain/DietNeRF"
            ],
            [
                "git",
                "https://github.com/google/mipnerf"
            ],
            [
                "doi",
                "https://doi.org/10.1109/CVPR52688.2022.00094",
                216
            ]
        ],
        "colab": "https://colab.research.google.com/drive/17GtPqdUCbG5CsmTnQFecPpoq_zpNKX7A",
        "update": 1654875664.27
    },
    {
        "name": "ViT",
        "description": "Vision Transformer and MLP-Mixer Architectures",
        "author": [
            [
                "Alexey Dosovitskiy",
                "https://scholar.google.com/citations?user=FXNJRDoAAAAJ"
            ],
            [
                "Lucas Beyer",
                "http://lucasb.eyer.be"
            ],
            [
                "Alexander Kolesnikov",
                "https://github.com/akolesnikoff"
            ],
            [
                "Dirk Weissenborn",
                "https://github.com/dirkweissenborn"
            ],
            [
                "Xiaohua Zhai",
                "https://github.com/xiaohuazhai"
            ],
            [
                "Thomas Unterthiner",
                "https://github.com/untom"
            ],
            [
                "Mostafa Dehghani",
                "https://www.mostafadehghani.com/"
            ],
            [
                "Matthias Minderer",
                "https://matthias.minderer.net/"
            ],
            [
                "Georg Heigold",
                "https://scholar.google.com/citations?user=WwqlChAAAAAJ"
            ],
            [
                "Sylvain Gelly",
                "https://scholar.google.com/citations?user=m7LvuTkAAAAJ"
            ],
            [
                "Jakob Uszkoreit",
                "https://scholar.google.com/citations?user=mOG0bwsAAAAJ"
            ],
            [
                "Neil Houlsby",
                "https://neilhoulsby.github.io/"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/google-research/vision_transformer",
                10580
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2010.11929"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2105.01601"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2105.01601"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2106.10270"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2106.01548"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2111.07991"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2203.08065"
            ],
            [
                "git",
                "https://github.com/huggingface/pytorch-image-models"
            ],
            [
                "kaggle",
                "https://www.kaggle.com/models"
            ],
            [
                "git",
                "https://github.com/google/flaxformer"
            ],
            [
                "yt",
                "https://youtu.be/TrdevFK_am4"
            ],
            [
                "yt",
                "https://youtu.be/HZ4j_U3FC94"
            ],
            [
                "yt",
                "https://youtu.be/7K4Z8RqjWIk"
            ],
            [
                "yt",
                "https://youtu.be/oDtcobGQ7xU?si=C2EgZTESzhTXFSq6"
            ],
            [
                "yt",
                "https://youtu.be/v6xj_DG-UEo"
            ],
            [
                "medium",
                "https://medium.com/@weiwen21/an-image-is-worth-16x16-words-transformers-for-image-recognition-at-scale-957f88e53726"
            ],
            [
                "blog post",
                "https://blog.research.google/2022/04/locked-image-tuning-adding-language.html"
            ]
        ],
        "colab": "https://colab.research.google.com/github/google-research/vision_transformer/blob/main/vit_jax.ipynb",
        "update": 1707218861.0
    },
    {
        "name": "FILM",
        "description": "A frame interpolation algorithm that synthesizes multiple intermediate frames from two input images with large in-between motion",
        "author": [
            [
                "Fitsum Reda",
                "https://fitsumreda.github.io/"
            ],
            [
                "Janne Kontkanen",
                "https://scholar.google.com/citations?user=MnXc4JQAAAAJ"
            ],
            [
                "Eric Tabellion",
                "http://www.tabellion.org/et/"
            ],
            [
                "Deqing Sun",
                "https://deqings.github.io/"
            ],
            [
                "Caroline Pantofaru",
                "https://scholar.google.com/citations?user=vKAKE1gAAAAJ"
            ],
            [
                "Brian Curless",
                "https://homes.cs.washington.edu/~curless/"
            ]
        ],
        "links": [
            [
                "project",
                "https://film-net.github.io/"
            ],
            [
                "yt",
                "https://youtu.be/OAD-BieIjH4"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2202.04901"
            ],
            [
                "git",
                "https://github.com/google-research/frame-interpolation",
                2867
            ],
            [
                "data",
                "http://data.csail.mit.edu/tofu/testset/vimeo_interp_test.zip"
            ],
            [
                "data",
                "https://vision.middlebury.edu/flow/data"
            ],
            [
                "data",
                "https://people.cs.umass.edu/~hzjiang/projects/superslomo/UCF101_results.zip"
            ],
            [
                "git",
                "https://github.com/sniklaus/softmax-splatting/blob/master/benchmark.py"
            ],
            [
                "tf",
                "https://www.tensorflow.org/tutorials/load_data/tfrecord"
            ],
            [
                "tf",
                "https://www.tensorflow.org/api_docs/python/tf/train/Example"
            ],
            [
                "tf",
                "https://www.tensorflow.org/guide/saved_model"
            ],
            [
                "doi",
                "https://doi.org/10.1007/978-3-031-20071-7_15",
                57
            ]
        ],
        "colab": "https://colab.research.google.com/drive/1sK0uc-GJxmdnaxHhYqD2afRknakpdTNZ",
        "update": 1714699976.07
    },
    {
        "name": "OWL-ViT",
        "description": "Simple Open-Vocabulary Object Detection with Vision Transformers",
        "author": [
            [
                "Matthias Minderer",
                "http://matthias.minderer.net/"
            ],
            [
                "Alexey Gritsenko",
                "https://github.com/AlexeyG"
            ],
            [
                "Austin Stone",
                "https://github.com/AustinCStone"
            ],
            [
                "Maxim Neumann",
                "https://github.com/maximneumann"
            ],
            [
                "Dirk Weissenborn",
                "https://github.com/dirkweissenborn"
            ],
            [
                "Alexey Dosovitskiy",
                "https://scholar.google.com/citations?user=FXNJRDoAAAAJ"
            ],
            [
                "Aravindh Mahendran",
                "https://github.com/aravindhm"
            ],
            [
                "Anurag Arnab",
                "https://github.com/anuragarnab"
            ],
            [
                "Mostafa Dehghani",
                "https://mostafadehghani.com/"
            ],
            [
                "Zhuoran Shen",
                "https://cmsflash.github.io/"
            ],
            [
                "Xiao Wang",
                "https://scholar.google.com/citations?user=ukyXqzMAAAAJ"
            ],
            [
                "Xiaohua Zhai",
                "https://github.com/xiaohuazhai"
            ],
            [
                "Thomas Kipf",
                "https://tkipf.github.io/"
            ],
            [
                "Neil Houlsby",
                "https://neilhoulsby.github.io/"
            ]
        ],
        "links": [
            [
                "arxiv",
                "https://arxiv.org/abs/2205.06230"
            ],
            [
                "hf",
                "https://huggingface.co/docs/transformers/model_doc/owlvit"
            ],
            [
                "doi",
                "https://doi.org/10.1007/978-3-031-20080-9_42",
                87
            ]
        ],
        "colab": "https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/zeroshot_object_detection_with_owlvit.ipynb",
        "update": 1692628501.0
    },
    {
        "name": "BiT",
        "description": "Big Transfer: General Visual Representation Learning",
        "author": [
            [
                "Alexander Kolesnikov",
                "https://github.com/akolesnikoff"
            ],
            [
                "Lucas Beyer",
                "http://lucasb.eyer.be"
            ],
            [
                "Xiaohua Zhai",
                "https://github.com/xiaohuazhai"
            ],
            [
                "Joan Puigcerver",
                "https://www.jpuigcerver.net/"
            ],
            [
                "Jessica Yung",
                "https://github.com/jessicayung"
            ],
            [
                "Sylvain Gelly",
                "https://scholar.google.com/citations?user=m7LvuTkAAAAJ"
            ],
            [
                "Neil Houlsby",
                "https://neilhoulsby.github.io/"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/google-research/big_transfer",
                1517
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/1912.11370"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2106.05237"
            ],
            [
                "doi",
                "https://doi.org/10.1007/978-3-030-58558-7_29",
                366
            ],
            [
                "yt",
                "https://youtu.be/k1GOF2jmX7c"
            ],
            [
                "yt",
                "https://youtu.be/0iTgt5-SOsU"
            ],
            [
                "yt",
                "https://youtu.be/X5Rhm__OxvA"
            ],
            [
                "medium",
                "https://sh-tsang.medium.com/review-big-transfer-bit-general-visual-representation-learning-cb4bf8ed9732"
            ],
            [
                "hf",
                "https://huggingface.co/google/bit-50"
            ]
        ],
        "colab": "https://colab.research.google.com/github/google-research/big_transfer/blob/master/colabs/big_transfer_tf2.ipynb",
        "update": 1605193450.0
    },
    {
        "name": "VITS",
        "description": "Parallel end-to-end TTS method that generates more natural sounding audio than current two-stage models",
        "author": [
            [
                "Jaehyeon Kim",
                "https://jaywalnut310.github.io/"
            ],
            [
                "Jungil Kong",
                "https://github.com/jik876"
            ],
            [
                "Juhee Son",
                "https://juheeuu.github.io/"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/jaywalnut310/vits",
                6923
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2106.06103"
            ],
            [
                "demo",
                "https://jaywalnut310.github.io/vits-demo/"
            ]
        ],
        "colab": "https://colab.research.google.com/drive/1CO61pZizDj7en71NQG_aqqKdGaA_SaBf",
        "update": 1629695429.206
    },
    {
        "name": "StylEx",
        "description": "Training a GAN to explain a classifier in StyleSpace",
        "author": [
            [
                "Oran Lang",
                "https://research.google/people/105975/"
            ],
            [
                "Yossi Gandelsman",
                "https://yossigandelsman.github.io/"
            ],
            [
                "Michal Yarom",
                "https://scholar.google.com/citations?user=GMVxiYgAAAAJ"
            ],
            [
                "Yoav Wald",
                "https://scholar.google.com/citations?user=hh5nOn4AAAAJ"
            ],
            [
                "Gal Elidan",
                "https://research.google/people/105719/"
            ],
            [
                "Avinatan Hassidim",
                "https://research.google/people/105831/"
            ],
            [
                "William Freeman",
                "https://billf.mit.edu/"
            ],
            [
                "Phillip Isola",
                "http://web.mit.edu/phillipi/"
            ],
            [
                "Amir Globerso",
                "https://cs3801.wixsite.com/amirgloberson"
            ],
            [
                "Michal Irani",
                "http://www.weizmann.ac.il/math/irani/"
            ],
            [
                "Inbar Mosseri",
                "https://research.google/people/InbarMosseri/"
            ]
        ],
        "links": [
            [
                "project",
                "https://explaining-in-style.github.io/"
            ],
            [
                "git",
                "https://github.com/google/explaining-in-style",
                220
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2104.13369"
            ],
            [
                "blog post",
                "https://ai.googleblog.com/2022/01/introducing-stylex-new-approach-for.html"
            ],
            [
                "yt",
                "https://youtu.be/wLk2eBdXH4M"
            ],
            [
                "supplementary",
                "https://explaining-in-style.github.io/supmat.html"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/1906.10112"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2011.12799"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/1912.04958"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/1710.01711"
            ],
            [
                "doi",
                "https://doi.org/10.1109/ICCV48922.2021.00073",
                69
            ]
        ],
        "colab": "https://colab.research.google.com/github/google/explaining-in-style/blob/main/Explaining_in_Style_AttFind.ipynb",
        "update": 1629914366.0
    },
    {
        "name": "Nerfies",
        "description": "First method capable of photorealistically reconstructing deformable scenes using photos/videos captured casually from mobile phones",
        "author": [
            [
                "Keunhong Park",
                "https://keunhong.com/"
            ],
            [
                "Utkarsh Sinha",
                "https://utkarshsinha.com/"
            ],
            [
                "Jon Barron",
                "https://jonbarron.info/"
            ],
            [
                "Sofien Bouaziz",
                "http://sofienbouaziz.com/"
            ],
            [
                "Dan Goldman",
                "https://www.danbgoldman.com/home/"
            ],
            [
                "Steve Seitz",
                "https://www.smseitz.com/"
            ],
            [
                "Ricardo Martin-Brualla",
                "https://ricardomartinbrualla.com/"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/google/nerfies",
                1698
            ],
            [
                "project",
                "https://nerfies.github.io/"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2011.12948"
            ],
            [
                "doi",
                "https://doi.org/10.1109/ICCV48922.2021.00581",
                560
            ],
            [
                "yt",
                "https://youtu.be/MrKrnHhk8IA"
            ],
            [
                "git",
                "https://github.com/google-research/google-research/tree/master/jaxnerf"
            ],
            [
                "yt",
                "https://youtu.be/IDMiMKWucaI"
            ],
            [
                "reddit",
                "https://www.reddit.com/r/photogrammetry/comments/k1i0ct/deformable_neural_radiance_fields_nerfies/"
            ]
        ],
        "colab": "https://colab.research.google.com/github/google/nerfies/blob/main/notebooks/Nerfies_Capture_Processing.ipynb",
        "update": 1638751158.0
    },
    {
        "name": "PIFu",
        "description": "Pixel-Aligned Implicit Function for High-Resolution Clothed Human Digitization",
        "author": [
            [
                "Ryota Natsume",
                "https://github.com/nanopoteto"
            ],
            [
                "Shunsuke Saito",
                "https://shunsukesaito.github.io/"
            ],
            [
                "Zeng Huang",
                "https://zeng.science/"
            ],
            [
                "Angjoo Kanazawa",
                "https://people.eecs.berkeley.edu/~kanazawa/"
            ],
            [
                "Hao Li",
                "http://hao.li"
            ]
        ],
        "links": [
            [
                "arxiv",
                "https://arxiv.org/abs/1905.05172"
            ],
            [
                "git",
                "https://github.com/shunsukesaito/PIFu",
                1778
            ],
            [
                "yt",
                "https://www.youtube.com/watch?v=S1FpjwKqtPs"
            ],
            [
                "doi",
                "https://doi.org/10.1109/ICCV.2019.00239",
                796
            ]
        ],
        "colab": "https://colab.research.google.com/drive/1GFSsqP2BWz4gtq0e-nki00ZHSirXwFyY",
        "update": 1728340911.739
    },
    {
        "name": "PIFuHD",
        "description": "Multi-Level Pixel-Aligned Implicit Function for High-Resolution 3D Human Digitization",
        "author": [
            [
                "Shunsuke Saito",
                "https://shunsukesaito.github.io/"
            ],
            [
                "Tomas Simon",
                "http://www.cs.cmu.edu/~tsimon/"
            ],
            [
                "Jason Saragih",
                "https://scholar.google.com/citations?user=ss-IvjMAAAAJ"
            ],
            [
                "Hanbyul Joo",
                "https://jhugestar.github.io/"
            ]
        ],
        "links": [
            [
                "arxiv",
                "https://arxiv.org/abs/2004.00452"
            ],
            [
                "git",
                "https://github.com/facebookresearch/pifuhd",
                9549
            ],
            [
                "yt",
                "https://youtu.be/uEDqCxvF5yc"
            ],
            [
                "yt",
                "https://www.youtube.com/watch?v=8qnwbbDS8xk"
            ],
            [
                "doi",
                "https://doi.org/10.1109/CVPR42600.2020.00016",
                493
            ]
        ],
        "colab": "https://colab.research.google.com/drive/11z58bl3meSzo6kFqkahMa35G5jmh2Wgt",
        "update": 1679843559.379
    },
    {
        "name": "IC-GAN",
        "description": "Instance-Conditioned GAN",
        "author": [
            [
                "Arantxa Casanova",
                "https://github.com/ArantxaCasanova"
            ],
            [
                "Marlène Careil",
                "https://www.linkedin.com/in/marl%C3%A8ne-careil-901804155"
            ],
            [
                "Jakob Verbeek",
                "http://thoth.inrialpes.fr/~verbeek/"
            ],
            [
                "Michał Drożdżal",
                "https://scholar.google.com/citations?user=XK_ktwQAAAAJ"
            ],
            [
                "Adriana Romero-Soriano",
                "https://sites.google.com/site/adriromsor"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/facebookresearch/ic_gan",
                535
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2109.05070"
            ],
            [
                "git",
                "https://github.com/facebookresearch/faiss"
            ],
            [
                "git",
                "https://github.com/ajbrock/BigGAN-PyTorch"
            ],
            [
                "git",
                "https://github.com/NVlabs/stylegan2-ada-pytorch"
            ],
            [
                "git",
                "https://github.com/bioinf-jku/TTUR"
            ],
            [
                "git",
                "https://github.com/mit-han-lab/data-efficient-gans"
            ],
            [
                "blog post",
                "https://ai.facebook.com/blog/instance-conditioned-gans/"
            ],
            [
                "neurips",
                "https://proceedings.neurips.cc/paper/2021/hash/e7ac288b0f2d41445904d071ba37aaff-Abstract.html"
            ]
        ],
        "colab": "https://colab.research.google.com/github/facebookresearch/ic_gan/blob/master/inference/icgan_colab.ipynb",
        "update": 1633095150.0
    },
    {
        "name": "OPT",
        "description": "Open Pre-trained Transformers is a family of NLP models trained on billions of tokens of text obtained from the internet",
        "author": [
            [
                "Susan Zhang",
                "https://github.com/suchenzang"
            ],
            [
                "Stephen Roller",
                "https://stephenroller.com/"
            ],
            [
                "Naman Goyal",
                "https://github.com/ngoyal2707"
            ],
            [
                "Mikel Artetxe",
                "https://github.com/artetxem"
            ],
            [
                "Moya Chen",
                "https://moyachen.com/"
            ],
            [
                "Christopher Dewan",
                "https://github.com/m3rlin45"
            ],
            [
                "Mona Diab",
                "https://scholar.google.com/citations?user=-y6SIhQAAAAJ"
            ],
            [
                "Xi Victoria Lin",
                "http://victorialin.net/"
            ],
            [
                "Todor Mihaylov",
                "https://github.com/tbmihailov"
            ],
            [
                "Myle Ott",
                "https://myleott.com/"
            ],
            [
                "Sam Shleifer",
                "https://github.com/sshleifer"
            ],
            [
                "Kurt Shuster",
                "https://github.com/klshuster"
            ],
            [
                "Daniel Simig",
                "https://scholar.google.com/citations?user=TtWU9fsAAAAJ"
            ],
            [
                "Punit Singh Koura",
                "https://github.com/punitkoura"
            ],
            [
                "Anjali Sridhar",
                "https://www.linkedin.com/in/anjalisridhar/"
            ],
            [
                "Tianlu Wang",
                "https://tianlu-wang.github.io/"
            ],
            [
                "Luke Zettlemoyer",
                "https://www.cs.washington.edu/people/faculty/lsz/"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/facebookresearch/metaseq/tree/main/projects/OPT",
                6518
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2205.01068"
            ],
            [
                "blog post",
                "https://ai.facebook.com/blog/democratizing-access-to-large-scale-language-models-with-opt-175b/"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/1906.02243"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2104.10350"
            ],
            [
                "git",
                "https://github.com/NVIDIA/Megatron-LM"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2201.11990"
            ],
            [
                "yt",
                "https://youtu.be/Ejg0OunCi9U"
            ]
        ],
        "colab": "https://colab.research.google.com/drive/14wnxMvD9zsiBQo2FtTpxn6w2cpXCcb-7",
        "update": 1656492993.669
    },
    {
        "name": "Detic",
        "description": "Detecting Twenty-thousand Classes using Image-level Supervision",
        "author": [
            [
                "Xingyi Zhou",
                "https://www.cs.utexas.edu/~zhouxy/"
            ],
            [
                "Rohit Girdhar",
                "https://rohitgirdhar.github.io/"
            ],
            [
                "Armand Joulin",
                "https://ai.facebook.com/people/armand-joulin/"
            ],
            [
                "Philipp Krähenbühl",
                "https://github.com/philkr"
            ],
            [
                "Ishan Misra",
                "https://imisra.github.io/"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/facebookresearch/Detic",
                1889
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2201.02605"
            ],
            [
                "git",
                "https://github.com/lvis-dataset/lvis-api"
            ],
            [
                "doi",
                "https://doi.org/10.1007/978-3-031-20077-9_21",
                209
            ]
        ],
        "colab": "https://colab.research.google.com/drive/1QtTW9-ukX2HKZGvt0QvVGqjuqEykoZKI",
        "update": 1654580960.311
    },
    {
        "name": "Mask2Former",
        "description": "Masked-attention Mask Transformer for Universal Image Segmentation",
        "author": [
            [
                "Bowen Cheng",
                "https://bowenc0221.github.io/"
            ],
            [
                "Ishan Misra",
                "https://imisra.github.io/"
            ],
            [
                "Alexander Schwing",
                "https://alexander-schwing.de/"
            ],
            [
                "Alexander Kirillov",
                "https://alexander-kirillov.github.io/"
            ],
            [
                "Rohit Girdhar",
                "https://rohitgirdhar.github.io/"
            ]
        ],
        "links": [
            [
                "arxiv",
                "https://arxiv.org/abs/2112.01527"
            ],
            [
                "project",
                "https://bowenc0221.github.io/mask2former/"
            ],
            [
                "git",
                "https://github.com/facebookresearch/Mask2Former",
                2600
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2112.10764"
            ],
            [
                "demo",
                "https://replicate.com/facebookresearch/mask2former"
            ],
            [
                "git",
                "https://github.com/facebookresearch/MaskFormer"
            ],
            [
                "hf",
                "https://huggingface.co/spaces/akhaliq/Mask2Former"
            ],
            [
                "doi",
                "https://doi.org/10.1109/CVPR52688.2022.00135",
                1029
            ]
        ],
        "colab": "https://colab.research.google.com/drive/1uIWE5KbGFSjrxey2aRd5pWkKNY1_SaNq",
        "update": 1644421484.286
    },
    {
        "name": "Demucs",
        "description": "Hybrid Spectrogram and Waveform Source Separation",
        "author": [
            [
                "Alexandre Défossez",
                "https://ai.honu.io/"
            ]
        ],
        "links": [
            [
                "arxiv",
                "https://arxiv.org/abs/2111.03600"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2010.01733"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2109.05418"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/1805.02410"
            ],
            [
                "git",
                "https://github.com/facebookresearch/demucs",
                8416
            ],
            [
                "git",
                "https://github.com/adefossez/mdx21_demucs"
            ],
            [
                "git",
                "https://github.com/CarlGao4/Demucs-Gui"
            ],
            [
                "git",
                "https://github.com/kuielab/mdx-net-submission"
            ],
            [
                "git",
                "https://github.com/f90/Wave-U-Net"
            ]
        ],
        "colab": "https://colab.research.google.com/drive/1dC9nVxk3V_VPjUADsnFu8EiT-xnU1tGH",
        "update": 1669043551.684
    },
    {
        "name": "ConvNeXt",
        "description": "A pure ConvNet model constructed entirely from standard ConvNet modules",
        "author": [
            [
                "Zhuang Liu",
                "https://liuzhuang13.github.io/"
            ],
            [
                "Hanzi Mao",
                "https://hanzimao.me/"
            ],
            [
                "Chao-Yuan Wu",
                "https://chaoyuan.org/"
            ],
            [
                "Christoph Feichtenhofer",
                "https://feichtenhofer.github.io/"
            ],
            [
                "Trevor Darrell",
                "https://people.eecs.berkeley.edu/~trevor/"
            ],
            [
                "Saining Xie",
                "https://www.sainingxie.com/"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/facebookresearch/ConvNeXt",
                5801
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2201.03545"
            ],
            [
                "yt",
                "https://youtu.be/QzCjXqFnWPE"
            ],
            [
                "yt",
                "https://youtu.be/idiIllIQOfU"
            ],
            [
                "yt",
                "https://youtu.be/QqejV0LNDHA"
            ],
            [
                "hf",
                "https://huggingface.co/spaces/akhaliq/convnext"
            ],
            [
                "git",
                "https://github.com/rwightman/pytorch-image-models"
            ],
            [
                "git",
                "https://github.com/facebookresearch/deit"
            ],
            [
                "git",
                "https://github.com/microsoft/unilm/tree/master/beit"
            ],
            [
                "doi",
                "https://doi.org/10.1109/CVPR52688.2022.01167",
                3060
            ]
        ],
        "colab": "https://colab.research.google.com/drive/1CBYTIZ4tBMsVL5cqu9N_-Q3TBprqsfEO",
        "update": 1642559181.197
    },
    {
        "name": "Omnivore",
        "description": "A single model which excels at classifying images, videos, and single-view 3D data using exactly the same model parameters",
        "author": [
            [
                "Rohit Girdhar",
                "http://rohitgirdhar.github.io/"
            ],
            [
                "Mannat Singh",
                "https://scholar.google.com/citations?user=QOO8OCcAAAAJ"
            ],
            [
                "Nikhila Ravi",
                "https://nikhilaravi.com/"
            ],
            [
                "Laurens Maaten",
                "https://lvdmaaten.github.io/"
            ],
            [
                "Armand Joulin",
                "https://ai.facebook.com/people/armand-joulin/"
            ],
            [
                "Ishan Misra",
                "https://imisra.github.io/"
            ]
        ],
        "links": [
            [
                "project",
                "https://facebookresearch.github.io/omnivore/"
            ],
            [
                "git",
                "https://github.com/facebookresearch/omnivore",
                559
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2201.08377"
            ],
            [
                "hf",
                "https://huggingface.co/spaces/akhaliq/omnivore"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2206.08356"
            ],
            [
                "pwc",
                "https://paperswithcode.com/dataset/epic-kitchens-100"
            ],
            [
                "doi",
                "https://doi.org/10.1109/CVPR52688.2022.01563",
                95
            ]
        ],
        "colab": "https://colab.research.google.com/github/facebookresearch/omnivore/blob/main/inference_tutorial.ipynb",
        "update": 1655212594.0
    },
    {
        "name": "ESM",
        "description": "Evolutionary Scale Modeling: Pretrained language models for proteins",
        "author": [
            [
                "Zeming Lin",
                "https://research.facebook.com/people/lin-zeming/"
            ],
            [
                "Roshan Rao",
                "https://rmrao.github.io/"
            ],
            [
                "Brian Hie",
                "https://brianhie.com/"
            ],
            [
                "Zhongkai Zhu",
                "https://www.linkedin.com/in/zhongkai-zhu-03a27424"
            ],
            [
                "Allan dos Santos Costa",
                "https://scholar.google.com/citations?user=Zb4RsFsAAAAJ"
            ],
            [
                "Maryam Fazel-Zarandi",
                "https://www.maryamfazel.com/"
            ],
            [
                "Tom Sercu",
                "https://tom.sercu.me/"
            ],
            [
                "Salvatore Candido",
                "https://scholar.google.com/citations?user=BDgbhmEAAAAJ"
            ],
            [
                "Alexander Rives",
                "https://scholar.google.com/citations?user=vqb78-gAAAAJ"
            ],
            [
                "Joshua Meier",
                "https://scholar.google.com/citations?user=2M0OltAAAAAJ"
            ],
            [
                "Robert Verkuil",
                "https://dblp.org/pid/296/8930.html"
            ],
            [
                "Jason Liu",
                "https://www.linkedin.com/in/liujiayi/"
            ],
            [
                "Chloe Hsu",
                "https://chloe-hsu.com/"
            ],
            [
                "Adam Lerer",
                "https://scholar.google.com/citations?user=Ad6O4-0AAAAJ"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/facebookresearch/esm",
                3291
            ],
            [
                "hf",
                "https://huggingface.co/docs/transformers/model_doc/esm"
            ],
            [
                "paper",
                "https://doi.org/10.1101/2022.07.20.500902"
            ],
            [
                "paper",
                "https://doi.org/10.1101/2021.07.09.450648"
            ],
            [
                "paper",
                "https://doi.org/10.1101/2022.04.10.487779"
            ],
            [
                "ICML",
                "https://proceedings.mlr.press/v139/rao21a.html"
            ],
            [
                "pubmed",
                "https://pubmed.ncbi.nlm.nih.gov/33876751/"
            ],
            [
                "paper",
                "https://doi.org/10.1101/2022.12.21.521521"
            ],
            [
                "ESM Atlas",
                "https://esmatlas.com/"
            ],
            [
                "doi",
                "https://doi.org/10.1101/622803",
                138
            ],
            [
                "FSDP",
                "https://fairscale.readthedocs.io/en/stable/api/nn/fsdp.html"
            ],
            [
                "data",
                "https://ftp.uniprot.org/pub/databases/uniprot/previous_releases/release-2018_03/uniref/"
            ],
            [
                "yt",
                "https://youtu.be/N-eisTvUYrk"
            ],
            [
                "yt",
                "https://youtu.be/GHoE4VkDehY"
            ],
            [
                "git",
                "https://github.com/sokrypton/ColabFold"
            ]
        ],
        "colab": "https://colab.research.google.com/github/sokrypton/ColabFold/blob/main/ESMFold.ipynb",
        "update": 1703791136.0
    },
    {
        "name": "CutLER",
        "description": "Simple approach for training unsupervised object detection and segmentation models",
        "author": [
            [
                "Xudong Wang",
                "https://people.eecs.berkeley.edu/~xdwang/"
            ],
            [
                "Rohit Girdhar",
                "https://rohitgirdhar.github.io/"
            ],
            [
                "Stella Yu",
                "https://www1.icsi.berkeley.edu/~stellayu/"
            ],
            [
                "Ishan Misra",
                "https://imisra.github.io/"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/facebookresearch/CutLER",
                950
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2301.11320"
            ],
            [
                "project",
                "http://people.eecs.berkeley.edu/~xdwang/projects/CutLER/"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/1706.02677"
            ],
            [
                "docs",
                "https://detectron2.readthedocs.io/en/latest/tutorials/datasets.html"
            ]
        ],
        "colab": "https://colab.research.google.com/drive/1NgEyFHvOfuA2MZZnfNPWg1w5gSr3HOBb",
        "update": 1690172357.24
    },
    {
        "name": "MoCo",
        "description": "Momentum Contrast for unsupervised visual representation learning",
        "author": [
            [
                "Kaiming He",
                "https://kaiminghe.github.io/"
            ],
            [
                "Haoqi Fan",
                "https://haoqifan.github.io/"
            ],
            [
                "Yuxin Wu",
                "https://ppwwyyxx.com/"
            ],
            [
                "Saining Xie",
                "http://sainingxie.com/"
            ],
            [
                "Ross Girshick",
                "https://www.rossgirshick.info/"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/facebookresearch/moco",
                4825
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/1911.05722"
            ],
            [
                "doi",
                "https://doi.org/10.1109/CVPR42600.2020.00975",
                6006
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2003.04297"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/1706.02677"
            ],
            [
                "git",
                "https://github.com/ppwwyyxx/moco.tensorflow"
            ],
            [
                "yt",
                "https://youtu.be/LvHwBQF14zs"
            ],
            [
                "yt",
                "https://youtu.be/4VVGtYPM8JE"
            ],
            [
                "yt",
                "https://youtu.be/o5Qh61dLDf0"
            ]
        ],
        "colab": "https://colab.research.google.com/github/facebookresearch/moco/blob/colab-notebook/colab/moco_cifar10_demo.ipynb",
        "update": 1597942578.0
    },
    {
        "name": "Segment Anything",
        "description": "The Segment Anything Model produces high quality object masks from input prompts such as points or boxes, and it can be used to generate masks for all objects in an image",
        "author": [
            [
                "Alexander Kirillov",
                "https://alexander-kirillov.github.io/"
            ],
            [
                "Eric Mintun",
                "https://ericmintun.github.io/"
            ],
            [
                "Nikhila Ravi",
                "https://nikhilaravi.com/"
            ],
            [
                "Hanzi Mao",
                "https://hanzimao.me/"
            ],
            [
                "Chloé Rolland",
                "https://scholar.google.com/citations?user=n-SnMhoAAAAJ"
            ],
            [
                "Laura Gustafson",
                "https://scholar.google.com/citations?user=c8IpF9gAAAAJ"
            ],
            [
                "Tete Xiao",
                "https://tetexiao.com/"
            ],
            [
                "Spencer Whitehead",
                "https://www.spencerwhitehead.com/"
            ],
            [
                "Alex Berg",
                "http://acberg.com/"
            ],
            [
                "Wan-Yen Lo",
                "https://github.com/wanyenlo"
            ],
            [
                "Piotr Dollár",
                "https://pdollar.github.io/"
            ],
            [
                "Ross Girshick",
                "https://www.rossgirshick.info/"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/facebookresearch/segment-anything",
                47959
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2304.02643"
            ],
            [
                "meta",
                "https://ai.facebook.com/research/publications/segment-anything/"
            ],
            [
                "website",
                "https://segment-anything.com/"
            ],
            [
                "meta",
                "https://ai.facebook.com/blog/segment-anything-foundation-model-image-segmentation/"
            ],
            [
                "data",
                "https://ai.facebook.com/datasets/segment-anything/"
            ],
            [
                "yt",
                "https://youtu.be/2O_vecl28OA"
            ],
            [
                "yt",
                "https://youtu.be/fVeW9a6wItM"
            ],
            [
                "yt",
                "https://youtu.be/FjYE0tKWOiY"
            ]
        ],
        "colab": "https://colab.research.google.com/github/facebookresearch/segment-anything/blob/main/notebooks/predictor_example.ipynb",
        "update": 1681149017.0
    },
    {
        "name": "Segment Anything 2",
        "description": "Foundation model towards solving promptable visual segmentation in images and videos",
        "author": [
            [
                "Nikhila Ravi",
                "https://nikhilaravi.com/"
            ],
            [
                "Valentin Gabeur",
                "https://gabeur.github.io/"
            ],
            [
                "Yuan-Ting Hu",
                "https://scholar.google.com/citations?user=E8DVVYQAAAAJ"
            ],
            [
                "Ronghang Hu",
                "https://ronghanghu.com/"
            ],
            [
                "Chaitanya Ryali",
                "https://scholar.google.com/citations?user=4LWx24UAAAAJ"
            ],
            [
                "Tengyu Ma",
                "https://scholar.google.com/citations?user=VeTSl0wAAAAJ"
            ],
            [
                "Haitham Khedr",
                "https://hkhedr.com/"
            ],
            [
                "Roman Rädle",
                "https://scholar.google.de/citations?user=Tpt57v0AAAAJ"
            ],
            [
                "Chloé Rolland",
                "https://scholar.google.com/citations?user=n-SnMhoAAAAJ"
            ],
            [
                "Laura Gustafson",
                "https://scholar.google.com/citations?user=c8IpF9gAAAAJ"
            ],
            [
                "Eric Mintun",
                "https://ericmintun.github.io/"
            ],
            [
                "Junting Pan",
                "https://junting.github.io/"
            ],
            [
                "Kalyan Vasudev",
                "lwala](https://scholar.google.co.in/citations?user=m34oaWEAAAAJ"
            ],
            [
                "Nicolas Carion",
                "https://www.nicolascarion.com/"
            ],
            [
                "Chao-Yuan",
                "u](https://chaoyuan.org/"
            ],
            [
                "Ross Girshick",
                "https://www.rossgirshick.info/"
            ],
            [
                "Piotr Dollár",
                "https://pdollar.github.io/"
            ],
            [
                "Christoph Feichtenhofer",
                "https://feichtenhofer.github.io/"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/facebookresearch/segment-anything-2",
                12879
            ],
            [
                "meta",
                "https://ai.meta.com/research/publications/sam-2-segment-anything-in-images-and-videos/"
            ],
            [
                "project",
                "https://ai.meta.com/sam2/"
            ],
            [
                "demo",
                "https://sam2.metademolab.com/"
            ],
            [
                "meta",
                "https://ai.meta.com/datasets/segment-anything-video"
            ],
            [
                "meta",
                "https://ai.meta.com/blog/segment-anything-2"
            ],
            [
                "hf",
                "https://huggingface.co/models?search=facebook/sam2"
            ],
            [
                "git",
                "https://github.com/zsef123/Connected_components_PyTorch"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2408.00714"
            ],
            [
                "twitter",
                "https://x.com/AIatMeta/status/1818055906179105010"
            ],
            [
                "yt",
                "https://www.youtube.com/watch?v=w-cmMcMZoZ4&t=2325s"
            ],
            [
                "yt",
                "https://youtu.be/O8QdvZbRDp4"
            ],
            [
                "yt",
                "https://www.youtube.com/live/Dv003fTyO-Y"
            ],
            [
                "yt",
                "https://youtu.be/IW7jFq3vQbw"
            ]
        ],
        "colab": "https://colab.research.google.com/github/facebookresearch/segment-anything-2/blob/main/notebooks/image_predictor_example.ipynb",
        "update": 1727753264.0
    },
    {
        "name": "MMS",
        "description": "The Massively Multilingual Speech project expands speech technology from about 100 languages to over 1000 by building a single multilingual speech recognition model supporting over 1100 languages, language identification models able to identify over 4000 languages, pretrained models supporting over 1400 languages, and text-to-speech models for over 1100 languages",
        "author": [
            [
                "Vineel Pratap",
                "https://github.com/vineelpratap"
            ],
            [
                "Andros Tjandra",
                "https://github.com/androstj"
            ],
            [
                "Bowen Shi",
                "https://scholar.google.com/citations?user=xqyoorYAAAAJ"
            ],
            [
                "Paden Tomasello",
                "https://scholar.google.com/citations?user=sBtWMGYAAAAJ"
            ],
            [
                "Arun Babu",
                "https://scholar.google.com/citations?user=oJfoTakAAAAJ"
            ],
            [
                "Sayani Kundu",
                "https://www.linkedin.com/in/sayani-kundu"
            ],
            [
                "Ali Elkahky",
                "https://scholar.google.com/citations?user=KB3S8RoAAAAJ"
            ],
            [
                "Zhaoheng Ni",
                "https://scholar.google.com/citations?user=SYFMSNsAAAAJ"
            ],
            [
                "Apoorv Vyas",
                "https://apoorv2904.github.io/"
            ],
            [
                "Maryam Fazel-Zarandi",
                "https://www.maryamfazel.com/"
            ],
            [
                "Alexei Baevski",
                "https://github.com/alexeib"
            ],
            [
                "Yossi Adi",
                "https://www.cs.huji.ac.il/~adiyoss/"
            ],
            [
                "Xiaohui Zhang",
                "https://github.com/xiaohui-zhang"
            ],
            [
                "Wei-Ning Hsu",
                "https://wnhsu.github.io/"
            ],
            [
                "Alexis Conneau",
                "https://github.com/aconneau"
            ],
            [
                "Michael Auli",
                "https://github.com/michaelauli"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/facebookresearch/fairseq/tree/main/examples/mms",
                30632
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2305.13516"
            ],
            [
                "meta",
                "https://ai.facebook.com/blog/multilingual-model-speech-recognition/"
            ],
            [
                "hf",
                "https://huggingface.co/docs/transformers/main/en/model_doc/mms"
            ],
            [
                "hf",
                "https://huggingface.co/facebook/mms-cclms/"
            ],
            [
                "hf",
                "https://huggingface.co/blog/mms_adapters"
            ],
            [
                "yt",
                "https://youtu.be/GEzxHxWys2s"
            ],
            [
                "yt",
                "https://youtu.be/g06agCmxS7I"
            ]
        ],
        "colab": "https://colab.research.google.com/github/facebookresearch/fairseq/blob/main/examples/mms/asr/tutorial/MMS_ASR_Inference_Colab.ipynb",
        "update": 1685065846.0
    },
    {
        "name": "CoTracker",
        "description": "Architecture that jointly tracks multiple points throughout an entire video",
        "author": [
            [
                "Nikita Karaev",
                "https://nikitakaraevv.github.io/"
            ],
            [
                "Ignacio Rocco",
                "https://www.irocco.info/"
            ],
            [
                "Benjamin Graham",
                "https://ai.meta.com/people/benjamin-graham/"
            ],
            [
                "Natalia Neverova",
                "https://nneverova.github.io/"
            ],
            [
                "Andrea Vedaldi",
                "https://www.robots.ox.ac.uk/~vedaldi/"
            ],
            [
                "Christian Rupprecht",
                "https://chrirupp.github.io/"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/facebookresearch/co-tracker",
                3946
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2307.07635"
            ],
            [
                "project",
                "https://co-tracker.github.io/"
            ],
            [
                "git",
                "https://github.com/benjiebob/BADJA"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2303.11898"
            ],
            [
                "yt",
                "https://youtu.be/w5QVc7BVGPA"
            ]
        ],
        "colab": "https://colab.research.google.com/github/facebookresearch/co-tracker/blob/main/notebooks/demo.ipynb",
        "update": 1729078372.0
    },
    {
        "name": "DINOv2",
        "description": "Produce high-performance visual features that can be directly employed with classifiers as simple as linear layers on a variety of computer vision tasks; these visual features are robust and perform well across domains without any requirement for fine-tuning",
        "author": [
            [
                "Maxime Oquab",
                "https://scholar.google.com/citations?user=5vteYV8AAAAJ"
            ],
            [
                "Timothée Darcet",
                "https://github.com/TimDarcet"
            ],
            [
                "Théo Moutakanni",
                "https://github.com/TheoMoutakanni"
            ],
            [
                "Huy Vo",
                "https://huyvvo.github.io/"
            ],
            [
                "Marc Szafraniec",
                "https://github.com/MarcSzafraniec/"
            ],
            [
                "Vasil Khalidov",
                "https://scholar.google.com/citations?user=tjazz3AAAAAJ"
            ],
            [
                "Pierre Fernandez",
                "https://pierrefdz.github.io/"
            ],
            [
                "Daniel Haziza",
                "https://scholar.google.com/citations?user=2eSKdFMAAAAJ"
            ],
            [
                "Francisco Massa",
                "https://github.com/fmassa"
            ],
            [
                "Alaaeldin El-Nouby",
                "https://aelnouby.github.io/"
            ],
            [
                "Mahmoud Assran",
                "http://www.midoassran.ca/"
            ],
            [
                "Nicolas Ballas",
                "https://scholar.google.com/citations?user=euUV4iUAAAAJ"
            ],
            [
                "Wojciech Galuba",
                "https://scholar.google.com/citations?user=jyaTX64AAAAJ"
            ],
            [
                "Russell Howes",
                "http://www.russellhowes.net/"
            ],
            [
                "Po-Yao Huang",
                "https://berniebear.github.io/"
            ],
            [
                "Shang-Wen Li",
                "https://swdanielli.github.io/"
            ],
            [
                "Ishan Misra",
                "http://imisra.github.io/"
            ],
            [
                "Michael Rabbat",
                "https://scholar.google.com/citations?user=cMPKe9UAAAAJ"
            ],
            [
                "Vasu Sharma",
                "https://vasusharma.github.io/"
            ],
            [
                "Gabriel Synnaeve",
                "https://syhw.github.io/"
            ],
            [
                "Hu Xu",
                "https://howardhsu.github.io/"
            ],
            [
                "Hervé Jegou",
                "https://github.com/jegou"
            ],
            [
                "Julien Mairal",
                "http://thoth.inrialpes.fr/people/mairal/"
            ],
            [
                "Patrick Labatut",
                "https://github.com/patricklabatut"
            ],
            [
                "Armand Joulin",
                "https://scholar.google.com/citations?user=kRJkDakAAAAJ"
            ],
            [
                "Piotr Bojanowski",
                "https://github.com/piotr-bojanowski"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/facebookresearch/dinov2",
                9351
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2304.07193"
            ],
            [
                "blog post",
                "https://ai.facebook.com/blog/dino-v2-computer-vision-self-supervised-learning/"
            ],
            [
                "demo",
                "https://dinov2.metademolab.com/"
            ],
            [
                "yt",
                "https://youtu.be/csEgtSh7jV4"
            ],
            [
                "yt",
                "https://www.youtube.com/live/KSZiJ4k28b4"
            ],
            [
                "yt",
                "https://youtu.be/RZEkdOc3szU"
            ],
            [
                "hf",
                "https://huggingface.co/docs/transformers/main/model_doc/dinov2"
            ],
            [
                "medium",
                "https://purnasaigudikandula.medium.com/dinov2-image-classification-visualization-and-paper-review-745bee52c826"
            ],
            [
                "medium",
                "https://towardsdatascience.com/meta-ais-another-revolutionary-large-scale-model-dinov2-for-image-feature-extraction-1114b287eadd"
            ]
        ],
        "colab": "https://colab.research.google.com/github/facebookresearch/dinov2/blob/main/notebooks/semantic_segmentation.ipynb",
        "update": 1693500235.0
    },
    {
        "name": "audio2photoreal",
        "description": "Framework for generating full-bodied photorealistic avatars that gesture according to the conversational dynamics of a dyadic interaction",
        "author": [
            [
                "Evonne Ng",
                "https://people.eecs.berkeley.edu/~evonne_ng/"
            ],
            [
                "Javier Romero",
                "https://scholar.google.com/citations?user=Wx62iOsAAAAJ"
            ],
            [
                "Timur Bagautdinov",
                "https://scholar.google.ch/citations?user=oLi7xJ0AAAAJ"
            ],
            [
                "Shaojie Bai",
                "https://jerrybai1995.github.io/"
            ],
            [
                "Trevor Darrell",
                "https://people.eecs.berkeley.edu/~trevor/"
            ],
            [
                "Angjoo Kanazawa",
                "https://people.eecs.berkeley.edu/~kanazawa/"
            ],
            [
                "Alexander Richard",
                "https://alexanderrichard.github.io/"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/facebookresearch/audio2photoreal",
                2711
            ],
            [
                "project",
                "https://people.eecs.berkeley.edu/~evonne_ng/projects/audio2photoreal/"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2401.01885"
            ],
            [
                "yt",
                "https://youtu.be/Y0GMaMtUynQ"
            ],
            [
                "git",
                "https://github.com/facebookresearch/ca_body"
            ]
        ],
        "colab": "https://colab.research.google.com/drive/1A6EwKM3PeX7dcKV66zxQWuP-v_dKlX_0",
        "update": 1726190724.769
    },
    {
        "name": "CartoonGAN",
        "description": "The implementation of the cartoon GAN model with PyTorch",
        "author": [
            [
                "Tobias Sunderdiek",
                "https://github.com/TobiasSunderdiek"
            ]
        ],
        "links": [
            [
                "doi",
                "https://doi.org/10.1109/CVPR.2018.00986",
                268
            ],
            [
                "project",
                "https://tobiassunderdiek.github.io/cartoon-gan/"
            ],
            [
                "kaggle",
                "https://www.kaggle.com/alamson/safebooru"
            ]
        ],
        "colab": "https://colab.research.google.com/github/TobiasSunderdiek/cartoon-gan/blob/master/CartoonGAN.ipynb",
        "update": 1637767942.0
    },
    {
        "name": "VToonify",
        "description": "Leverages the mid- and high-resolution layers of StyleGAN to render high-quality artistic portraits based on the multi-scale content features extracted by an encoder to better preserve the frame details",
        "author": [
            [
                "Shuai Yang",
                "https://williamyang1991.github.io/"
            ],
            [
                "Liming Jiang",
                "https://liming-jiang.com/"
            ],
            [
                "Ziwei Liu",
                "https://liuziwei7.github.io/"
            ],
            [
                "Chen Change Loy",
                "https://www.mmlab-ntu.com/person/ccloy/"
            ]
        ],
        "links": [
            [
                "project",
                "https://www.mmlab-ntu.com/project/vtoonify/"
            ],
            [
                "git",
                "https://github.com/williamyang1991/VToonify",
                3550
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2209.11224"
            ],
            [
                "yt",
                "https://youtu.be/0_OmVhDgYuY"
            ],
            [
                "hf",
                "https://huggingface.co/spaces/PKUWilliamYang/VToonify"
            ],
            [
                "hf",
                "https://huggingface.co/PKUWilliamYang/VToonify/tree/main/models"
            ],
            [
                "git",
                "https://github.com/rosinality/stylegan2-pytorch"
            ],
            [
                "git",
                "https://github.com/zllrunning/face-parsing.PyTorch"
            ],
            [
                "git",
                "https://github.com/zhujiapeng/LowRankGAN"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2001.02890"
            ],
            [
                "doi",
                "https://doi.org/10.1145/3550454.3555437",
                40
            ]
        ],
        "colab": "http://colab.research.google.com/github/williamyang1991/VToonify/blob/master/notebooks/inference_playground.ipynb",
        "update": 1665111118.0
    },
    {
        "name": "DualStyleGAN",
        "description": "More challenging exemplar-based high-resolution portrait style transfer by introducing a novel DualStyleGAN with flexible control of dual styles of the original face domain and the extended artistic portrait domain",
        "author": [
            [
                "Shuai Yang",
                "https://williamyang1991.github.io/"
            ],
            [
                "Liming Jiang",
                "https://liming-jiang.com/"
            ],
            [
                "Ziwei Liu",
                "https://liuziwei7.github.io/"
            ],
            [
                "Chen Change Loy",
                "https://www.mmlab-ntu.com/person/ccloy/"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/williamyang1991/DualStyleGAN",
                1645
            ],
            [
                "project",
                "https://www.mmlab-ntu.com/project/dualstylegan/"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2203.13248"
            ],
            [
                "yt",
                "https://youtu.be/scZTu77jixI"
            ],
            [
                "hf",
                "https://huggingface.co/spaces/Gradio-Blocks/DualStyleGAN"
            ],
            [
                "hf",
                "https://huggingface.co/spaces/hysts/DualStyleGAN"
            ],
            [
                "data",
                "https://cs.nju.edu.cn/rl/WebCaricature.htm"
            ],
            [
                "data",
                "https://www.gwern.net/Crops#danbooru2019-portraits"
            ],
            [
                "git",
                "https://github.com/lowfuel/progrock-stable"
            ],
            [
                "git",
                "https://github.com/rosinality/stylegan2-pytorch"
            ],
            [
                "git",
                "https://github.com/TreB1eN/InsightFace_Pytorch"
            ],
            [
                "doi",
                "https://doi.org/10.1109/CVPR52688.2022.00754",
                69
            ]
        ],
        "colab": "https://colab.research.google.com/github/williamyang1991/DualStyleGAN/blob/master/notebooks/inference_playground.ipynb",
        "update": 1648108415.0
    },
    {
        "name": "GP-UNIT",
        "description": "Novel framework, Generative Prior-guided UNsupervised Image-to-image Translation, to improve the overall quality and applicability of the translation algorithm",
        "author": [
            [
                "Shuai Yang",
                "https://williamyang1991.github.io/"
            ],
            [
                "Liming Jiang",
                "https://liming-jiang.com/"
            ],
            [
                "Ziwei Liu",
                "https://liuziwei7.github.io/"
            ],
            [
                "Chen Change Loy",
                "https://www.mmlab-ntu.com/person/ccloy/"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/williamyang1991/GP-UNIT",
                193
            ],
            [
                "project",
                "https://www.mmlab-ntu.com/project/gpunit/"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2204.03641"
            ],
            [
                "yt",
                "https://youtu.be/dDApWs_oDrM"
            ],
            [
                "git",
                "https://github.com/clovaai/stargan-v2#datasets-and-pre-trained-networks"
            ],
            [
                "git",
                "https://github.com/switchablenorms/CelebAMask-HQ"
            ],
            [
                "git",
                "https://github.com/NVlabs/metfaces-dataset"
            ],
            [
                "ImageNet",
                "https://image-net.org/download.php"
            ],
            [
                "git",
                "https://github.com/TreB1eN/InsightFace_Pytorch"
            ],
            [
                "git",
                "https://github.com/NVlabs/SPADE"
            ],
            [
                "git",
                "https://github.com/nvlabs/imaginaire"
            ],
            [
                "git",
                "https://doi.org/10.1109/CVPR52688.2022.01779"
            ]
        ],
        "colab": "https://colab.research.google.com/github/williamyang1991/GP-UNIT/blob/main/notebooks/inference_playground.ipynb",
        "update": 1648904310.0
    },
    {
        "name": "YOLOv6",
        "description": "Single-stage object detection framework dedicated to industrial applications",
        "author": [
            [
                "Kaiheng Weng",
                "https://github.com/khwengXU"
            ],
            [
                "Meng Cheng",
                "https://github.com/MTChengMeng"
            ],
            [
                "Yiduo Li",
                "https://github.com/yili123123"
            ],
            [
                "Xiangxiang Chu",
                "https://scholar.google.com/citations?&user=jn21pUsAAAAJ"
            ],
            [
                "Xiaolin Wei",
                "https://scholar.google.com/citations?user=s5b7lU4AAAAJ"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/meituan/YOLOv6",
                5724
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2209.02976"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2301.05586"
            ],
            [
                "yt",
                "https://youtu.be/3OpwcGU7VvE"
            ],
            [
                "yt",
                "https://youtu.be/GJ0lVOE3a7c"
            ],
            [
                "yt",
                "https://youtu.be/3hqkbqJ5ag8"
            ],
            [
                "data",
                "https://cocodataset.org/#download"
            ],
            [
                "docs",
                "https://yolov6-docs.readthedocs.io/zh_CN/latest/"
            ],
            [
                "git",
                "https://github.com/FeiGeChuanShu/ncnn-android-yolov6"
            ],
            [
                "git",
                "https://github.com/DefTruth/lite.ai.toolkit/blob/main/lite/ort/cv/yolov6.cpp"
            ],
            [
                "git",
                "https://github.com/Linaom1214/TensorRT-For-YOLO-Series"
            ],
            [
                "git",
                "https://github.com/zhiqwang/yolov5-rt-stack/tree/main/deployment/tensorrt-yolov6"
            ],
            [
                "yt",
                "https://youtu.be/fFCWrMFH2UY"
            ],
            [
                "blog post",
                "https://learnopencv.com/yolov6-object-detection/"
            ]
        ],
        "colab": "https://colab.research.google.com/github/meituan/YOLOv6/blob/master/turtorial.ipynb",
        "update": 1696751826.0
    },
    {
        "name": "YOLOv7",
        "description": "Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors",
        "author": [
            [
                "Chien-Yao Wang",
                "https://scholar.google.com/citations?user=DkQh4M4AAAAJ"
            ],
            [
                "Alexey Bochkovskiy",
                "http://www.alexeyab.com/"
            ],
            [
                "Mark Liao",
                "https://www.iis.sinica.edu.tw/pages/liao/"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/WongKinYiu/yolov7",
                13445
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2207.02696"
            ],
            [
                "pwc",
                "https://paperswithcode.com/sota/real-time-object-detection-on-coco?p=yolov7-trainable-bag-of-freebies-sets-new"
            ],
            [
                "data",
                "http://images.cocodataset.org/annotations/annotations_trainval2017.zip"
            ],
            [
                "data",
                "http://images.cocodataset.org/zips/train2017.zip"
            ],
            [
                "data",
                "http://images.cocodataset.org/zips/val2017.zip"
            ],
            [
                "data",
                "https://github.com/WongKinYiu/yolov7/releases/download/v0.1/coco2017labels-segments.zip"
            ],
            [
                "git",
                "https://github.com/WongKinYiu/yolor"
            ],
            [
                "git",
                "https://github.com/WongKinYiu/PyTorch_YOLOv4"
            ],
            [
                "git",
                "https://github.com/WongKinYiu/ScaledYOLOv4"
            ],
            [
                "git",
                "https://github.com/Megvii-BaseDetection/YOLOX"
            ],
            [
                "git",
                "https://github.com/DingXiaoH/RepVGG"
            ],
            [
                "git",
                "https://github.com/JUGGHM/OREPA_CVPR2022"
            ],
            [
                "git",
                "https://github.com/TexasInstruments/edgeai-yolov5/tree/yolo-pose"
            ],
            [
                "yt",
                "https://www.youtube.com/playlist?list=PL_Nji0JOuXg2QMohGK7wfzgJ-MavzXRHW"
            ],
            [
                "yt",
                "https://youtu.be/-QWxJ0j9EY8"
            ]
        ],
        "colab": "https://colab.research.google.com/github/WongKinYiu/yolov7/blob/main/tools/compare_YOLOv7_vs_YOLOv5m6_half.ipynb",
        "update": 1660020407.0
    },
    {
        "name": "YOLOv9",
        "description": "Learning What You Want to Learn Using Programmable Gradient Information",
        "author": [
            [
                "Chien-Yao Wang",
                "https://scholar.google.com/citations?user=DkQh4M4AAAAJ"
            ],
            [
                "I-Hau Yeh",
                "https://ieeexplore.ieee.org/author/37088448531"
            ],
            [
                "Hong-Yuan Mark Liao",
                "https://homepage.iis.sinica.edu.tw/pages/liao/index_zh.html"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/WongKinYiu/yolov9",
                9044
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2402.13616"
            ],
            [
                "hf",
                "https://huggingface.co/spaces/kadirnar/Yolov9"
            ],
            [
                "hf",
                "https://huggingface.co/merve/yolov9"
            ],
            [
                "blog post",
                "https://learnopencv.com/yolov9-advancing-the-yolo-legacy/"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2309.16921"
            ],
            [
                "git",
                "https://github.com/WongKinYiu/yolor"
            ],
            [
                "git",
                "https://github.com/VDIGPKU/DynamicDet"
            ],
            [
                "git",
                "https://github.com/DingXiaoH/RepVGG"
            ],
            [
                "medium",
                "https://medium.com/@Mert.A/how-to-use-yolov9-for-object-detection-93598ad88d7d"
            ],
            [
                "yt",
                "https://youtu.be/XHT2c8jT3Bc"
            ],
            [
                "yt",
                "https://youtu.be/3iLJ6YWPg28"
            ],
            [
                "yt",
                "https://youtu.be/dccf_sJF0Gg"
            ]
        ],
        "colab": "https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov9-object-detection-on-custom-dataset.ipynb",
        "update": 1709632497.0
    },
    {
        "name": "YOLOv10",
        "description": "Aim to further advance the performance-efficiency boundary of YOLOs from both the post-processing and model architecture",
        "author": [
            [
                "Ao Wang",
                "https://github.com/jameslahm"
            ],
            [
                "Hui Chen",
                "https://huichen24.github.io/"
            ],
            [
                "Kai Chen",
                "https://scholar.google.com/citations?user=bZQX708AAAAJ"
            ],
            [
                "Zijia Lin",
                "https://sites.google.com/site/linzijia72"
            ],
            [
                "Jungong Han",
                "https://jungonghan.github.io/"
            ],
            [
                "Guiguang Ding",
                "https://scholar.google.com/citations?user=B7F3yt4AAAAJ"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/THU-MIG/yolov10",
                10053
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2405.14458"
            ],
            [
                "hf",
                "https://huggingface.co/collections/jameslahm/yolov10-665b0d90b0b5bb85129460c2"
            ],
            [
                "hf",
                "https://huggingface.co/spaces/jameslahm/YOLOv10"
            ],
            [
                "hf",
                "https://huggingface.co/spaces/kadirnar/Yolov10"
            ],
            [
                "hf",
                "https://huggingface.co/spaces/Xenova/yolov10-web"
            ],
            [
                "blog post",
                "https://learnopencv.com/yolov10/"
            ],
            [
                "demo",
                "https://openbayes.com/console/public/tutorials/im29uYrnIoz"
            ],
            [
                "git",
                "https://github.com/rlggyp/YOLOv10-OpenVINO-CPP-Inference"
            ],
            [
                "git",
                "https://github.com/Seeed-Projects/jetson-examples/blob/main/reComputer/scripts/yolov10/README.md"
            ],
            [
                "git",
                "https://github.com/kaylorchen/rk3588-yolo-demo"
            ],
            [
                "git",
                "https://github.com/openvinotoolkit/openvino_notebooks/blob/latest/notebooks/yolov10-optimization/yolov10-optimization.ipynb"
            ],
            [
                "git",
                "https://github.com/sujanshresstha/YOLOv10_DeepSORT"
            ],
            [
                "git",
                "https://github.com/CVHub520/X-AnyLabeling"
            ],
            [
                "git",
                "https://github.com/DanielSarmiento04/yolov10cpp"
            ],
            [
                "git",
                "https://github.com/lyuwenyu/RT-DETR"
            ],
            [
                "medium",
                "https://medium.com/@batuhansenerr/yolov10-custom-object-detection-bd7298ddbfd3"
            ],
            [
                "reddit",
                "https://www.reddit.com/r/GPTFutureScience/comments/1d34rj1/yolov10_the_future_of_realtime_object_detection/"
            ],
            [
                "medium",
                "https://medium.com/@sunidhi.ashtekar/yolov10-revolutionizing-real-time-object-detection-72ef04ad441a"
            ],
            [
                "yt",
                "https://youtu.be/29tnSxhB3CY"
            ],
            [
                "yt",
                "https://youtu.be/2ZFJbeJXXDM"
            ],
            [
                "yt",
                "https://youtu.be/wM6nO75keOQ"
            ]
        ],
        "colab": "https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov10-object-detection-on-custom-dataset.ipynb",
        "update": 1724178645.0
    },
    {
        "name": "ByteTrack",
        "description": "Multi-Object Tracking by Associating Every Detection Box",
        "author": [
            [
                "Yifu Zhang",
                "https://github.com/ifzhang"
            ],
            [
                "Peize Sun",
                "https://peizesun.github.io/"
            ],
            [
                "Yi Jiang",
                "https://github.com/iFighting"
            ],
            [
                "Dongdong Yu",
                "https://miracle-fmh.github.io/"
            ],
            [
                "Ping Luo",
                "http://luoping.me/"
            ],
            [
                "Xinggang Wang",
                "https://xinggangw.info/"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/ifzhang/ByteTrack",
                4852
            ],
            [
                "git",
                "https://github.com/Megvii-BaseDetection/YOLOX"
            ],
            [
                "git",
                "https://github.com/ifzhang/FairMOT"
            ],
            [
                "git",
                "https://github.com/PeizeSun/TransTrack"
            ],
            [
                "git",
                "https://github.com/samylee/Towards-Realtime-MOT-Cpp"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2110.06864"
            ],
            [
                "data",
                "https://motchallenge.net/"
            ],
            [
                "data",
                "https://www.crowdhuman.org/"
            ],
            [
                "pwc",
                "https://paperswithcode.com/task/multi-object-tracking"
            ],
            [
                "doi",
                "https://doi.org/10.1007/978-3-031-20047-2_1",
                755
            ]
        ],
        "colab": "https://colab.research.google.com/drive/1bDilg4cmXFa8HCKHbsZ_p16p0vrhLyu0",
        "update": 1635566517.748
    },
    {
        "name": "3D Photo Inpainting",
        "description": "Method for converting a single RGB-D input image into a 3D photo, i.e., a multi-layer representation for novel view synthesis that contains hallucinated color and depth structures in regions occluded in the original view",
        "author": [
            [
                "Meng-Li Shih",
                "https://shihmengli.github.io/"
            ],
            [
                "Shih-Yang Su",
                "https://lemonatsu.github.io/"
            ],
            [
                "Johannes Kopf",
                "https://johanneskopf.de/"
            ],
            [
                "Jia-Bin Huang",
                "https://jbhuang0604.github.io/"
            ]
        ],
        "links": [
            [
                "arxiv",
                "https://arxiv.org/abs/2004.04727"
            ],
            [
                "git",
                "https://github.com/vt-vl-lab/3d-photo-inpainting",
                6933
            ],
            [
                "project",
                "https://shihmengli.github.io/3D-Photo-Inpainting/"
            ],
            [
                "doi",
                "https://doi.org/10.1109/CVPR42600.2020.00805",
                188
            ]
        ],
        "colab": "https://colab.research.google.com/drive/1706ToQrkIZshRSJSHvZ1RuCiM__YX3Bz",
        "update": 1588579458.042
    },
    {
        "name": "FGVC",
        "description": "Method first extracts and completes motion edges, and then uses them to guide piecewise-smooth flow completion with sharp edges",
        "author": [
            [
                "Chen Gao",
                "http://chengao.vision/"
            ],
            [
                "Ayush Saraf",
                "https://github.com/ayush29feb"
            ],
            [
                "Johannes Kopf",
                "https://johanneskopf.de/"
            ],
            [
                "Jia-Bin Huang",
                "https://jbhuang0604.github.io/"
            ]
        ],
        "links": [
            [
                "arxiv",
                "https://arxiv.org/abs/2009.01835"
            ],
            [
                "git",
                "https://github.com/vt-vl-lab/FGVC",
                1552
            ],
            [
                "project",
                "http://chengao.vision/FGVC/"
            ],
            [
                "yt",
                "https://www.youtube.com/watch?v=CHHVPxHT7rc"
            ],
            [
                "doi",
                "https://doi.org/10.1007/978-3-030-58610-2_42",
                82
            ]
        ],
        "colab": "https://colab.research.google.com/drive/1pb6FjWdwq_q445rG2NP0dubw7LKNUkqc",
        "update": 1609361825.432
    },
    {
        "name": "Instance-aware Image Colorization",
        "description": "Novel deep learning framework to achieve instance-aware colorization",
        "author": [
            [
                "Jheng-Wei Su",
                "https://github.com/ericsujw"
            ]
        ],
        "links": [
            [
                "arxiv",
                "https://arxiv.org/abs/2005.10825"
            ],
            [
                "git",
                "https://github.com/ericsujw/InstColorization",
                712
            ],
            [
                "project",
                "https://ericsujw.github.io/InstColorization/"
            ],
            [
                "yt",
                "https://www.youtube.com/watch?v=Zj1N4uE1ehk"
            ],
            [
                "doi",
                "https://doi.org/10.1109/CVPR42600.2020.00799",
                145
            ]
        ],
        "colab": "https://colab.research.google.com/github/ericsujw/InstColorization/blob/master/InstColorization.ipynb",
        "update": 1598782500.0
    },
    {
        "name": "Adversarial Patch",
        "description": "A method to create universal, robust, targeted adversarial image patches in the real world",
        "author": [
            [
                "Tom Brown",
                "https://github.com/nottombrown"
            ]
        ],
        "links": [
            [
                "arxiv",
                "https://arxiv.org/abs/1712.09665"
            ]
        ],
        "colab": "https://colab.research.google.com/github/cleverhans-lab/cleverhans/blob/master/examples/adversarial_patch/AdversarialPatch.ipynb",
        "update": 1611756606.0
    },
    {
        "name": "MakeItTalk",
        "description": "A method that generates expressive talking-head videos from a single facial image with audio as the only input",
        "author": [
            [
                "Yang Zhou",
                "https://people.umass.edu/~yangzhou/"
            ],
            [
                "Xintong Han",
                "http://users.umiacs.umd.edu/~xintong/"
            ],
            [
                "Eli Shechtman",
                "https://research.adobe.com/person/eli-shechtman/"
            ],
            [
                "Jose Echevarria",
                "http://www.jiechevarria.com/"
            ],
            [
                "Evangelos Kalogerakis",
                "https://people.cs.umass.edu/~kalo/"
            ],
            [
                "Dingzeyu Li",
                "https://dingzeyu.li/"
            ]
        ],
        "links": [
            [
                "arxiv",
                "https://arxiv.org/abs/2004.12992"
            ],
            [
                "project",
                "https://people.umass.edu/~yangzhou/MakeItTalk/"
            ],
            [
                "git",
                "https://github.com/yzhou359/MakeItTalk",
                980
            ],
            [
                "yt",
                "https://www.youtube.com/watch?v=vUMGKASgbf8"
            ],
            [
                "data",
                "https://drive.google.com/drive/folders/1EwuAy3j1b9Zc1MsidUfxG_pJGc_cV60O"
            ],
            [
                "doi",
                "https://doi.org/10.1145/3414685.3417774",
                201
            ]
        ],
        "colab": "https://colab.research.google.com/github/iboyles/makeittalknow/blob/main/working_quick_demo_of_makeittalk_07_2023.ipynb",
        "update": 1690462580.0
    },
    {
        "name": "Taming Transformers for High-Resolution Image Synthesis",
        "description": "We combine the efficiancy of convolutional approaches with the expressivity of transformers by introducing a convolutional VQGAN, which learns a codebook of context-rich visual parts, whose composition is modeled with an autoregressive transformer",
        "author": [
            [
                "Patrick Esser",
                "https://github.com/pesser"
            ],
            [
                "Robin Rombach",
                "https://github.com/rromb"
            ],
            [
                "Björn Ommer",
                "https://ommer-lab.com/people/ommer/"
            ]
        ],
        "links": [
            [
                "arxiv",
                "https://arxiv.org/abs/2012.09841"
            ],
            [
                "git",
                "https://github.com/CompVis/taming-transformers",
                5849
            ],
            [
                "project",
                "https://compvis.github.io/taming-transformers/"
            ],
            [
                "doi",
                "https://doi.org/10.1109/CVPR46437.2021.01268",
                1011
            ]
        ],
        "colab": "https://colab.research.google.com/github/CompVis/taming-transformers/blob/master/scripts/taming-transformers.ipynb",
        "update": 1642096207.0
    },
    {
        "name": "Geometry-Free View Synthesis",
        "description": "Is a geometric model required to synthesize novel views from a single image?",
        "author": [
            [
                "Robin Rombach",
                "https://github.com/rromb"
            ],
            [
                "Patrick Esser",
                "https://github.com/pesser"
            ],
            [
                "Björn Ommer",
                "https://ommer-lab.com/people/ommer/"
            ]
        ],
        "links": [
            [
                "arxiv",
                "https://arxiv.org/abs/2104.07652"
            ],
            [
                "data",
                "https://google.github.io/realestate10k/"
            ],
            [
                "git",
                "https://github.com/CompVis/geometry-free-view-synthesis",
                378
            ],
            [
                "git",
                "https://github.com/colmap/colmap"
            ],
            [
                "doi",
                "https://doi.org/10.1109/ICCV48922.2021.01409",
                42
            ]
        ],
        "colab": "https://colab.research.google.com/github/CompVis/geometry-free-view-synthesis/blob/master/scripts/braindance.ipynb",
        "update": 1619095273.0
    },
    {
        "name": "Rethinking Style Transfer: From Pixels to Parameterized Brushstrokes",
        "description": "A method to stylize images by optimizing parameterized brushstrokes instead of pixels",
        "author": [
            [
                "Dmytro Kotovenko",
                "https://scholar.google.de/citations?user=T_U8yxwAAAAJ"
            ],
            [
                "Matthias Wright",
                "https://matthias-wright.github.io/"
            ],
            [
                "Arthur Heimbrecht",
                "https://github.com/arwehei"
            ],
            [
                "Björn Ommer",
                "https://ommer-lab.com/people/ommer/"
            ]
        ],
        "links": [
            [
                "project",
                "https://compvis.github.io/brushstroke-parameterized-style-transfer/"
            ],
            [
                "git",
                "https://github.com/CompVis/brushstroke-parameterized-style-transfer",
                165
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2103.17185"
            ],
            [
                "doi",
                "https://doi.org/10.1109/CVPR46437.2021.01202",
                44
            ]
        ],
        "colab": "https://colab.research.google.com/github/CompVis/brushstroke-parameterized-style-transfer/blob/tensorflow_v2/notebooks/BrushstrokeStyleTransfer_TF2.ipynb",
        "update": 1622625301.0
    },
    {
        "name": "LDM",
        "description": "High-Resolution Image Synthesis with Latent Diffusion Models",
        "author": [
            [
                "Robin Rombach",
                "https://github.com/rromb"
            ],
            [
                "Andreas Blattmann",
                "https://github.com/ablattmann"
            ],
            [
                "Dominik Lorenz",
                "https://github.com/qp-qp"
            ],
            [
                "Patrick Esser",
                "https://github.com/pesser"
            ],
            [
                "Björn Ommer",
                "https://ommer-lab.com/people/ommer/"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/CompVis/latent-diffusion",
                12006
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2112.10752"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2202.09778"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2111.02114"
            ],
            [
                "git",
                "https://github.com/fyu/lsun"
            ],
            [
                "git",
                "https://github.com/openai/guided-diffusion"
            ],
            [
                "git",
                "https://github.com/lucidrains/denoising-diffusion-pytorch"
            ],
            [
                "git",
                "https://github.com/lucidrains/x-transformers"
            ],
            [
                "hf",
                "https://huggingface.co/spaces/multimodalart/latentdiffusion"
            ],
            [
                "doi",
                "https://doi.org/10.1109/CVPR52688.2022.01042",
                4377
            ]
        ],
        "colab": "https://colab.research.google.com/github/CompVis/latent-diffusion/blob/master/scripts/latent_imagenet_diffusion.ipynb",
        "update": 1649081868.0
    },
    {
        "name": "Stable Dreamfusion",
        "description": "Using a pretrained 2D text-to-image diffusion model to perform text-to-3D synthesis",
        "author": [
            [
                "Jiaxiang Tang",
                "https://me.kiui.moe/"
            ],
            [
                "Ben Poole",
                "https://cs.stanford.edu/~poole/"
            ],
            [
                "Ajay Jain",
                "https://ajayj.com/"
            ],
            [
                "Jon Barron",
                "https://jonbarron.info/"
            ],
            [
                "Ben Mildenhall",
                "https://bmild.github.io/"
            ]
        ],
        "links": [
            [
                "project",
                "https://dreamfusion3d.github.io/"
            ],
            [
                "git",
                "https://github.com/ashawkey/stable-dreamfusion",
                8339
            ],
            [
                "git",
                "https://github.com/ashawkey/torch-ngp"
            ],
            [
                "hf",
                "https://huggingface.co/runwayml/stable-diffusion-v1-5"
            ],
            [
                "pt",
                "https://pytorch.org/docs/stable/cpp_extension.html#torch.utils.cpp_extension.load"
            ],
            [
                "git",
                "https://github.com/hoffstadt/DearPyGui"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2209.14988"
            ],
            [
                "yt",
                "https://youtu.be/uM5NPodZZ1U?t=219"
            ],
            [
                "yt",
                "https://youtu.be/zWD5ZR5GtJM"
            ],
            [
                "yt",
                "https://youtu.be/L3G0dx1Q0R8"
            ],
            [
                "yt",
                "https://youtu.be/dIgDbBTztUM"
            ]
        ],
        "colab": "https://colab.research.google.com/drive/1MXT3yfOFvO0ooKEfiUUvTKwUkrrlCHpF",
        "update": 1680575748.565
    },
    {
        "name": "NFNet",
        "description": "An adaptive gradient clipping technique, a significantly improved class of Normalizer-Free ResNets",
        "author": [
            [
                "Andrew Brock",
                "https://github.com/ajbrock"
            ],
            [
                "Soham De",
                "https://sohamde.github.io/"
            ],
            [
                "Samuel L. Smith",
                "https://scholar.google.co.uk/citations?user=fyEqU5oAAAAJ"
            ],
            [
                "Karen Simonyan",
                "https://scholar.google.com/citations?user=L7lMQkQAAAAJ"
            ]
        ],
        "links": [
            [
                "arxiv",
                "https://arxiv.org/abs/2102.06171"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2101.08692"
            ],
            [
                "git",
                "https://github.com/deepmind/deepmind-research/tree/master/nfnets",
                13307
            ],
            [
                "git",
                "https://github.com/deepmind/jaxline"
            ],
            [
                "yt",
                "https://youtu.be/rNkHjZtH0RQ"
            ],
            [
                "yt",
                "https://www.youtube.com/live/qyy2WhRRSI4?feature=share"
            ]
        ],
        "colab": "https://colab.research.google.com/github/deepmind/deepmind-research/blob/master/nfnets/nfnet_demo_colab.ipynb",
        "update": 1613583776.0
    },
    {
        "name": "Skillful Precipitation Nowcasting Using Deep Generative Models of Radar",
        "description": "Open-sourced dataset and model snapshot for precipitation nowcasting",
        "author": [
            [
                "Suman Ravuri",
                "https://www.linkedin.com/in/suman-ravuri-81928082"
            ],
            [
                "Karel Lenc",
                "https://www.robots.ox.ac.uk/~karel/"
            ],
            [
                "Matthew Willson",
                "https://www.linkedin.com/in/matthew-willson-6a1b422"
            ],
            [
                "Dmitry Kangin",
                "https://scholar.google.com/citations?user=vv-leaMAAAAJ"
            ],
            [
                "Rémi Lam",
                "https://github.com/remilam"
            ],
            [
                "Piotr Mirowski",
                "https://piotrmirowski.com/"
            ],
            [
                "Maria Athanassiadou",
                "https://scholar.google.com/citations?user=VtkgHP0AAAAJ"
            ],
            [
                "Sheleem Kashem",
                "https://www.linkedin.com/in/sheleemkashem/"
            ],
            [
                "Rachel Prudden",
                "https://computerscience.exeter.ac.uk/staff/rep218"
            ],
            [
                "Amol Mandhane",
                "https://github.com/amol-mandhane"
            ],
            [
                "Aidan Clark",
                "https://scholar.google.com/citations?user=_19DrfIAAAAJ"
            ],
            [
                "Andrew Brock",
                "https://github.com/ajbrock"
            ],
            [
                "Karen Simonyan",
                "https://scholar.google.com/citations?user=L7lMQkQAAAAJ"
            ],
            [
                "Raia Hadsell",
                "https://github.com/raiah"
            ],
            [
                "Niall Robinson",
                "https://github.com/niallrobinson"
            ],
            [
                "Ellen Clancy",
                "https://www.linkedin.com/in/ellen-clancy-815967124"
            ],
            [
                "Shakir Mohamed",
                "https://www.shakirm.com/"
            ]
        ],
        "links": [
            [
                "arxiv",
                "https://arxiv.org/abs/2104.00954"
            ],
            [
                "git",
                "https://github.com/deepmind/deepmind-research/tree/master/nowcasting",
                13307
            ],
            [
                "tf",
                "https://www.tensorflow.org/hub"
            ],
            [
                "local kernel",
                "https://research.google.com/colaboratory/local-runtimes.html"
            ],
            [
                "blog post",
                "https://deepmind.com/blog/article/nowcasting"
            ],
            [
                "doi",
                "https://doi.org/10.1038/s41586-021-03854-z",
                443
            ]
        ],
        "colab": "https://colab.research.google.com/github/deepmind/deepmind-research/blob/master/nowcasting/Open_sourced_dataset_and_model_snapshot_for_precipitation_nowcasting.ipynb",
        "update": 1632927630.0
    },
    {
        "name": "AlphaFold",
        "description": "Highly accurate protein structure prediction",
        "author": [
            [
                "John Jumper",
                "https://scholar.google.com/citations?user=a5goOh8AAAAJ"
            ],
            [
                "Richard Evans",
                "http://www.doc.ic.ac.uk/~re14/"
            ],
            [
                "Alexander Pritzel",
                "https://scholar.google.com/citations?user=GPgAyU0AAAAJ"
            ],
            [
                "Tim Green",
                "http://tfgg.me/"
            ],
            [
                "Michael Figurnov",
                "https://figurnov.ru/"
            ],
            [
                "Olaf Ronneberger",
                "https://lmb.informatik.uni-freiburg.de/people/ronneber/"
            ],
            [
                "Kathryn Tunyasuvunakool",
                "https://scholar.google.com/citations?user=eEqNGagAAAAJ"
            ],
            [
                "Russ Bates",
                "https://scholar.google.com/citations?user=Koes5ewAAAAJ"
            ],
            [
                "Augustin Žídek",
                "https://augustin.zidek.eu/"
            ],
            [
                "Anna Potapenko",
                "http://apotapenko.com/"
            ],
            [
                "Alex Bridgland",
                "https://scholar.google.com/citations?user=VWmXKPMAAAAJ"
            ],
            [
                "Clemens Meyer",
                "https://scholar.google.com/citations?user=EWLZiM8AAAAJ"
            ],
            [
                "Simon Kohl",
                "https://www.simonkohl.com/"
            ],
            [
                "Andrew Ballard",
                "https://scholar.google.com/citations?user=syjQhAMAAAAJ"
            ],
            [
                "Bernardino Romera-Paredes",
                "https://sites.google.com/site/romeraparedes/"
            ],
            [
                "Stanislav Nikolov",
                "https://scholar.google.co.uk/citations?user=O-b7pBEAAAAJ"
            ],
            [
                "Rishub Jain",
                "http://rishub.me/"
            ]
        ],
        "links": [
            [
                "paper",
                "https://www.nature.com/articles/s41586-021-03828-1"
            ],
            [
                "wiki",
                "https://en.wikipedia.org/wiki/AlphaFold"
            ],
            [
                "git",
                "https://github.com/deepmind/alphafold/",
                12940
            ],
            [
                "blog post",
                "https://deepmind.com/blog/article/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology"
            ],
            [
                "blog post",
                "https://deepmind.com/blog/article/putting-the-power-of-alphafold-into-the-worlds-hands"
            ],
            [
                "git",
                "https://github.com/deepmind/tree"
            ],
            [
                "git",
                "https://github.com/deepmind/chex"
            ],
            [
                "yt",
                "https://www.youtube.com/watch?v=gg7WjuFs8F4"
            ],
            [
                "yt",
                "https://www.youtube.com/watch?v=B9PL__gVxLI"
            ],
            [
                "pwc",
                "https://paperswithcode.com/method/alphafold"
            ],
            [
                "doi",
                "https://doi.org/10.1038/s41586-021-03819-2",
                24593
            ]
        ],
        "colab": "https://colab.research.google.com/github/deepmind/alphafold/blob/master/notebooks/AlphaFold.ipynb",
        "update": 1713204484.0
    },
    {
        "name": "Arnheim",
        "description": "Generative Art Using Neural Visual Grammars and Dual Encoders",
        "author": [
            [
                "Chrisantha Fernando",
                "https://www.chrisantha.co.uk/"
            ],
            [
                "Ali Eslami",
                "http://arkitus.com/"
            ],
            [
                "Jean-Baptiste Alayrac",
                "https://www.jbalayrac.com/"
            ],
            [
                "Piotr Mirowski",
                "https://piotrmirowski.com/"
            ],
            [
                "Dylan Banarse",
                "https://www.2ne1.com/"
            ],
            [
                "Simon Osindero",
                "https://scholar.google.com/citations?user=Jq8ZS5kAAAAJ"
            ]
        ],
        "links": [
            [
                "arxiv",
                "https://arxiv.org/abs/2105.00162"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2106.14843"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/1801.07729"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/1606.02580"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/1609.09106"
            ],
            [
                "git",
                "https://github.com/deepmind/arnheim",
                235
            ],
            [
                "git",
                "https://github.com/openai/dall-e"
            ],
            [
                "wiki",
                "https://en.wikipedia.org/wiki/Compositional_pattern-producing_network"
            ],
            [
                "yt",
                "https://www.youtube.com/watch?v=U7guaMdeF4g"
            ],
            [
                "yt",
                "https://www.youtube.com/watch?v=zh0goLbS-l0"
            ],
            [
                "yt",
                "https://www.youtube.com/watch?v=SYJGNt7yu6M"
            ],
            [
                "yt",
                "https://www.youtube.com/watch?v=MxkYKa0x5AU"
            ]
        ],
        "colab": "https://colab.research.google.com/github/deepmind/arnheim/blob/master/arnheim_2.ipynb",
        "update": 1636635016.0
    },
    {
        "name": "Functa",
        "description": "From data to functa: Your data point is a function and you can treat it like one",
        "author": [
            [
                "Emilien Dupont",
                "https://emiliendupont.github.io/"
            ],
            [
                "Hyunjik Kim",
                "https://hyunjik11.github.io/"
            ],
            [
                "Ali Eslami",
                "http://arkitus.com/"
            ],
            [
                "Danilo Rezende",
                "https://danilorezende.com/about/"
            ],
            [
                "Dan Rosenbaum",
                "https://danrsm.github.io/"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/deepmind/functa",
                149
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2201.12204"
            ],
            [
                "tf",
                "https://www.tensorflow.org/datasets/catalog/celeb_a_hq"
            ],
            [
                "git",
                "https://github.com/sxyu/pixel-nerf"
            ],
            [
                "git",
                "https://github.com/deepmind/jaxline"
            ]
        ],
        "colab": "https://colab.research.google.com/github/deepmind/functa/blob/main/modulation_visualization_colab.ipynb",
        "update": 1664010406.0
    },
    {
        "name": "AlphaTensor",
        "description": "Discovering faster matrix multiplication algorithms with reinforcement learning",
        "author": [
            [
                "Alhussein Fawzi",
                "http://www.alhusseinfawzi.info/"
            ],
            [
                "Matej Balog",
                "http://matejbalog.eu/"
            ],
            [
                "Aja Huang",
                "https://en.wikipedia.org/wiki/Aja_Huang"
            ],
            [
                "Thomas Hubert",
                "https://scholar.google.com/citations?user=WXG0QfMAAAAJ"
            ],
            [
                "Bernardino Romera-Paredes",
                "https://sites.google.com/site/romeraparedes/"
            ],
            [
                "Mohammadamin Barekatain",
                "http://barekatain.me/"
            ],
            [
                "Alexander Novikov",
                "https://scholar.google.com/citations?user=jMUkLqwAAAAJ"
            ],
            [
                "Francisco Ruiz",
                "https://franrruiz.github.io/"
            ],
            [
                "Julian Schrittwieser",
                "https://www.furidamu.org/"
            ],
            [
                "Grzegorz Swirszcz",
                "https://sites.google.com/site/grzegorzswirszcz/home"
            ],
            [
                "David Silver",
                "https://www.davidsilver.uk/"
            ],
            [
                "Demis Hassabis",
                "https://en.wikipedia.org/wiki/Demis_Hassabis"
            ],
            [
                "Pushmeet Kohli",
                "https://sites.google.com/site/pushmeet/"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/google-deepmind/alphatensor",
                2689
            ],
            [
                "deepmind",
                "https://www.deepmind.com/blog/discovering-novel-algorithms-with-alphatensor"
            ],
            [
                "yt",
                "https://youtu.be/3N3Bl5AA5QU"
            ],
            [
                "yt",
                "https://youtu.be/gpYnDls4PdQ"
            ],
            [
                "yt",
                "https://youtu.be/IYgZS2EvnLI"
            ],
            [
                "yt",
                "https://youtu.be/8ILk4Wjo5rc"
            ],
            [
                "doi",
                "https://doi.org/10.1038/s41586-022-05172-4",
                223
            ]
        ],
        "colab": "https://colab.research.google.com/github/deepmind/alphatensor/blob/master/nonequivalence/inspect_factorizations_notebook.ipynb",
        "update": 1664877948.0
    },
    {
        "name": "TAPIR",
        "description": "Tracking Any Point with per-frame Initialization and temporal Refinement",
        "author": [
            [
                "Carl Doersch",
                "http://www.carldoersch.com/"
            ],
            [
                "Yi Yang",
                "https://yangyi02.github.io/"
            ],
            [
                "Mel Vecerik",
                "https://scholar.google.com/citations?user=Jvi_XPAAAAAJ"
            ],
            [
                "Dilara Gokay",
                "https://scholar.google.com/citations?user=cnbENAEAAAAJ"
            ],
            [
                "Ankush Gupta",
                "https://ankushgupta.org/"
            ],
            [
                "Yusuf Aytar",
                "https://people.csail.mit.edu/yusuf/"
            ],
            [
                "Joao Carreira",
                "https://scholar.google.com/citations?user=IUZ-7_cAAAAJ"
            ],
            [
                "Andrew Zisserman",
                "https://www.robots.ox.ac.uk/~az/"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/google-deepmind/tapnet",
                1328
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2306.08637"
            ],
            [
                "blog post",
                "https://deepmind-tapir.github.io/"
            ],
            [
                "blog post",
                "https://deepmind-tapir.github.io/blogpost.html"
            ],
            [
                "git",
                "https://github.com/google-research/kubric/tree/main/challenges/point_tracking"
            ],
            [
                "deepmind",
                "https://www.deepmind.com/open-source/kinetics"
            ],
            [
                "yt",
                "https://youtu.be/2HSHofqoJ9M"
            ],
            [
                "medium",
                "https://medium.com/@jumabek4044/what-is-tapir-tracking-any-point-with-per-frame-initialization-and-temporal-refinement-and-how-it-bdad9946dc53"
            ],
            [
                "yt",
                "https://youtu.be/I1DQJH3v7Nk"
            ],
            [
                "neurips",
                "https://proceedings.neurips.cc/paper_files/paper/2022/hash/58168e8a92994655d6da3939e7cc0918-Abstract-Datasets_and_Benchmarks.html"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2308.15975"
            ]
        ],
        "colab": "https://colab.research.google.com/github/deepmind/tapnet/blob/master/colabs/causal_tapir_demo.ipynb",
        "update": 1732961522.0
    },
    {
        "name": "GraphCast",
        "description": "Learning skillful medium-range global weather forecasting",
        "author": [
            [
                "Rémi Lam",
                "https://github.com/remilam"
            ],
            [
                "Alvaro Sanchez-Gonzalez",
                "https://github.com/alvarosg"
            ],
            [
                "Matthew Willson",
                "https://github.com/mjwillson"
            ],
            [
                "Peter Wirnsberger",
                "https://pewi.org/"
            ],
            [
                "Meire Fortunato",
                "https://scholar.google.com/citations?user=_fMHSIUAAAAJ"
            ],
            [
                "Ferran Alet",
                "https://scholar.google.com/citations?user=1lmBq3QAAAAJ"
            ],
            [
                "Suman Ravuri",
                "https://www.linkedin.com/in/suman-ravuri-81928082"
            ],
            [
                "Timo Ewalds",
                "https://github.com/tewalds"
            ],
            [
                "Zach Eaton-Rosen",
                "https://scholar.google.com/citations?user=mQ3zD_wAAAAJ"
            ],
            [
                "Weihua Hu",
                "https://weihua916.github.io/"
            ],
            [
                "Alexander Merose",
                "https://alex.merose.com/"
            ],
            [
                "Stephan Hoyer",
                "https://stephanhoyer.com/"
            ],
            [
                "George Holland",
                "https://www.linkedin.com/in/g-aracil-holland"
            ],
            [
                "Oriol Vinyals",
                "https://research.google/people/oriol-vinyals/"
            ],
            [
                "Jacklynn Stott",
                "https://linkedin.com/in/jacklynnstott"
            ],
            [
                "Alexander Pritzel",
                "https://github.com/a-pritzel"
            ],
            [
                "Shakir Mohamed",
                "https://www.shakirm.com/"
            ],
            [
                "Peter Battaglia",
                "https://scholar.google.com/citations?user=nQ7Ij30AAAAJ"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/google-deepmind/graphcast",
                4729
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2212.12794"
            ],
            [
                "data",
                "https://www.ecmwf.int/en/forecasts/datasets/reanalysis-datasets/era5"
            ],
            [
                "git",
                "https://github.com/google-deepmind/chex"
            ],
            [
                "git",
                "https://github.com/dask/dask"
            ],
            [
                "git",
                "https://github.com/google-deepmind/jaxline"
            ],
            [
                "git",
                "https://github.com/google-deepmind/tree"
            ],
            [
                "git",
                "https://github.com/mikedh/trimesh"
            ],
            [
                "doi",
                "https://doi.org/10.1126/science.adi2336",
                208
            ],
            [
                "deepmind",
                "https://deepmind.google/discover/blog/graphcast-ai-model-for-faster-and-more-accurate-global-weather-forecasting/"
            ],
            [
                "medium",
                "https://towardsdatascience.com/graphcast-how-to-get-things-done-f2fd5630c5fb"
            ],
            [
                "yt",
                "https://youtu.be/BufUW7h9TB8"
            ],
            [
                "yt",
                "https://youtu.be/PD1v5PCJs_o"
            ],
            [
                "yt",
                "https://youtu.be/Eul-JN9Nwb0"
            ],
            [
                "yt",
                "https://youtu.be/BTyhgp9Hugc"
            ],
            [
                "yt",
                "https://youtu.be/aJ_H4exg0xU"
            ]
        ],
        "colab": "https://colab.research.google.com/github/deepmind/graphcast/blob/master/graphcast_demo.ipynb",
        "update": 1733321961.0
    },
    {
        "name": "Ithaca",
        "description": "First Deep Neural Network for the textual restoration, geographical and chronological attribution of ancient Greek inscriptions",
        "author": [
            [
                "Yannis Assael",
                "https://www.assael.gr/"
            ],
            [
                "Thea Sommerschield",
                "https://theasommerschield.it/"
            ],
            [
                "Brendan Shillingford",
                "https://github.com/bshillingford"
            ],
            [
                "Mahyar Bordbar",
                "https://scholar.google.com/citations?user=KB3ldSQAAAAJ"
            ],
            [
                "John Pavlopoulos",
                "https://ipavlopoulos.github.io/"
            ],
            [
                "Marita Chatzipanagiotou",
                "https://gr.linkedin.com/in/marita-chatzipanagiotou-b0611a1a2"
            ],
            [
                "Ion Androutsopoulos",
                "https://pages.aueb.gr/users/ion/"
            ],
            [
                "Jonathan Prag",
                "https://www.classics.ox.ac.uk/people/dr-jonathan-prag"
            ],
            [
                "Nando de Freitas",
                "https://www.cs.ox.ac.uk/people/nando.defreitas/"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/google-deepmind/ithaca",
                549
            ],
            [
                "doi",
                "https://doi.org/10.1038/s41586-022-04448-z",
                82
            ],
            [
                "project",
                "https://ithaca.deepmind.com/"
            ],
            [
                "git",
                "https://github.com/sommerschield/iphi"
            ],
            [
                "reddit",
                "https://www.reddit.com/r/MachineLearning/comments/tgeo0q/r_restoring_and_attributing_ancient_texts_using/"
            ],
            [
                "medium",
                "https://odsc.medium.com/deep-neural-networks-could-be-key-to-ancient-text-restoration-and-attribution-research-shows-81a2d89d9413"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/1910.06262"
            ],
            [
                "medium",
                "https://medium.com/syncedreview/ithaca-paper-published-in-nature-the-first-dnn-designed-for-textual-restoration-and-geographical-ef395d56697e"
            ]
        ],
        "colab": "https://colab.research.google.com/github/deepmind/ithaca/blob/master/colabs/ithaca_inference.ipynb",
        "update": 1700592971.0
    },
    {
        "name": "Neural Style Transfer",
        "description": "Implementation of Neural Style Transfer in Keras 2.0+",
        "author": [
            [
                "Somshubra Majumdar",
                "http://titu1994.github.io/"
            ]
        ],
        "links": [
            [
                "arxiv",
                "http://arxiv.org/abs/1508.06576"
            ],
            [
                "git",
                "https://github.com/titu1994/Neural-Style-Transfer",
                2276
            ],
            [
                "arxiv",
                "http://arxiv.org/abs/1605.04603"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/1606.05897"
            ],
            [
                "doi",
                "https://doi.org/10.1167/16.12.326",
                769
            ]
        ],
        "colab": "https://colab.research.google.com/github/titu1994/Neural-Style-Transfer/blob/master/NeuralStyleTransfer.ipynb",
        "update": 1611308406.0
    },
    {
        "name": "BERT score",
        "description": "An automatic evaluation metric for text generation",
        "author": [
            [
                "Tianyi Zhang",
                "https://tiiiger.github.io/"
            ]
        ],
        "links": [
            [
                "arxiv",
                "https://arxiv.org/abs/1904.09675"
            ],
            [
                "git",
                "https://github.com/Tiiiger/bert_score",
                1625
            ]
        ],
        "colab": "https://colab.research.google.com/github/Tiiiger/bert_score/blob/master/example/Demo.ipynb",
        "update": 1583434940.0
    },
    {
        "name": "SIREN",
        "description": "Implicit Neural Representations with Periodic Activation Functions",
        "author": [
            [
                "Vincent Sitzmann",
                "https://vsitzmann.github.io/"
            ],
            [
                "Julien Martel",
                "http://web.stanford.edu/~jnmartel/"
            ]
        ],
        "links": [
            [
                "project",
                "https://vsitzmann.github.io/siren/"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2006.09661"
            ],
            [
                "git",
                "https://github.com/vsitzmann/siren",
                1773
            ],
            [
                "yt",
                "https://www.youtube.com/watch?v=Q2fLWGBeaiI"
            ],
            [
                "data",
                "https://drive.google.com/drive/folders/1_iq__37-hw7FJOEUK1tX7mdp8SKB368K"
            ],
            [
                "neurips",
                "https://proceedings.neurips.cc/paper/2020/hash/53c04118df112c13a8c34b38343b9c10-Abstract.html"
            ]
        ],
        "colab": "https://colab.research.google.com/github/vsitzmann/siren/blob/master/explore_siren.ipynb",
        "update": 1593038401.0
    },
    {
        "name": "Open-Unmix",
        "description": "A deep neural network reference implementation for music source separation, applicable for researchers, audio engineers and artists",
        "author": [
            [
                "Fabian-Robert Stöter",
                "http://faroit.com/"
            ],
            [
                "Antoine Liutkus",
                "https://github.com/aliutkus"
            ]
        ],
        "links": [
            [
                "doi",
                "https://doi.org/10.21105/joss.01667",
                142
            ],
            [
                "project",
                "https://sigsep.github.io/open-unmix/"
            ],
            [
                "git",
                "https://github.com/sigsep/open-unmix-pytorch",
                1290
            ],
            [
                "data",
                "https://sigsep.github.io/datasets/musdb.html#musdb18-compressed-stems"
            ],
            [
                "git",
                "https://github.com/sigsep/norbert"
            ],
            [
                "pwc",
                "https://paperswithcode.com/sota/music-source-separation-on-musdb18?p=open-unmix-a-reference-implementation-for"
            ],
            [
                "yt",
                "https://www.youtube.com/playlist?list=PLhA3b2k8R3t0VpYCpCTU2B1h604rvnV4N"
            ]
        ],
        "colab": "https://colab.research.google.com/drive/1mijF0zGWxN-KaxTnd0q6hayAlrID5fEQ",
        "update": 1727257686.773
    },
    {
        "name": "LaSAFT",
        "description": "Latent Source Attentive Frequency Transformation for Conditioned Source Separation",
        "author": [
            [
                "Woosung Choi",
                "https://ws-choi.github.io/"
            ]
        ],
        "links": [
            [
                "project",
                "https://lasaft.github.io/"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2010.11631"
            ],
            [
                "git",
                "https://github.com/ws-choi/Conditioned-Source-Separation-LaSAFT",
                85
            ],
            [
                "data",
                "https://sigsep.github.io/datasets/musdb.html"
            ],
            [
                "doi",
                "https://doi.org/10.1109/ICASSP39728.2021.9413896",
                19
            ]
        ],
        "colab": "https://colab.research.google.com/github/ws-choi/Conditioned-Source-Separation-LaSAFT/blob/master/colab_demo/LaSAFT_with_GPoCM_Stella_Jang_Example.ipynb",
        "update": 1604247416.0
    },
    {
        "name": "GLIDE",
        "description": "Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models",
        "author": [
            [
                "Alex Nichol",
                "https://aqnichol.com/"
            ],
            [
                "Prafulla Dhariwal",
                "https://github.com/prafullasd"
            ],
            [
                "Aditya Ramesh",
                "http://adityaramesh.com/"
            ],
            [
                "Pranav Shyam",
                "https://github.com/pranv"
            ],
            [
                "Pamela Mishkin",
                "https://manlikemishap.github.io/"
            ],
            [
                "Bob McGrew",
                "https://github.com/bmcgrew"
            ],
            [
                "Ilya Sutskever",
                "http://www.cs.utoronto.ca/~ilya/"
            ],
            [
                "Mark Chen",
                "https://scholar.google.com/citations?user=5fU-QMwAAAAJ"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/openai/glide-text2im",
                3558
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2112.10741"
            ],
            [
                "yt",
                "https://youtu.be/ItKi3h7IY2o"
            ]
        ],
        "colab": "https://colab.research.google.com/github/openai/glide-text2im/blob/master/notebooks/inpaint.ipynb",
        "update": 1640137579.0
    },
    {
        "name": "CLIP",
        "description": "A neural network which efficiently learns visual concepts from natural language supervision",
        "author": [
            [
                "Jong Wook Kim",
                "https://jongwook.kim/"
            ],
            [
                "Alec Radford",
                "http://newmu.github.io/"
            ],
            [
                "Ilya Sutskever",
                "http://www.cs.utoronto.ca/~ilya/"
            ]
        ],
        "links": [
            [
                "project",
                "https://openai.com/blog/clip/"
            ],
            [
                "git",
                "https://github.com/openai/CLIP",
                26311
            ],
            [
                "paper",
                "https://cdn.openai.com/papers/Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf"
            ],
            [
                "data",
                "https://www.cs.toronto.edu/~kriz/cifar.html"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2103.00020"
            ],
            [
                "slides",
                "https://icml.cc/media/icml-2021/Slides/9193.pdf"
            ]
        ],
        "colab": "https://colab.research.google.com/github/openai/clip/blob/master/Interacting_with_CLIP.ipynb",
        "update": 1611933723.0
    },
    {
        "name": "Whisper",
        "description": "Automatic speech recognition system trained on 680,000 hours of multilingual and multitask supervised data collected from the web",
        "author": [
            [
                "Alec Radford",
                "http://newmu.github.io/"
            ],
            [
                "Jong Wook Kim",
                "https://jongwook.kim/"
            ],
            [
                "Tao Xu",
                "https://github.com/bayesian"
            ],
            [
                "Greg Brockman",
                "https://gregbrockman.com/"
            ],
            [
                "Christine McLeavey",
                "http://christinemcleavey.com/"
            ],
            [
                "Ilya Sutskever",
                "http://www.cs.toronto.edu/~ilya/"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/openai/whisper",
                72292
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2212.04356"
            ],
            [
                "blog post",
                "https://openai.com/research/whisper"
            ],
            [
                "git",
                "https://github.com/kkroening/ffmpeg-python"
            ],
            [
                "yt",
                "https://youtu.be/OCBZtgQGt1I"
            ],
            [
                "yt",
                "https://youtu.be/8SQV-B83tPU"
            ],
            [
                "yt",
                "https://youtu.be/nE5iVtwKerA"
            ]
        ],
        "colab": "https://colab.research.google.com/github/openai/whisper/blob/master/notebooks/LibriSpeech.ipynb",
        "update": 1663776583.0
    },
    {
        "name": "AvatarCLIP",
        "description": "A zero-shot text-driven framework for 3D avatar generation and animation",
        "author": [
            [
                "Fangzhou Hong",
                "https://hongfz16.github.io/"
            ],
            [
                "Mingyuan Zhang",
                "https://scholar.google.com/citations?user=2QLD4fAAAAAJ"
            ],
            [
                "Liang Pan",
                "https://scholar.google.com/citations?user=lSDISOcAAAAJ"
            ],
            [
                "Zhongang Cai",
                "https://caizhongang.github.io/"
            ],
            [
                "Lei Yang",
                "https://scholar.google.com/citations?user=jZH2IPYAAAAJ"
            ],
            [
                "Ziwei Liu",
                "https://liuziwei7.github.io/"
            ]
        ],
        "links": [
            [
                "project",
                "https://hongfz16.github.io/projects/AvatarCLIP.html"
            ],
            [
                "git",
                "https://github.com/hongfz16/AvatarCLIP",
                1072
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2205.08535"
            ],
            [
                "yt",
                "https://youtu.be/-l2ZMeoASGY"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2112.01455"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2112.03221"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2112.05139"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2203.13333"
            ],
            [
                "git",
                "https://github.com/daniilidis-group/neural_renderer"
            ],
            [
                "data",
                "https://www.di.ens.fr/willow/research/surreal/data/"
            ],
            [
                "git",
                "https://github.com/GuyTevet/MotionCLIP"
            ],
            [
                "git",
                "https://github.com/Totoro97/NeuS"
            ],
            [
                "git",
                "https://github.com/vchoutas/smplx"
            ],
            [
                "git",
                "https://github.com/nghorbani/human_body_prior"
            ],
            [
                "doi",
                "https://doi.org/10.1145/3528223.3530094",
                121
            ]
        ],
        "colab": "https://colab.research.google.com/drive/1dfaecX7xF3nP6fyXc8XBljV5QY1lc1TR",
        "update": 1652625609.406
    },
    {
        "name": "CLIPasso",
        "description": "Semantically-Aware Object Sketching",
        "author": [
            [
                "Yael Vinker",
                "https://yaelvi116.wixsite.com/mysite"
            ],
            [
                "Ehsan Pajouheshgar",
                "https://pajouheshgar.github.io/"
            ],
            [
                "Jessica Y. Bo",
                "https://jessica-bo.github.io/"
            ],
            [
                "Roman Bachmann",
                "https://roman-bachmann.github.io/"
            ],
            [
                "Amit Bermano",
                "https://www.cs.tau.ac.il/~amberman/"
            ],
            [
                "Daniel Cohen-Or",
                "https://danielcohenor.com/"
            ],
            [
                "Amir Zamir",
                "https://vilab.epfl.ch/zamir/"
            ],
            [
                "Ariel Shamir",
                "https://faculty.runi.ac.il/arik/site/index.asp"
            ]
        ],
        "links": [
            [
                "project",
                "https://clipasso.github.io/clipasso/"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2202.05822"
            ],
            [
                "demo",
                "https://replicate.com/yael-vinker/clipasso"
            ],
            [
                "git",
                "https://github.com/yael-vinker/CLIPasso",
                856
            ],
            [
                "git",
                "https://github.com/BachiLi/diffvg"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2106.14843"
            ]
        ],
        "colab": "https://colab.research.google.com/github/yael-vinker/CLIPasso/blob/main/CLIPasso.ipynb",
        "update": 1647874524.0
    },
    {
        "name": "DA-CLIP",
        "description": "Degradation-aware vision-language model to better transfer pretrained vision-language models to low-level vision tasks as a universal framework for image restoration",
        "author": [
            [
                "Ziwei Luo",
                "https://algolzw.github.io/"
            ],
            [
                "Fredrik Gustafsson",
                "http://www.fregu856.com/"
            ],
            [
                "Zheng Zhao",
                "https://zz.zabemon.com/"
            ],
            [
                "Jens Sjölund",
                "https://github.com/jsjol"
            ],
            [
                "Thomas Schön",
                "https://user.it.uu.se/~thosc112/index.html"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/Algolzw/daclip-uir",
                670
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2310.01018"
            ],
            [
                "project",
                "https://algolzw.github.io/daclip-uir/"
            ],
            [
                "hf",
                "https://huggingface.co/weblzw/daclip-uir-ViT-B-32-irsde"
            ],
            [
                "git",
                "https://github.com/Algolzw/image-restoration-sde"
            ]
        ],
        "colab": "https://colab.research.google.com/github/camenduru/daclip-uir-colab/blob/main/daclip_uir_gradio_colab.ipynb",
        "update": 1697055123.0
    },
    {
        "name": "ClipCap",
        "description": "CLIP Prefix for Image Captioning",
        "author": [
            [
                "Ron Mokady",
                "https://rmokady.github.io/"
            ],
            [
                "Amir Hertz",
                "https://github.com/amirhertz"
            ],
            [
                "Amit Bermano",
                "https://www.cs.tau.ac.il/~amberman/"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/rmokady/CLIP_prefix_caption",
                1323
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2111.09734"
            ],
            [
                "hf",
                "https://huggingface.co/spaces/akhaliq/CLIP_prefix_captioning"
            ],
            [
                "data",
                "https://cocodataset.org/"
            ],
            [
                "medium",
                "https://medium.com/@uppalamukesh/clipcap-clip-prefix-for-image-captioning-3970c73573bc"
            ],
            [
                "yt",
                "https://youtu.be/VQDrmuccWDo"
            ]
        ],
        "colab": "https://colab.research.google.com/github/rmokady/CLIP_prefix_caption/blob/main/notebooks/clip_prefix_captioning_inference.ipynb#scrollTo=glBzYsgIwhwF",
        "update": 1644913172.0
    },
    {
        "name": "MotionDiffuse",
        "description": "The first diffusion model-based text-driven motion generation framework, which demonstrates several desired properties over existing methods",
        "author": [
            [
                "Mingyuan Zhang",
                "https://mingyuan-zhang.github.io/"
            ],
            [
                "Zhongang Cai",
                "https://caizhongang.github.io/"
            ],
            [
                "Liang Pan",
                "https://github.com/paul007pl"
            ],
            [
                "Fangzhou Hong",
                "https://hongfz16.github.io/"
            ],
            [
                "Xinying Guo",
                "https://gxyes.github.io/"
            ],
            [
                "Lei Yang",
                "https://scholar.google.com/citations?user=jZH2IPYAAAAJ"
            ],
            [
                "Ziwei Liu",
                "https://liuziwei7.github.io/"
            ]
        ],
        "links": [
            [
                "project",
                "https://mingyuan-zhang.github.io/projects/MotionDiffuse.html"
            ],
            [
                "git",
                "https://github.com/mingyuan-zhang/MotionDiffuse",
                865
            ],
            [
                "yt",
                "https://youtu.be/U5PTnw490SA"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2208.15001"
            ],
            [
                "hf",
                "https://huggingface.co/spaces/mingyuan/MotionDiffuse"
            ]
        ],
        "colab": "https://colab.research.google.com/drive/1Dp6VsZp2ozKuu9ccMmsDjyij_vXfCYb3",
        "update": 1665675834.251
    },
    {
        "name": "SMPLer-X",
        "description": "Scaling up EHPS towards the first generalist foundation model, with up to ViT-Huge as the backbone and training with up to 4.5M instances from diverse data sources",
        "author": [
            [
                "Zhongang Cai",
                "https://caizhongang.github.io/"
            ],
            [
                "Wanqi Yin",
                "https://scholar.google.com/citations?user=zlIJwBEAAAAJ"
            ],
            [
                "Ailing Zeng",
                "https://ailingzeng.site/"
            ],
            [
                "Chen Wei",
                "https://github.com/Wei-Chen-hub"
            ],
            [
                "Qingping Sun",
                "https://github.com/ttxskk"
            ],
            [
                "Yanjun Wang",
                "https://github.com/WYJSJTU"
            ],
            [
                "Hui En Pang",
                "https://pangyyyyy.github.io/"
            ],
            [
                "Haiyi Mei",
                "https://haiyi-mei.com/"
            ],
            [
                "Mingyuan Zhang",
                "https://mingyuan-zhang.github.io/"
            ],
            [
                "Lei Zhang",
                "https://www.leizhang.org/"
            ],
            [
                "Chen Change Loy",
                "https://www.mmlab-ntu.com/person/ccloy/"
            ],
            [
                "Lei Yang",
                "https://scholar.google.com/citations?user=jZH2IPYAAAAJ"
            ],
            [
                "Ziwei Liu",
                "https://liuziwei7.github.io/"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/caizhongang/SMPLer-X",
                1023
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2309.17448"
            ],
            [
                "project",
                "https://caizhongang.com/projects/SMPLer-X/"
            ],
            [
                "yt",
                "https://youtu.be/DepTqbPpVzY"
            ],
            [
                "git",
                "https://github.com/open-mmlab/mmhuman3d/blob/main/docs/human_data.md"
            ],
            [
                "git",
                "https://github.com/mks0601/Hand4Whole_RELEASE"
            ],
            [
                "git",
                "https://github.com/IDEA-Research/OSX"
            ],
            [
                "yt",
                "https://youtu.be/aFTGFInUnM4"
            ],
            [
                "neurips",
                "https://neurips.cc/virtual/2023/poster/73473"
            ],
            [
                "reddit",
                "https://www.reddit.com/r/machinelearningnews/comments/176c5z7/this_ai_research_proposes_smplerx_a_generalist/"
            ]
        ],
        "colab": "https://colab.research.google.com/github/camenduru/SMPLer-X-colab/blob/main/SMPLer_X_colab.ipynb",
        "update": 1702929479.0
    },
    {
        "name": "HiDT",
        "description": "A generative image-to-image model and a new upsampling scheme that allows to apply image translation at high resolution",
        "author": [
            [
                "Denis Korzhenkov",
                "https://github.com/denkorzh"
            ],
            [
                "Gleb Sterkin",
                "https://github.com/belkakari"
            ],
            [
                "Sergey Nikolenko",
                "https://logic.pdmi.ras.ru/~sergey/"
            ],
            [
                "Victor Lempitsky",
                "http://sites.skoltech.ru/compvision/members/vilem/"
            ]
        ],
        "links": [
            [
                "project",
                "https://saic-mdal.github.io/HiDT/"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2003.08791"
            ],
            [
                "yt",
                "https://www.youtube.com/playlist?list=PLuvGzlEQXT1KQuKrfBBEWh2f3PToxyeM5"
            ],
            [
                "git",
                "https://github.com/saic-mdal/HiDT",
                4
            ],
            [
                "yt",
                "https://www.youtube.com/watch?v=EWKAgwgqXB4"
            ],
            [
                "doi",
                "https://doi.org/10.1109/CVPR42600.2020.00751",
                49
            ]
        ],
        "colab": "https://colab.research.google.com/github/saic-mdal/hidt/blob/master/notebooks/HighResolutionDaytimeTranslation.ipynb",
        "update": 1690188190.0
    },
    {
        "name": "Wav2Lip",
        "description": "A Lip Sync Expert Is All You Need for Speech to Lip Generation In the Wild",
        "author": [
            [
                "Prajwal Renukanand",
                "https://github.com/prajwalkr"
            ],
            [
                "Rudrabha Mukhopadhyay",
                "https://rudrabha.github.io/"
            ],
            [
                "Vinay Namboodiri",
                "https://vinaypn.github.io/"
            ],
            [
                "C. V. Jawahar",
                "https://faculty.iiit.ac.in/~jawahar/"
            ]
        ],
        "links": [
            [
                "project",
                "http://cvit.iiit.ac.in/research/projects/cvit-projects/a-lip-sync-expert-is-all-you-need-for-speech-to-lip-generation-in-the-wild/"
            ],
            [
                "demo",
                "http://bhaasha.iiit.ac.in/lipsync/"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2008.10010"
            ],
            [
                "git",
                "https://github.com/Rudrabha/Wav2Lip",
                10912
            ],
            [
                "yt",
                "https://www.youtube.com/watch?v=0fXaDCZNOJc"
            ],
            [
                "data",
                "https://www.robots.ox.ac.uk/~vgg/data/lip_reading/lrs2.html"
            ],
            [
                "doi",
                "https://doi.org/10.1145/3394171.3413532",
                391
            ]
        ],
        "colab": "https://colab.research.google.com/github/eyaler/avatars4all/blob/master/melaflefon.ipynb",
        "update": 1719505421.0
    },
    {
        "name": "MSG-Net",
        "description": "Multi-style Generative Network with a novel Inspiration Layer, which retains the functionality of optimization-based approaches and has the fast speed of feed-forward networks",
        "author": [
            [
                "Hang Zhang",
                "https://hangzhang.org/"
            ],
            [
                "Kristin Dana",
                "https://www.ece.rutgers.edu/~kdana/dana.html"
            ]
        ],
        "links": [
            [
                "project",
                "http://computervisionrutgers.github.io/MSG-Net/"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/1703.06953"
            ],
            [
                "yt",
                "https://www.youtube.com/watch?v=oy6pWNWBt4Y"
            ],
            [
                "doi",
                "https://doi.org/10.1007/978-3-030-11018-5_32",
                67
            ]
        ],
        "colab": "https://colab.research.google.com/github/zhanghang1989/PyTorch-Multi-Style-Transfer/blob/master/msgnet.ipynb",
        "update": 1611607121.0
    },
    {
        "name": "SkyAR",
        "description": "A vision-based method for video sky replacement and harmonization, which can automatically generate realistic and dramatic sky backgrounds in videos with controllable styles",
        "author": [
            [
                "Zhengxia Zou",
                "http://www-personal.umich.edu/~zzhengxi/"
            ]
        ],
        "links": [
            [
                "project",
                "https://jiupinjia.github.io/skyar/"
            ],
            [
                "git",
                "https://github.com/jiupinjia/SkyAR",
                2010
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2010.11800"
            ],
            [
                "yt",
                "https://www.youtube.com/watch?v=zal9Ues0aOQ"
            ],
            [
                "doi",
                "https://doi.org/10.1109/TIP.2022.3192717",
                9
            ]
        ],
        "colab": "https://colab.research.google.com/github/jiupinjia/SkyAR/blob/master/colab_demo.ipynb",
        "update": 1610977367.0
    },
    {
        "name": "Neural Magic Eye",
        "description": "Learning to See and Understand the Scene Behind an Autostereogram",
        "author": [
            [
                "Zhengxia Zou",
                "http://www-personal.umich.edu/~zzhengxi/"
            ],
            [
                "Tianyang Shi",
                "https://www.shitianyang.tech/"
            ],
            [
                "Yi Yuan",
                "https://yiyuan1991.github.io/"
            ],
            [
                "Zhenwei Shi",
                "http://levir.buaa.edu.cn/"
            ]
        ],
        "links": [
            [
                "project",
                "https://jiupinjia.github.io/neuralmagiceye/"
            ],
            [
                "git",
                "https://github.com/jiupinjia/neural-magic-eye",
                72
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2012.15692"
            ],
            [
                "yt",
                "https://www.youtube.com/watch?v=Fkh7DEblqJ8"
            ]
        ],
        "colab": "https://colab.research.google.com/drive/1f59dFLJ748i2TleE54RkbUZSMo9Hyx7l",
        "update": 1609465943.743
    },
    {
        "name": "Stylized Neural Painting",
        "description": "An image-to-painting translation method that generates vivid and realistic painting artworks with controllable styles",
        "author": [
            [
                "Zhengxia Zou",
                "http://www-personal.umich.edu/~zzhengxi/"
            ],
            [
                "Tianyang Shi",
                "https://www.shitianyang.tech/"
            ],
            [
                "Yi Yuan",
                "https://yiyuan1991.github.io/"
            ],
            [
                "Zhenwei Shi",
                "http://levir.buaa.edu.cn/"
            ]
        ],
        "links": [
            [
                "project",
                "https://jiupinjia.github.io/neuralpainter/"
            ],
            [
                "git",
                "https://github.com/jiupinjia/stylized-neural-painting",
                1571
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2011.08114"
            ],
            [
                "yt",
                "https://www.youtube.com/watch?v=oerb-nwrXhk"
            ],
            [
                "doi",
                "https://doi.org/10.1109/CVPR46437.2021.01543",
                54
            ]
        ],
        "colab": "https://colab.research.google.com/drive/1ch_41GtcQNQT1NLOA21vQJ_rQOjjv9D8",
        "update": 1606835403.466
    },
    {
        "name": "Generating Piano Music with Transformer",
        "description": "This Colab notebook lets you play with pretrained Transformer models for piano music generation, based on the Music Transformer",
        "author": [
            [
                "Ian Simon",
                "https://github.com/iansimon"
            ],
            [
                "Anna Huang",
                "https://github.com/czhuang"
            ],
            [
                "Jesse Engel",
                "https://github.com/jesseengel"
            ],
            [
                "Curtis Hawthorne",
                "https://github.com/cghawthorne"
            ]
        ],
        "links": [
            [
                "arxiv",
                "https://arxiv.org/abs/1706.03762"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/1809.04281"
            ],
            [
                "blog post",
                "http://g.co/magenta/music-transformer"
            ]
        ],
        "colab": "https://colab.research.google.com/notebooks/magenta/piano_transformer/piano_transformer.ipynb",
        "update": 1568584800.0
    },
    {
        "name": "GrooVAE",
        "description": "Some applications of machine learning for generating and manipulating beats and drum performances",
        "author": [
            [
                "Jon Gillick",
                "https://www.jongillick.com/"
            ],
            [
                "Adam Roberts",
                "https://github.com/adarob"
            ],
            [
                "Jesse Engel",
                "https://github.com/jesseengel"
            ]
        ],
        "links": [
            [
                "arxiv",
                "https://arxiv.org/abs/1905.06118"
            ],
            [
                "blog post",
                "https://g.co/magenta/groovae"
            ],
            [
                "data",
                "https://g.co/magenta/groove-datasets"
            ],
            [
                "yt",
                "https://www.youtube.com/watch?v=x2YLmXzovDo"
            ],
            [
                "git",
                "https://github.com/magenta/magenta/tree/main/magenta/models/music_vae",
                19216
            ],
            [
                "web app",
                "https://groove-drums.glitch.me/"
            ]
        ],
        "colab": "https://colab.research.google.com/github/tensorflow/magenta-demos/blob/master/colab-notebooks/GrooVAE.ipynb",
        "update": 1675294830.0
    },
    {
        "name": "Multitrack MusicVAE",
        "description": "The models in this notebook are capable of encoding and decoding single measures of up to 8 tracks, optionally conditioned on an underlying chord",
        "author": [
            [
                "Ian Simon",
                "https://github.com/iansimon"
            ],
            [
                "Adam Roberts",
                "https://github.com/adarob"
            ],
            [
                "Colin Raffel",
                "https://colinraffel.com//"
            ],
            [
                "Jesse Engel",
                "https://github.com/jesseengel"
            ],
            [
                "Curtis Hawthorne",
                "https://github.com/cghawthorne"
            ],
            [
                "Douglas Eck",
                "https://github.com/douglaseck"
            ]
        ],
        "links": [
            [
                "arxiv",
                "https://arxiv.org/abs/1806.00195"
            ],
            [
                "blog post",
                "http://g.co/magenta/multitrack"
            ]
        ],
        "colab": "https://colab.research.google.com/github/magenta/magenta-demos/blob/master/colab-notebooks/Multitrack_MusicVAE.ipynb",
        "update": 1675294830.0
    },
    {
        "name": "MusicVAE",
        "description": "A Hierarchical Latent Vector Model for Learning Long-Term Structure in Music",
        "author": [
            [
                "Adam Roberts",
                "https://github.com/adarob"
            ],
            [
                "Jesse Engel",
                "https://github.com/jesseengel"
            ],
            [
                "Colin Raffel",
                "https://colinraffel.com//"
            ],
            [
                "Curtis Hawthorne",
                "https://github.com/cghawthorne"
            ],
            [
                "Douglas Eck",
                "https://github.com/douglaseck"
            ]
        ],
        "links": [
            [
                "arxiv",
                "https://arxiv.org/abs/1803.05428"
            ],
            [
                "blog post",
                "https://g.co/magenta/music-vae"
            ],
            [
                "yt",
                "https://www.youtube.com/playlist?list=PLBUMAYA6kvGU8Cgqh709o5SUvo-zHGTxr"
            ],
            [
                "project",
                "https://magenta.tensorflow.org/music-vae"
            ]
        ],
        "colab": "https://colab.research.google.com/github/magenta/magenta-demos/blob/master/colab-notebooks/MusicVAE.ipynb",
        "update": 1675294830.0
    },
    {
        "name": "MusicXML Documentation",
        "description": "The goal of this notebook is to explore one of the magenta libraries for music",
        "author": [
            [
                "Prakruti Joshi",
                "https://github.com/prakruti-joshi"
            ],
            [
                "Falak Shah",
                "https://falaktheoptimist.github.io/"
            ],
            [
                "Twisha Naik",
                "https://github.com/twisha96"
            ]
        ],
        "links": [
            [
                "musicXML",
                "https://www.musicxml.com/for-developers/"
            ],
            [
                "magenta",
                "https://magenta.tensorflow.org/"
            ],
            [
                "music theory",
                "http://musictheoryblog.blogspot.com/2008/02/learn-music-theory.html"
            ]
        ],
        "colab": "https://colab.research.google.com/github/magenta/magenta-demos/blob/master/colab-notebooks/MusicXML_Document_Structure_Documentation.ipynb",
        "update": 1610121997.0
    },
    {
        "name": "SVG VAE",
        "description": "A colab demo for the SVG VAE model",
        "author": [
            [
                "Raphael Gontijo Lopes",
                "https://raphagl.com/"
            ]
        ],
        "links": [
            [
                "arxiv",
                "https://arxiv.org/abs/1904.02632"
            ],
            [
                "blog post",
                "https://magenta.tensorflow.org/svg-vae"
            ],
            [
                "doi",
                "https://doi.org/10.1109/ICCV.2019.00802",
                57
            ]
        ],
        "colab": "https://colab.research.google.com/github/magenta/magenta-demos/blob/master/colab-notebooks/vae_svg_decoding.ipynb",
        "update": 1610121997.0
    },
    {
        "name": "Latent Constraints",
        "description": "Conditional Generation from Unconditional Generative Models",
        "author": [
            [
                "Jesse Engel",
                "https://github.com/jesseengel"
            ],
            [
                "Matthew Hoffman",
                "http://matthewdhoffman.com/"
            ],
            [
                "Adam Roberts",
                "https://github.com/adarob"
            ]
        ],
        "links": [
            [
                "arxiv",
                "https://arxiv.org/abs/1711.05772"
            ],
            [
                "data",
                "http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html"
            ]
        ],
        "colab": "https://colab.research.google.com/notebooks/latent_constraints/latentconstraints.ipynb",
        "update": 1511737200.0
    },
    {
        "name": "Performance RNN",
        "description": "This notebook shows you how to generate new performed compositions from a trained model",
        "author": [
            [
                "Ian Simon",
                "https://github.com/iansimon"
            ],
            [
                "Sageev Oore",
                "https://github.com/osageev"
            ],
            [
                "Curtis Hawthorne",
                "https://github.com/cghawthorne"
            ]
        ],
        "links": [
            [
                "blog post",
                "https://magenta.tensorflow.org/performance-rnn"
            ],
            [
                "data",
                "http://www.piano-e-competition.com/"
            ],
            [
                "git",
                "https://github.com/magenta/magenta/tree/master/magenta/models/performance_rnn",
                19216
            ]
        ],
        "colab": "https://colab.research.google.com/notebooks/magenta/performance_rnn/performance_rnn.ipynb",
        "update": 1499724000.0
    },
    {
        "name": "Onsets and Frames",
        "description": "Onsets and Frames is an automatic music transcription framework with piano and drums models",
        "author": [
            [
                "Curtis Hawthorne",
                "https://github.com/cghawthorne"
            ],
            [
                "Erich Elsen",
                "https://github.com/ekelsen"
            ]
        ],
        "links": [
            [
                "arxiv",
                "https://arxiv.org/abs/1710.11153"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/1810.12247"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2004.00188"
            ],
            [
                "blog post",
                "http://g.co/magenta/onsets-frames"
            ],
            [
                "data",
                "https://g.co/magenta/maestro-wave2midi2wave"
            ],
            [
                "data",
                "https://magenta.tensorflow.org/datasets/e-gmd"
            ],
            [
                "git",
                "https://github.com/magenta/magenta/tree/main/magenta/models/onsets_frames_transcription",
                19216
            ]
        ],
        "colab": "https://colab.research.google.com/notebooks/magenta/onsets_frames_transcription/onsets_frames_transcription.ipynb",
        "update": 1585778400.0
    },
    {
        "name": "GANSynth",
        "description": "This notebook is a demo GANSynth, which generates audio with Generative Adversarial Networks",
        "author": [
            [
                "Jesse Engel",
                "https://github.com/jesseengel"
            ]
        ],
        "links": [
            [
                "arxiv",
                "https://arxiv.org/abs/1902.08710"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/1809.11096"
            ],
            [
                "project",
                "https://storage.googleapis.com/magentadata/papers/gansynth/index.html"
            ],
            [
                "git",
                "https://github.com/magenta/magenta/tree/main/magenta/models/gansynth",
                19216
            ]
        ],
        "colab": "https://colab.research.google.com/notebooks/magenta/gansynth/gansynth_demo.ipynb",
        "update": 1551049200.0
    },
    {
        "name": "NSynth",
        "description": "This colab notebook has everything you need to upload your own sounds and use NSynth models to reconstruct and interpolate between them",
        "author": [
            [
                "Jesse Engel",
                "https://github.com/jesseengel"
            ],
            [
                "Cinjon Resnick",
                "https://github.com/cinjon"
            ],
            [
                "Adam Roberts",
                "https://github.com/adarob"
            ],
            [
                "Sander Dieleman",
                "https://benanne.github.io/"
            ],
            [
                "Karen Simonyan",
                "https://scholar.google.com/citations?user=L7lMQkQAAAAJ"
            ],
            [
                "Mohammad Norouzi",
                "https://norouzi.github.io/"
            ],
            [
                "Douglas Eck",
                "https://github.com/douglaseck"
            ]
        ],
        "links": [
            [
                "arxiv",
                "https://arxiv.org/abs/1704.01279"
            ],
            [
                "blog post",
                "https://magenta.tensorflow.org/nsynth"
            ],
            [
                "git",
                "https://github.com/tensorflow/magenta/tree/master/magenta/models/nsynth",
                19216
            ],
            [
                "data",
                "https://magenta.tensorflow.org/datasets/nsynth"
            ],
            [
                "yt",
                "https://www.youtube.com/watch?v=AaALLWQmCdI"
            ],
            [
                "yt",
                "https://www.youtube.com/watch?v=BOoSy-Pg8is"
            ],
            [
                "tutorial",
                "https://magenta.tensorflow.org/nsynth-fastgen"
            ]
        ],
        "colab": "https://colab.research.google.com/notebooks/magenta/nsynth/nsynth.ipynb",
        "update": 1491429600.0
    },
    {
        "name": "Big GAN",
        "description": "Large Scale GAN Training for High Fidelity Natural Image Synthesis",
        "author": [
            [
                "Andrew Brock",
                "https://github.com/ajbrock"
            ],
            [
                "Jeff Donahue",
                "https://jeffdonahue.com/"
            ],
            [
                "Karen Simonyan",
                "https://scholar.google.com/citations?user=L7lMQkQAAAAJ"
            ]
        ],
        "links": [
            [
                "arxiv",
                "https://arxiv.org/abs/1809.11096"
            ]
        ],
        "colab": "https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/biggan_generation_with_tf_hub.ipynb",
        "update": 1691098225.0
    },
    {
        "name": "Pixel2Style2Pixel",
        "description": "Encoding in Style: A StyleGAN Encoder for Image-to-Image Translation",
        "author": [
            [
                "Elad Richardson",
                "https://github.com/eladrich"
            ],
            [
                "Yuval Alaluf",
                "https://yuval-alaluf.github.io/"
            ],
            [
                "Yotam Nitzan",
                "https://yotamnitzan.github.io/"
            ],
            [
                "Daniel Cohen-Or",
                "https://danielcohenor.com/"
            ]
        ],
        "links": [
            [
                "project",
                "https://eladrich.github.io/pixel2style2pixel/"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2008.00951"
            ],
            [
                "git",
                "https://github.com/eladrich/pixel2style2pixel",
                3199
            ],
            [
                "git",
                "https://github.com/rosinality/stylegan2-pytorch"
            ],
            [
                "git",
                "https://github.com/HuangYG123/CurricularFace"
            ],
            [
                "yt",
                "https://youtu.be/bfvSwhqsTgM"
            ],
            [
                "doi",
                "https://doi.org/10.1109/CVPR46437.2021.00232",
                652
            ]
        ],
        "colab": "https://colab.research.google.com/github/eladrich/pixel2style2pixel/blob/master/notebooks/inference_playground.ipynb",
        "update": 1622564684.0
    },
    {
        "name": "ReStyle",
        "description": "A Residual-Based StyleGAN Encoder via Iterative Refinement",
        "author": [
            [
                "Yuval Alaluf",
                "https://yuval-alaluf.github.io/"
            ],
            [
                "Or Patashnik",
                "https://orpatashnik.github.io/"
            ],
            [
                "Daniel Cohen-Or",
                "https://danielcohenor.com/"
            ]
        ],
        "links": [
            [
                "project",
                "https://yuval-alaluf.github.io/restyle-encoder/"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2104.02699"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2008.00951"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2102.02766"
            ],
            [
                "git",
                "https://github.com/yuval-alaluf/restyle-encoder",
                1034
            ],
            [
                "git",
                "https://github.com/rosinality/stylegan2-pytorch"
            ],
            [
                "git",
                "https://github.com/TreB1eN/InsightFace_Pytorch"
            ],
            [
                "doi",
                "https://doi.org/10.1109/ICCV48922.2021.00664",
                210
            ]
        ],
        "colab": "https://colab.research.google.com/github/yuval-alaluf/restyle-encoder/blob/master/notebooks/inference_playground.ipynb",
        "update": 1621625007.0
    },
    {
        "name": "encoder4editing",
        "description": "Designing an Encoder for StyleGAN Image Manipulation",
        "author": [
            [
                "Omer Tov",
                "https://github.com/omertov"
            ],
            [
                "Yuval Alaluf",
                "https://yuval-alaluf.github.io/"
            ],
            [
                "Yotam Nitzan",
                "https://yotamnitzan.github.io/"
            ],
            [
                "Or Patashnik",
                "https://orpatashnik.github.io/"
            ],
            [
                "Daniel Cohen-Or",
                "https://danielcohenor.com/"
            ]
        ],
        "links": [
            [
                "arxiv",
                "https://arxiv.org/abs/2102.02766"
            ],
            [
                "git",
                "https://github.com/omertov/encoder4editing",
                945
            ],
            [
                "git",
                "https://github.com/eladrich/pixel2style2pixel"
            ],
            [
                "doi",
                "https://doi.org/10.1145/3450626.3459838",
                401
            ]
        ],
        "colab": "https://colab.research.google.com/github/omertov/encoder4editing/blob/master/notebooks/inference_playground.ipynb",
        "update": 1638462049.0
    },
    {
        "name": "SAM",
        "description": "Age Transformation Using a Style-Based Regression Model",
        "author": [
            [
                "Yuval Alaluf",
                "https://yuval-alaluf.github.io/"
            ],
            [
                "Or Patashnik",
                "https://orpatashnik.github.io/"
            ],
            [
                "Daniel Cohen-Or",
                "https://danielcohenor.com/"
            ]
        ],
        "links": [
            [
                "project",
                "https://yuval-alaluf.github.io/SAM/"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2102.02754"
            ],
            [
                "yt",
                "https://youtu.be/X_pYC_LtBFw"
            ],
            [
                "git",
                "https://github.com/yuval-alaluf/SAM",
                662
            ],
            [
                "git",
                "https://github.com/eladrich/pixel2style2pixel"
            ],
            [
                "git",
                "https://github.com/rosinality/stylegan2-pytorch"
            ],
            [
                "doi",
                "https://doi.org/10.1145/3450626.3459805",
                84
            ]
        ],
        "colab": "http://colab.research.google.com/github/yuval-alaluf/SAM/blob/master/notebooks/animation_inference_playground.ipynb",
        "update": 1619421369.0
    },
    {
        "name": "HyperStyle",
        "description": "A hypernetwork that learns to modulate StyleGAN's weights to faithfully express a given image in editable regions of the latent space",
        "author": [
            [
                "Yuval Alaluf",
                "https://yuval-alaluf.github.io/"
            ],
            [
                "Omer Tov",
                "https://github.com/omertov"
            ],
            [
                "Ron Mokady",
                "https://rmokady.github.io/"
            ],
            [
                "Rinon Gal",
                "https://rinongal.github.io/"
            ],
            [
                "Amit Bermano",
                "https://www.cs.tau.ac.il/~amberman/"
            ]
        ],
        "links": [
            [
                "project",
                "https://yuval-alaluf.github.io/hyperstyle/"
            ],
            [
                "git",
                "https://github.com/yuval-alaluf/hyperstyle",
                1006
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2111.15666"
            ],
            [
                "yt",
                "https://youtu.be/_sbXmLY2jMw"
            ],
            [
                "git",
                "https://github.com/NVlabs/ffhq-dataset"
            ],
            [
                "data",
                "https://ai.stanford.edu/~jkrause/cars/car_dataset.html"
            ],
            [
                "git",
                "https://github.com/clovaai/stargan-v2"
            ],
            [
                "git",
                "https://github.com/rosinality/stylegan2-pytorch"
            ],
            [
                "git",
                "https://github.com/TreB1eN/InsightFace_Pytorch"
            ],
            [
                "git",
                "https://github.com/HuangYG123/CurricularFace"
            ],
            [
                "git",
                "https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/1904.03189"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2012.09036"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2005.07727"
            ],
            [
                "git",
                "https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py"
            ],
            [
                "git",
                "https://github.com/dvschultz/stylegan2-ada-pytorch"
            ],
            [
                "doi",
                "https://doi.org/10.1109/CVPR52688.2022.01796",
                140
            ]
        ],
        "colab": "https://colab.research.google.com/github/yuval-alaluf/hyperstyle/blob/master/notebooks/inference_playground.ipynb",
        "update": 1638516493.0
    },
    {
        "name": "MobileSAM",
        "description": "Towards Lightweight SAM for Mobile Applications",
        "author": [
            [
                "Chaoning Zhang",
                "https://github.com/ChaoningZhang"
            ],
            [
                "Dongshen Han",
                "https://github.com/dongshenhan"
            ],
            [
                "Yu Qiao",
                "https://github.com/qiaoyu1002"
            ],
            [
                "Jung Uk Kim",
                "https://visualai.khu.ac.kr/"
            ],
            [
                "Sung-Ho Bae",
                "https://scholar.google.com/citations?user=EULut5oAAAAJ"
            ],
            [
                "Seungkyu Lee",
                "https://scholar.google.com/citations?user=3Pf6C6cAAAAJ"
            ],
            [
                "Choong Seon Hong",
                "https://scholar.google.com/citations?user=oKANWloAAAAJ"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/ChaoningZhang/MobileSAM",
                4873
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2306.14289"
            ],
            [
                "git",
                "https://github.com/jolibrain/joliGEN"
            ],
            [
                "git",
                "https://github.com/akbartus/MobileSAM-in-the-Browser"
            ],
            [
                "git",
                "https://github.com/qiaoyu1002/Inpaint-Anything"
            ],
            [
                "twitter",
                "https://twitter.com/_akhaliq/status/1674410573075718145"
            ],
            [
                "git",
                "https://github.com/qiaoyu1002/Personalize-SAM"
            ],
            [
                "git",
                "https://github.com/Jumpat/SegmentAnythingin3D"
            ],
            [
                "git",
                "https://github.com/vietanhdev/anylabeling"
            ],
            [
                "git",
                "https://github.com/wangsssky/SonarSAM"
            ],
            [
                "git",
                "https://github.com/continue-revolution/sd-webui-segment-anything"
            ],
            [
                "yt",
                "https://youtu.be/eTEfq_kWabQ"
            ]
        ],
        "colab": "https://colab.research.google.com/github/ChaoningZhang/MobileSAM/blob/master/notebooks/predictor_example.ipynb",
        "update": 1688093852.0
    },
    {
        "name": "Talking Head Anime from a Single Image",
        "description": "The network takes as input an image of an anime character's face and a desired pose, and it outputs another image of the same character in the given pose",
        "author": [
            [
                "Pramook Khungurn",
                "https://pkhungurn.github.io/"
            ]
        ],
        "links": [
            [
                "project",
                "https://pkhungurn.github.io/talking-head-anime/"
            ],
            [
                "yt",
                "https://youtu.be/kMQCERkTdO0"
            ],
            [
                "yt",
                "https://youtu.be/T1Gp-RxFZwU"
            ],
            [
                "yt",
                "https://youtu.be/FioRJ6x_RbI"
            ],
            [
                "git",
                "https://github.com/pkhungurn/talking-head-anime-demo",
                2000
            ],
            [
                "git",
                "https://github.com/lincolnhard/head-pose-estimation"
            ],
            [
                "wiki",
                "https://en.wikipedia.org/wiki/Virtual_YouTuber"
            ],
            [
                "wiki",
                "https://en.wikipedia.org/wiki/MikuMikuDance"
            ]
        ],
        "colab": "https://colab.research.google.com/github/pkhungurn/talking-head-anime-demo/blob/master/tha_colab.ipynb",
        "update": 1614116307.0
    },
    {
        "name": "AnimeGANv2",
        "description": "An improved version of AnimeGAN - it prevents the generation of high-frequency artifacts by simply changing the normalization of features in the network",
        "author": [
            [
                "Xin Chen",
                "https://github.com/TachibanaYoshino"
            ],
            [
                "Gang Liu",
                "https://github.com/lg0061408"
            ],
            [
                "bryandlee",
                "https://github.com/bryandlee"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/bryandlee/animegan2-pytorch",
                4405
            ],
            [
                "git",
                "https://github.com/TachibanaYoshino/AnimeGANv2"
            ],
            [
                "git",
                "https://github.com/TachibanaYoshino/AnimeGAN"
            ],
            [
                "project",
                "https://tachibanayoshino.github.io/AnimeGANv2/"
            ],
            [
                "hf",
                "https://huggingface.co/spaces/akhaliq/AnimeGANv2"
            ],
            [
                "doi",
                "https://doi.org/10.1007/978-981-15-5577-0_18",
                56
            ]
        ],
        "colab": "https://colab.research.google.com/github/bryandlee/animegan2-pytorch/blob/master/colab_demo.ipynb",
        "update": 1637114941.0
    },
    {
        "name": "AnimeGANv3",
        "description": "Double-tail generative adversarial network for fast photo animation",
        "author": [
            [
                "Gang Liu",
                "https://github.com/lg0061408"
            ],
            [
                "Xin Chen",
                "https://github.com/TachibanaYoshino"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/TachibanaYoshino/AnimeGANv3",
                1756
            ],
            [
                "project",
                "https://tachibanayoshino.github.io/AnimeGANv3/"
            ],
            [
                "yt",
                "https://youtu.be/EosubeJmAnE"
            ],
            [
                "yt",
                "https://youtu.be/5qLUflWb45E"
            ],
            [
                "yt",
                "https://youtu.be/iFjiaPlhVm4"
            ],
            [
                "yt",
                "https://youtu.be/vJqQQMRYKh0"
            ],
            [
                "yt",
                "https://youtu.be/0KaScDxgyBw"
            ],
            [
                "yt",
                "https://youtu.be/6WXhjXb5a-o"
            ],
            [
                "doi",
                "http://doi.org/10.1587/transinf.2023EDP7061",
                1
            ]
        ],
        "colab": "https://colab.research.google.com/drive/1XYNWwM8Xq-U7KaTOqNap6A-Yq1f-V-FB",
        "update": 1700720822.566
    },
    {
        "name": "NeRViS",
        "description": "An algorithm for full-frame video stabilization by first estimating dense warp fields",
        "author": [
            [
                "Yu-Lun Liu",
                "http://www.cmlab.csie.ntu.edu.tw/~yulunliu/"
            ],
            [
                "Wei-Sheng Lai",
                "https://www.wslai.net/"
            ],
            [
                "Ming-Hsuan Yang",
                "https://faculty.ucmerced.edu/mhyang/"
            ],
            [
                "Yung-Yu Chuang",
                "https://www.csie.ntu.edu.tw/~cyy/"
            ],
            [
                "Jia-Bin Huang",
                "https://jbhuang0604.github.io/"
            ]
        ],
        "links": [
            [
                "project",
                "https://alex04072000.github.io/NeRViS/"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2102.06205"
            ],
            [
                "yt",
                "https://youtu.be/KO3sULs4hso"
            ],
            [
                "git",
                "https://github.com/alex04072000/NeRViS",
                13
            ],
            [
                "git",
                "https://github.com/cxjyxxme/deep-online-video-stabilization"
            ],
            [
                "git",
                "https://github.com/jinsc37/DIFRINT"
            ],
            [
                "data",
                "http://liushuaicheng.org/SIGGRAPH2013/database.html"
            ],
            [
                "doi",
                "https://doi.org/10.1109/ICCV48922.2021.00230",
                30
            ]
        ],
        "colab": "https://colab.research.google.com/drive/1l-fUzyM38KJMZyKMBWw_vu7ZUyDwgdYH",
        "update": 1618125726.731
    },
    {
        "name": "StyleGAN-NADA",
        "description": "Zero-Shot non-adversarial domain adaptation of pre-trained generators",
        "author": [
            [
                "Rinon Gal",
                "https://rinongal.github.io/"
            ],
            [
                "Or Patashnik",
                "https://orpatashnik.github.io/"
            ],
            [
                "Haggai Maron",
                "https://haggaim.github.io/"
            ],
            [
                "Gal Chechik",
                "https://research.nvidia.com/person/gal-chechik"
            ],
            [
                "Daniel Cohen-Or",
                "https://danielcohenor.com/"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/rinongal/StyleGAN-nada",
                1157
            ],
            [
                "git",
                "https://github.com/rosinality/stylegan2-pytorch/"
            ],
            [
                "git",
                "https://github.com/NVlabs/stylegan2-ada"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2108.00946"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2103.17249"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2104.02699"
            ],
            [
                "project",
                "https://stylegan-nada.github.io/"
            ],
            [
                "doi",
                "https://doi.org/10.1145/3528223.3530164",
                266
            ]
        ],
        "colab": "https://colab.research.google.com/github/rinongal/stylegan-nada/blob/main/stylegan_nada.ipynb",
        "update": 1660072354.0
    },
    {
        "name": "textual-inversion",
        "description": "An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion",
        "author": [
            [
                "Rinon Gal",
                "https://rinongal.github.io/"
            ],
            [
                "Yuval Alaluf",
                "https://yuval-alaluf.github.io/"
            ],
            [
                "Yuval Atzmon",
                "https://research.nvidia.com/person/yuval-atzmon"
            ],
            [
                "Or Patashnik",
                "https://orpatashnik.github.io/"
            ],
            [
                "Amit Bermano",
                "https://www.cs.tau.ac.il/~amberman/"
            ],
            [
                "Gal Chechik",
                "https://research.nvidia.com/person/gal-chechik"
            ],
            [
                "Daniel Cohen-Or",
                "https://danielcohenor.com/"
            ]
        ],
        "links": [
            [
                "project",
                "https://textual-inversion.github.io/"
            ],
            [
                "git",
                "https://github.com/rinongal/textual_inversion",
                2933
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2208.01618"
            ],
            [
                "yt",
                "https://youtu.be/f3oXa7_SYek"
            ],
            [
                "yt",
                "https://youtu.be/opD_H9bED9Y"
            ]
        ],
        "colab": "https://colab.research.google.com/github/rinongal/textual_inversion/blob/master/scripts/latent_imagenet_diffusion.ipynb",
        "update": 1661112343.0
    },
    {
        "name": "StyleCLIP",
        "description": "Text-Driven Manipulation of StyleGAN Imager",
        "author": [
            [
                "Or Patashnik",
                "https://orpatashnik.github.io/"
            ],
            [
                "Zongze Wu",
                "https://github.com/betterze"
            ],
            [
                "Eli Shechtman",
                "https://research.adobe.com/person/eli-shechtman/"
            ],
            [
                "Daniel Cohen-Or",
                "https://danielcohenor.com/"
            ],
            [
                "Dani Lischinski",
                "https://pages.cs.huji.ac.il/danix/"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/orpatashnik/StyleCLIP",
                4016
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2103.17249"
            ],
            [
                "yt",
                "https://youtu.be/5icI0NgALnQ"
            ],
            [
                "yt",
                "https://youtu.be/PhR1gpXDu0w"
            ],
            [
                "yt",
                "https://youtu.be/d1OET63Ulwc"
            ],
            [
                "yt",
                "https://youtu.be/RAXrwPskNso"
            ],
            [
                "git",
                "https://github.com/rosinality/stylegan2-pytorch/"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2011.12799"
            ],
            [
                "doi",
                "https://doi.org/10.1109/ICCV48922.2021.00209",
                581
            ]
        ],
        "colab": "https://colab.research.google.com/github/orpatashnik/StyleCLIP/blob/main/notebooks/StyleCLIP_global_torch.ipynb",
        "update": 1667170587.0
    },
    {
        "name": "Parallel WaveGAN",
        "description": "State-of-the-art non-autoregressive models to build your own great vocoder",
        "author": [
            [
                "Tomoki Hayashi",
                "https://kan-bayashi.github.io/"
            ]
        ],
        "links": [
            [
                "demo",
                "https://kan-bayashi.github.io/ParallelWaveGAN/"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/1910.11480"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/1910.06711"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2005.05106"
            ],
            [
                "git",
                "https://github.com/kan-bayashi/ParallelWaveGAN",
                1575
            ],
            [
                "git",
                "https://github.com/NVIDIA/tacotron2"
            ],
            [
                "git",
                "https://github.com/espnet/espnet"
            ],
            [
                "doi",
                "https://doi.org/10.1109/ICASSP40776.2020.9053795",
                351
            ]
        ],
        "colab": "https://colab.research.google.com/github/espnet/notebook/blob/master/espnet2_tts_realtime_demo.ipynb",
        "update": 1685600659.0
    },
    {
        "name": "SimSwap",
        "description": "An efficient framework, called Simple Swap, aiming for generalized and high fidelity face swapping",
        "author": [
            [
                "Xuanhong Chen",
                "https://github.com/neuralchen"
            ],
            [
                "Bingbing Ni",
                "https://scholar.google.com/citations?user=eUbmKwYAAAAJ"
            ],
            [
                "Yanhao Ge",
                "https://scholar.google.com/citations?user=h6tuBAcAAAAJ"
            ]
        ],
        "links": [
            [
                "arxiv",
                "https://arxiv.org/abs/2106.06340"
            ],
            [
                "git",
                "https://github.com/neuralchen/SimSwap",
                4577
            ],
            [
                "git",
                "https://github.com/deepinsight/insightface"
            ],
            [
                "doi",
                "https://doi.org/10.1145/3394171.3413630",
                174
            ]
        ],
        "colab": "https://colab.research.google.com/github/neuralchen/SimSwap/blob/master/SimSwap%20colab.ipynb",
        "update": 1637752744.0
    },
    {
        "name": "TediGAN",
        "description": "Framework for multi-modal image generation and manipulation with textual descriptions",
        "author": [
            [
                "Weihao Xia",
                "https://github.com/weihaox"
            ],
            [
                "Yujiu Yang",
                "http://www.fiesta.tsinghua.edu.cn/pi/3/24"
            ],
            [
                "Jing-Hao Xue",
                "http://www.homepages.ucl.ac.uk/~ucakjxu/"
            ],
            [
                "Baoyuan Wu",
                "https://sites.google.com/site/baoyuanwu2015/home"
            ]
        ],
        "links": [
            [
                "arxiv",
                "https://arxiv.org/abs/2012.03308"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2104.08910"
            ],
            [
                "yt",
                "https://youtu.be/L8Na2f5viAM"
            ],
            [
                "git",
                "https://github.com/IIGROUP/TediGAN",
                378
            ],
            [
                "git",
                "https://github.com/weihaox/Multi-Modal-CelebA-HQ"
            ],
            [
                "git",
                "https://github.com/NVlabs/ffhq-dataset"
            ],
            [
                "git",
                "https://github.com/rosinality/stylegan2-pytorch/"
            ],
            [
                "git",
                "https://github.com/fyu/lsun"
            ],
            [
                "doi",
                "https://doi.org/10.1109/CVPR46437.2021.00229",
                215
            ]
        ],
        "colab": "http://colab.research.google.com/github/weihaox/TediGAN/blob/master/playground.ipynb",
        "update": 1625063305.0
    },
    {
        "name": "NeX",
        "description": "View synthesis based on enhancements of multiplane image that can reproduce NeXt-level view-dependent effects in real time",
        "author": [
            [
                "Suttisak Wizadwongsa",
                "https://www.linkedin.com/in/suttisak-wizadwongsa-763a931a5/"
            ],
            [
                "Pakkapon Phongthawee",
                "http://pureexe.github.io/"
            ],
            [
                "Jiraphon Yenphraphai",
                "https://www.linkedin.com/in/jiraphon-yenphraphai-990ba6175/"
            ],
            [
                "Supasorn Suwajanakorn",
                "https://www.supasorn.com/"
            ]
        ],
        "links": [
            [
                "project",
                "https://nex-mpi.github.io/"
            ],
            [
                "yt",
                "https://www.youtube.com/watch?v=HyfkF7Z-ddA"
            ],
            [
                "git",
                "https://github.com/nex-mpi/nex-code",
                597
            ],
            [
                "git",
                "https://github.com/Fyusion/LLFF"
            ],
            [
                "data",
                "https://vistec-my.sharepoint.com/personal/pakkapon_p_s19_vistec_ac_th/_layouts/15/onedrive.aspx?id=%2Fpersonal%2Fpakkapon%5Fp%5Fs19%5Fvistec%5Fac%5Fth%2FDocuments%2Fpublic%2FVLL%2FNeX%2Fshiny%5Fdatasets&originalPath=aHR0cHM6Ly92aXN0ZWMtbXkuc2hhcmVwb2ludC5jb20vOmY6L2cvcGVyc29uYWwvcGFra2Fwb25fcF9zMTlfdmlzdGVjX2FjX3RoL0VuSVVoc1JWSk9kTnNaXzRzbWRoeWUwQjh6MFZseHFPUjM1SVIzYnAwdUd1cFE%5FcnRpbWU9WXRVQTQtQTcyVWc"
            ],
            [
                "data",
                "https://vistec-my.sharepoint.com/personal/pakkapon_p_s19_vistec_ac_th/_layouts/15/onedrive.aspx?originalPath=aHR0cHM6Ly92aXN0ZWMtbXkuc2hhcmVwb2ludC5jb20vOmY6L2cvcGVyc29uYWwvcGFra2Fwb25fcF9zMTlfdmlzdGVjX2FjX3RoL0VyalBSUkw5Sm5GSXA4TU42ZDFqRXVvQjNYVm94SmtmZlBqZm9QeWhIa2owZGc%5FcnRpbWU9bC0yYWctRTcyVWc&id=%2Fpersonal%2Fpakkapon%5Fp%5Fs19%5Fvistec%5Fac%5Fth%2FDocuments%2Fpublic%2FVLL%2FNeX%2Fmodified%5Fdataset"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2103.05606"
            ],
            [
                "vistec",
                "https://vistec.ist/"
            ],
            [
                "doi",
                "https://doi.org/10.1109/CVPR46437.2021.00843",
                165
            ]
        ],
        "colab": "https://colab.research.google.com/drive/1hXVvYdAwLA0EFg2zrafJUE0bFgB_F7PU",
        "update": 1616692728.707
    },
    {
        "name": "Anycost GAN",
        "description": "Interactive natural image editing",
        "author": [
            [
                "Ji Lin",
                "http://linji.me/"
            ],
            [
                "Richard Zhang",
                "https://richzhang.github.io/"
            ],
            [
                "Frieder Ganz",
                "https://scholar.google.com/citations?user=u9ySZkUAAAAJ"
            ],
            [
                "Song Han",
                "https://songhan.mit.edu/"
            ],
            [
                "Jun-Yan Zhu",
                "https://www.cs.cmu.edu/~junyanz/"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/mit-han-lab/anycost-gan",
                777
            ],
            [
                "project",
                "https://hanlab.mit.edu/projects/anycost-gan/"
            ],
            [
                "yt",
                "https://www.youtube.com/watch?v=_yEziPl9AkM"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2103.03243"
            ],
            [
                "git",
                "https://github.com/NVlabs/stylegan2"
            ],
            [
                "git",
                "https://github.com/rosinality/stylegan2-pytorch"
            ],
            [
                "git",
                "https://github.com/NVlabs/ffhq-dataset"
            ],
            [
                "git",
                "https://github.com/switchablenorms/CelebAMask-HQ"
            ],
            [
                "git",
                "https://github.com/fyu/lsun"
            ],
            [
                "doi",
                "https://doi.org/10.1109/CVPR46437.2021.01474",
                49
            ]
        ],
        "colab": "https://colab.research.google.com/github/mit-han-lab/anycost-gan/blob/master/notebooks/intro_colab.ipynb",
        "update": 1658353304.0
    },
    {
        "name": "Rewriting a Deep Generative Model",
        "description": "We ask if a deep network can be reprogrammed to follow different rules, by enabling a user to directly change the weights, instead of training with a data set",
        "author": [
            [
                "David Bau",
                "https://people.csail.mit.edu/davidbau/home/"
            ],
            [
                "Steven Liu",
                "http://people.csail.mit.edu/stevenliu/"
            ],
            [
                "Tongzhou Wang",
                "https://ssnl.github.io/"
            ],
            [
                "Jun-Yan Zhu",
                "https://www.cs.cmu.edu/~junyanz/"
            ],
            [
                "Antonio Torralba",
                "https://groups.csail.mit.edu/vision/torralbalab/"
            ]
        ],
        "links": [
            [
                "arxiv",
                "https://arxiv.org/abs/2007.15646"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/1912.04958"
            ],
            [
                "yt",
                "https://www.youtube.com/watch?v=i2_-zNqtEPk"
            ],
            [
                "yt",
                "https://rewriting.csail.mit.edu/video/"
            ],
            [
                "project",
                "https://rewriting.csail.mit.edu/"
            ],
            [
                "git",
                "https://github.com/davidbau/rewriting",
                537
            ],
            [
                "git",
                "https://github.com/NVlabs/stylegan2"
            ],
            [
                "git",
                "https://github.com/rosinality/stylegan2-pytorch"
            ],
            [
                "doi",
                "https://doi.org/10.1007/978-3-030-58452-8_21",
                45
            ]
        ],
        "colab": "https://colab.research.google.com/github/davidbau/rewriting/blob/master/notebooks/rewriting-interface.ipynb",
        "update": 1596232932.0
    },
    {
        "name": "SeFa",
        "description": "A closed-form approach for unsupervised latent semantic factorization in GANs",
        "author": [
            [
                "Yujun Shen",
                "https://shenyujun.github.io/"
            ],
            [
                "Bolei Zhou",
                "https://boleizhou.github.io/"
            ]
        ],
        "links": [
            [
                "project",
                "https://genforce.github.io/sefa/"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2007.06600"
            ],
            [
                "git",
                "https://github.com/genforce/sefa",
                964
            ],
            [
                "yt",
                "https://www.youtube.com/watch?v=OFHW2WbXXIQ"
            ],
            [
                "doi",
                "https://doi.org/10.1109/CVPR46437.2021.00158",
                306
            ]
        ],
        "colab": "https://colab.research.google.com/github/genforce/sefa/blob/master/docs/SeFa.ipynb",
        "update": 1607220242.0
    },
    {
        "name": "HiGAN",
        "description": "Semantic Hierarchy Emerges in Deep Generative Representations for Scene Synthesis",
        "author": [
            [
                "Ceyuan Yang",
                "https://ceyuan.me/"
            ],
            [
                "Yujun Shen",
                "https://shenyujun.github.io/"
            ],
            [
                "Bolei Zhou",
                "https://boleizhou.github.io/"
            ]
        ],
        "links": [
            [
                "project",
                "https://genforce.github.io/higan/"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/1911.09267"
            ],
            [
                "git",
                "https://github.com/genforce/higan",
                162
            ],
            [
                "yt",
                "https://www.youtube.com/watch?v=X5yWu2Jwjpg"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/1412.6856"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/1906.10112"
            ],
            [
                "doi",
                "https://doi.org/10.1007/s11263-020-01429-5",
                101
            ]
        ],
        "colab": "https://colab.research.google.com/github/genforce/higan/blob/master/docs/HiGAN_Bedroom.ipynb",
        "update": 1602651962.0
    },
    {
        "name": "InterFaceGAN",
        "description": "Interpreting the Latent Space of GANs for Semantic Face Editing",
        "author": [
            [
                "Yujun Shen",
                "https://shenyujun.github.io/"
            ],
            [
                "Jinjin Gu",
                "https://www.jasongt.com/"
            ],
            [
                "Xiaoou Tang",
                "https://www.ie.cuhk.edu.hk/people/xotang.shtml"
            ],
            [
                "Bolei Zhou",
                "https://boleizhou.github.io/"
            ]
        ],
        "links": [
            [
                "project",
                "https://genforce.github.io/interfacegan/"
            ],
            [
                "git",
                "https://github.com/genforce/interfacegan",
                1509
            ],
            [
                "yt",
                "https://www.youtube.com/watch?v=uoftpl3Bj6w"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/1907.10786"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2005.09635"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/1710.10196"
            ],
            [
                "git",
                "https://github.com/tkarras/progressive_growing_of_gans"
            ],
            [
                "git",
                "https://github.com/NVlabs/stylegan"
            ],
            [
                "doi",
                "https://doi.org/10.1109/CVPR42600.2020.00926",
                605
            ]
        ],
        "colab": "https://colab.research.google.com/github/genforce/interfacegan/blob/master/docs/InterFaceGAN.ipynb",
        "update": 1602571096.0
    },
    {
        "name": "Disentangled Lifespan Face Synthesis",
        "description": "LFS model is proposed to disentangle the key face characteristics including shape, texture and identity so that the unique shape and texture age transformations can be modeled effectively",
        "author": [
            [
                "Sen He",
                "https://senhe.github.io/"
            ],
            [
                "Wentong Liao",
                "https://www.tnt.uni-hannover.de/en/staff/liao/"
            ],
            [
                "Michael Yang",
                "https://sites.google.com/site/michaelyingyang/"
            ],
            [
                "Yi-Zhe Song",
                "http://personal.ee.surrey.ac.uk/Personal/Y.Song/"
            ],
            [
                "Bodo Rosenhahn",
                "https://scholar.google.com/citations?user=qq3TxtcAAAAJ"
            ],
            [
                "Tao Xiang",
                "http://personal.ee.surrey.ac.uk/Personal/T.Xiang/index.html"
            ]
        ],
        "links": [
            [
                "project",
                "https://senhe.github.io/projects/iccv_2021_lifespan_face/"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2108.02874"
            ],
            [
                "git",
                "https://github.com/SenHe/DLFS",
                67
            ],
            [
                "yt",
                "https://www.youtube.com/watch?v=uklX03ns0m0"
            ]
        ],
        "colab": "https://colab.research.google.com/drive/1fgVAoxCSaqPkj0rUK4RmBh7GTQRqLNpE",
        "update": 1645544461.162
    },
    {
        "name": "Lifespan Age Transformation Synthesis",
        "description": "Multi-domain image-to-image generative adversarial network architecture, whose learned latent space models a continuous bi-directional aging process",
        "author": [
            [
                "Roy Or-El",
                "https://homes.cs.washington.edu/~royorel/"
            ],
            [
                "Soumyadip Sengupta",
                "https://homes.cs.washington.edu/~soumya91/"
            ],
            [
                "Ohad Fried",
                "https://www.ohadf.com/"
            ],
            [
                "Eli Shechtman",
                "https://research.adobe.com/person/eli-shechtman/"
            ],
            [
                "Ira Kemelmacher-Shlizerman",
                "https://www.irakemelmacher.com/"
            ]
        ],
        "links": [
            [
                "project",
                "https://grail.cs.washington.edu/projects/lifespan_age_transformation_synthesis/"
            ],
            [
                "yt",
                "https://youtu.be/_jTFcjN2hBk"
            ],
            [
                "yt",
                "https://youtu.be/9fulnt2_q_Y"
            ],
            [
                "git",
                "https://github.com/royorel/Lifespan_Age_Transformation_Synthesis",
                582
            ],
            [
                "git",
                "https://github.com/royorel/FFHQ-Aging-Dataset"
            ],
            [
                "git",
                "https://github.com/NVIDIA/pix2pixHD"
            ],
            [
                "git",
                "https://github.com/rosinality/style-based-gan-pytorch"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2003.09764"
            ],
            [
                "doi",
                "https://doi.org/10.1007/978-3-030-58539-6_44",
                63
            ]
        ],
        "colab": "https://colab.research.google.com/github/royorel/Lifespan_Age_Transformation_Synthesis/blob/master/LATS_demo.ipynb",
        "update": 1604172028.0
    },
    {
        "name": "Real-ESRGAN",
        "description": "Extend the powerful ESRGAN to a practical restoration application, which is trained with pure synthetic data",
        "author": [
            [
                "Xintao Wang",
                "https://xinntao.github.io/"
            ],
            [
                "Liangbin Xie",
                "https://liangbinxie.github.io/"
            ],
            [
                "Chao Dong",
                "https://scholar.google.com/citations?user=OSDCB0UAAAAJ"
            ],
            [
                "Ying Shan",
                "https://scholar.google.com/citations?user=4oXBp9UAAAAJ"
            ]
        ],
        "links": [
            [
                "arxiv",
                "https://arxiv.org/abs/2107.10833"
            ],
            [
                "git",
                "https://github.com/xinntao/Real-ESRGAN",
                28671
            ],
            [
                "git",
                "https://github.com/xinntao/ESRGAN"
            ],
            [
                "git",
                "https://github.com/xinntao/facexlib"
            ],
            [
                "git",
                "https://github.com/xinntao/HandyView"
            ],
            [
                "git",
                "https://github.com/Tencent/ncnn"
            ],
            [
                "git",
                "https://github.com/nihui/waifu2x-ncnn-vulkan"
            ],
            [
                "doi",
                "https://doi.org/10.1109/ICCVW54120.2021.00217",
                643
            ]
        ],
        "colab": "https://colab.research.google.com/drive/1k2Zod6kSHEvraybHl50Lys0LerhyTMCo",
        "update": 1663524144.647
    },
    {
        "name": "GFPGAN",
        "description": "Towards Real-World Blind Face Restoration with Generative Facial Prior",
        "author": [
            [
                "Xintao Wang",
                "https://xinntao.github.io/"
            ],
            [
                "Yu Li",
                "https://yu-li.github.io/"
            ],
            [
                "Honglun Zhang",
                "https://scholar.google.com/citations?user=KjQLROoAAAAJ"
            ],
            [
                "Ying Shan",
                "https://scholar.google.com/citations?user=4oXBp9UAAAAJ"
            ]
        ],
        "links": [
            [
                "project",
                "https://xinntao.github.io/projects/gfpgan"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2101.04061"
            ],
            [
                "git",
                "https://github.com/TencentARC/GFPGAN",
                35961
            ],
            [
                "git",
                "https://github.com/xinntao/facexlib"
            ],
            [
                "git",
                "https://github.com/xinntao/HandyView"
            ],
            [
                "git",
                "https://github.com/NVlabs/ffhq-dataset"
            ],
            [
                "doi",
                "https://doi.org/10.1109/CVPR46437.2021.00905",
                284
            ]
        ],
        "colab": "https://colab.research.google.com/drive/1sVsoBd9AjckIXThgtZhGrHRfFI6UUYOo",
        "update": 1657678412.056
    },
    {
        "name": "PhotoMaker",
        "description": "Efficient personalized text-to-image generation method, which mainly encodes an arbitrary number of input ID images into a stack ID embedding for preserving ID information",
        "author": [
            [
                "Zhen Li",
                "https://paper99.github.io/"
            ],
            [
                "Mingdeng Cao",
                "https://github.com/ljzycmd"
            ],
            [
                "Xintao Wang",
                "https://xinntao.github.io/"
            ],
            [
                "Zhongang Qi",
                "https://scholar.google.com/citations?user=zJvrrusAAAAJ"
            ],
            [
                "Ming-Ming Cheng",
                "https://mmcheng.net/cmm/"
            ],
            [
                "Ying Shan",
                "https://scholar.google.com/citations?user=4oXBp9UAAAAJ"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/TencentARC/PhotoMaker",
                9601
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2312.04461"
            ],
            [
                "project",
                "https://photo-maker.github.io/"
            ],
            [
                "hf",
                "https://huggingface.co/TencentARC/PhotoMaker"
            ],
            [
                "git",
                "https://github.com/bmaltais/PhotoMaker"
            ],
            [
                "git",
                "https://github.com/sdbds/PhotoMaker-for-windows"
            ],
            [
                "git",
                "https://github.com/ZHO-ZHO-ZHO/ComfyUI-PhotoMaker"
            ],
            [
                "git",
                "https://github.com/mit-han-lab/fastcomposer"
            ],
            [
                "git",
                "https://github.com/TencentARC/T2I-Adapter"
            ],
            [
                "git",
                "https://github.com/tencent-ailab/IP-Adapter"
            ],
            [
                "yt",
                "https://youtu.be/NWIdzTEk5O4"
            ],
            [
                "yt",
                "https://youtu.be/ZTck128jfFY"
            ],
            [
                "reddit",
                "https://www.reddit.com/r/StableDiffusion/comments/197bfj9/tencentarc_releases_photomaker/"
            ],
            [
                "medium",
                "https://medium.com/@christopheverdier/photomaker-the-art-of-ai-consistent-characters-generation-cf2cd037bc3e"
            ]
        ],
        "colab": "https://colab.research.google.com/github/TencentARC/PhotoMaker/blob/main/photomaker_demo.ipynb",
        "update": 1705566602.0
    },
    {
        "name": "Live Speech Portraits",
        "description": "Real-Time Photorealistic Talking-Head Animation",
        "author": [
            [
                "Yuanxun Lu",
                "https://github.com/YuanxunLu"
            ],
            [
                "Jinxiang Chai",
                "https://scholar.google.com/citations?user=OcN1_gwAAAAJ"
            ],
            [
                "Xun Cao",
                "https://cite.nju.edu.cn/People/Faculty/20190621/i5054.html"
            ]
        ],
        "links": [
            [
                "project",
                "https://yuanxunlu.github.io/projects/LiveSpeechPortraits/"
            ],
            [
                "git",
                "https://github.com/YuanxunLu/LiveSpeechPortraits",
                1217
            ],
            [
                "git",
                "https://github.com/lelechen63/ATVGnet"
            ],
            [
                "git",
                "https://github.com/lelechen63/Talking-head-Generation-with-Rhythmic-Head-Motion"
            ],
            [
                "git",
                "https://github.com/DinoMan/speech-driven-animation"
            ],
            [
                "git",
                "https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2109.10595"
            ],
            [
                "doi",
                "https://doi.org/10.1145/3478513.3480484",
                103
            ]
        ],
        "colab": "https://colab.research.google.com/drive/1tKvi-9kY3GkEK8lgtfTSM70rMFo_TY50",
        "update": 1632685360.817
    },
    {
        "name": "StyleCariGAN",
        "description": "Caricature Generation via StyleGAN Feature Map Modulation",
        "author": [
            [
                "Wonjong Jang",
                "https://wonjongg.github.io/"
            ],
            [
                "Gwangjin Ju",
                "https://github.com/jugwangjin"
            ],
            [
                "Yucheol Jung",
                "https://ycjung.info/"
            ],
            [
                "Jiaolong Yang",
                "https://jlyang.org/"
            ],
            [
                "Xin Tong",
                "https://www.microsoft.com/en-us/research/people/xtong/"
            ],
            [
                "Seungyong Lee",
                "https://scholar.google.com/citations?user=yGPH-nAAAAAJ"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/wonjongg/StyleCariGAN",
                294
            ],
            [
                "git",
                "https://github.com/NVlabs/stylegan2"
            ],
            [
                "git",
                "https://github.com/rosinality/stylegan2-pytorch"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2107.04331"
            ],
            [
                "project",
                "https://wonjongg.github.io/StyleCariGAN/"
            ],
            [
                "yt",
                "https://www.youtube.com/watch?v=kpHbGOlI-BU"
            ],
            [
                "doi",
                "https://doi.org/10.1145/3450626.3459860",
                36
            ]
        ],
        "colab": "https://colab.research.google.com/drive/1HDRQGm7pvC9mAb6Lktoft_SmY9sCq_Qg",
        "update": 1638304604.158
    },
    {
        "name": "SpecVQGAN",
        "description": "Taming the visually guided sound generation by shrinking a training dataset to a set of representative vectors",
        "author": [
            [
                "Vladimir Iashin",
                "https://iashin.ai/"
            ],
            [
                "Esa Rahtu",
                "https://esa.rahtu.fi/"
            ]
        ],
        "links": [
            [
                "project",
                "https://iashin.ai/SpecVQGAN"
            ],
            [
                "arxiv",
                "http://arxiv.org/abs/2110.08791"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2012.09841"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/1711.00937"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2008.00820"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/1712.01393"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/1512.08512"
            ],
            [
                "yt",
                "https://www.youtube.com/watch?v=Bucb3nAa398"
            ],
            [
                "git",
                "https://github.com/v-iashin/SpecVQGAN",
                353
            ],
            [
                "git",
                "https://github.com/PeihaoChen/regnet"
            ],
            [
                "git",
                "https://github.com/toshas/torch-fidelity"
            ],
            [
                "git",
                "https://github.com/descriptinc/melgan-neurips"
            ],
            [
                "git",
                "https://github.com/google/lyra"
            ],
            [
                "wiki",
                "https://en.wikipedia.org/wiki/Foley_(filmmaking)"
            ],
            [
                "wiki",
                "https://en.wikipedia.org/wiki/Row-_and_column-major_order"
            ],
            [
                "wiki",
                "https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence"
            ]
        ],
        "colab": "https://colab.research.google.com/drive/1pxTIMweAKApJZ3ZFqyBee3HtMqFpnwQ0",
        "update": 1720775693.365
    },
    {
        "name": "Instant-NGP",
        "description": "Instant Neural Graphics Primitives with a Multiresolution Hash Encoding",
        "author": [
            [
                "Thomas Müller",
                "https://tom94.net/"
            ],
            [
                "Alex Evans",
                "https://research.nvidia.com/person/alex-evans"
            ],
            [
                "Christoph Schied",
                "https://research.nvidia.com/person/christoph-schied"
            ],
            [
                "Alexander Keller",
                "https://research.nvidia.com/person/alex-keller"
            ]
        ],
        "links": [
            [
                "project",
                "https://nvlabs.github.io/instant-ngp/"
            ],
            [
                "blog post",
                "https://developer.nvidia.com/blog/getting-started-with-nvidia-instant-nerfs/"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2201.05989"
            ],
            [
                "tutorial",
                "https://www.nvidia.com/en-us/on-demand/session/siggraph2022-sigg22-s-16/"
            ],
            [
                "git",
                "https://github.com/NVlabs/instant-ngp",
                16081
            ],
            [
                "git",
                "https://github.com/NVlabs/tiny-cuda-nn"
            ],
            [
                "git",
                "https://github.com/IDLabMedia/large-lightfields-dataset"
            ],
            [
                "git",
                "https://github.com/nickponline/dd-nerf-dataset"
            ],
            [
                "yt",
                "https://youtu.be/j8tMk-GE8hY"
            ],
            [
                "yt",
                "https://youtu.be/8GbENSmdVeE"
            ],
            [
                "yt",
                "https://youtu.be/DJ2hcC1orc4"
            ],
            [
                "yt",
                "https://youtu.be/z3-fjYzd0BA"
            ],
            [
                "git",
                "https://github.com/ocornut/imgui"
            ],
            [
                "git",
                "https://github.com/nothings/stb"
            ],
            [
                "doi",
                "https://doi.org/10.1145/3528223.3530127",
                1455
            ]
        ],
        "colab": "https://colab.research.google.com/github/NVlabs/instant-ngp/blob/master/notebooks/instant_ngp.ipynb",
        "update": 1674034790.0
    },
    {
        "name": "LaMa",
        "description": "Resolution-robust Large Mask Inpainting with Fourier Convolutions",
        "author": [
            [
                "Roman Suvorov",
                "https://github.com/windj007"
            ],
            [
                "Elizaveta Logacheva",
                "https://github.com/elimohl"
            ],
            [
                "Anton Mashikhin",
                "https://www.linkedin.com/in/heyt0ny/"
            ],
            [
                "Anastasia Remizova",
                "https://github.com/feathernox"
            ],
            [
                "Arsenii Ashukha",
                "https://ashukha.com/"
            ],
            [
                "Aleksei Silvestrov",
                "https://www.linkedin.com/in/%D0%B0%D0%BB%D0%B5%D0%BA%D1%81%D0%B5%D0%B9-%D1%81%D0%B8%D0%BB%D1%8C%D0%B2%D0%B5%D1%81%D1%82%D1%80%D0%BE%D0%B2-141b99b6/"
            ],
            [
                "Naejin Kong",
                "https://github.com/naejin-kong"
            ],
            [
                "Harshith Goka",
                "https://github.com/h9399-goka"
            ],
            [
                "Kiwoong Park",
                "https://github.com/kyoong-park"
            ],
            [
                "Victor Lempitsky",
                "http://sites.skoltech.ru/compvision/members/vilem/"
            ]
        ],
        "links": [
            [
                "project",
                "https://saic-mdal.github.io/lama-project/"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2109.07161"
            ],
            [
                "git",
                "https://github.com/saic-mdal/lama",
                8162
            ],
            [
                "git",
                "https://github.com/andy971022/auto-lama"
            ],
            [
                "git",
                "https://github.com/richzhang/PerceptualSimilarity"
            ],
            [
                "git",
                "https://github.com/Po-Hsun-Su/pytorch-ssim"
            ],
            [
                "git",
                "https://github.com/mseitzer/pytorch-fid"
            ],
            [
                "doi",
                "https://doi.org/10.1109/WACV51458.2022.00323",
                409
            ]
        ],
        "colab": "https://colab.research.google.com/github/saic-mdal/lama/blob/master/colab/LaMa_inpainting.ipynb",
        "update": 1690934325.0
    },
    {
        "name": "RVM",
        "description": "Robust High-Resolution Video Matting with Temporal Guidance",
        "author": [
            [
                "Shanchuan Lin",
                "https://github.com/PeterL1n"
            ],
            [
                "Linjie Yang",
                "https://sites.google.com/site/linjieyang89/"
            ],
            [
                "Imran Saleemi",
                "http://www.cs.ucf.edu/~imran/"
            ],
            [
                "Soumyadip Sengupta",
                "https://homes.cs.washington.edu/~soumya91/"
            ]
        ],
        "links": [
            [
                "project",
                "https://peterl1n.github.io/RobustVideoMatting"
            ],
            [
                "arxiv",
                "http://arxiv.org/abs/2108.11515"
            ],
            [
                "yt",
                "https://youtu.be/Jvzltozpbpk"
            ],
            [
                "yt",
                "https://youtu.be/Ay-mGCEYEzM"
            ],
            [
                "git",
                "https://github.com/PeterL1n/RobustVideoMatting",
                8641
            ],
            [
                "git",
                "https://github.com/NVIDIA/VideoProcessingFramework"
            ],
            [
                "git",
                "https://github.com/FeiGeChuanShu/ncnn_Android_RobustVideoMatting"
            ],
            [
                "doi",
                "https://doi.org/10.1109/WACV51458.2022.00319",
                88
            ]
        ],
        "colab": "https://colab.research.google.com/drive/10z-pNKRnVNsp0Lq9tH1J_XPZ7CBC_uHm",
        "update": 1637742544.201
    },
    {
        "name": "T0",
        "description": "Multitask Prompted Training Enables Zero-Shot Task Generalization",
        "author": [
            [
                "Victor Sanh",
                "https://github.com/VictorSanh"
            ],
            [
                "Albert Webson",
                "https://representation.ai/"
            ],
            [
                "Colin Raffel",
                "https://colinraffel.com//"
            ],
            [
                "Stephen Bach",
                "http://cs.brown.edu/people/sbach/"
            ],
            [
                "Lintang Sutawika",
                "https://github.com/lintangsutawika"
            ],
            [
                "Zaid Alyafeai",
                "https://github.com/zaidalyafeai"
            ],
            [
                "Antoine Chaffin",
                "https://antoine.chaffin.fr/"
            ],
            [
                "Arnaud Stiegler",
                "https://github.com/arnaudstiegler"
            ],
            [
                "Teven Scao",
                "https://scholar.google.com/citations?user=ik0_vxsAAAAJ"
            ],
            [
                "Arun Raja",
                "https://www.arunraja.dev/"
            ],
            [
                "Manan Dey",
                "https://github.com/manandey"
            ],
            [
                "M Saiful Bari",
                "https://sbmaruf.github.io/"
            ],
            [
                "Canwen Xu",
                "https://www.canwenxu.net/"
            ],
            [
                "Urmish Thakker",
                "https://github.com/Urmish"
            ],
            [
                "Shanya Sharma",
                "https://shanyas10.github.io/"
            ],
            [
                "Eliza Szczechla",
                "https://elsanns.github.io/"
            ],
            [
                "Taewoon Kim",
                "https://tae898.github.io/"
            ],
            [
                "Gunjan Chhablani",
                "https://gchhablani.github.io/"
            ],
            [
                "Nihal Nayak",
                "https://nihalnayak.github.io/"
            ],
            [
                "Debajyoti Datta",
                "http://debajyotidatta.github.io/"
            ],
            [
                "Jonathan Chang",
                "https://github.com/cccntu/"
            ],
            [
                "Mike Tian-Jian Jiang",
                "https://github.com/tianjianjiang"
            ],
            [
                "Matteo Manica",
                "https://github.com/drugilsberg"
            ],
            [
                "Sheng Shen",
                "https://sincerass.github.io/"
            ],
            [
                "Zheng Xin Yong",
                "https://yongzx.github.io/"
            ],
            [
                "Harshit Pandey",
                "https://scholar.google.com/citations?user=BPIs78gAAAAJ"
            ],
            [
                "Rachel Bawden",
                "https://rbawden.github.io/"
            ],
            [
                "Trishala Neeraj",
                "https://github.com/trishalaneeraj"
            ],
            [
                "Jos Rozen",
                "https://scholar.google.com/citations?user=OxEDKogAAAAJ"
            ],
            [
                "Abheesht Sharma",
                "https://github.com/abheesht-sharma"
            ],
            [
                "Andrea Santilli",
                "https://teelinsan.github.io/"
            ],
            [
                "Thibault Fevry",
                "http://thibaultfevry.com/"
            ],
            [
                "Jason Alan Fries",
                "https://web.stanford.edu/~jfries/"
            ],
            [
                "Ryan Teehan",
                "https://github.com/rteehas"
            ],
            [
                "Stella Biderman",
                "https://www.stellabiderman.com/"
            ],
            [
                "Leo Gao",
                "https://github.com/leogao2"
            ],
            [
                "Tali Bers",
                "https://github.com/tbers-coursera"
            ],
            [
                "Thomas Wolf",
                "https://thomwolf.io/"
            ],
            [
                "Alexander M. Rush",
                "https://scholar.google.com/citations?user=LIjnUGgAAAAJ"
            ]
        ],
        "links": [
            [
                "arxiv",
                "https://arxiv.org/abs/2110.08207"
            ],
            [
                "yt",
                "https://youtu.be/iJ0IVZgGjTM"
            ],
            [
                "yt",
                "https://youtu.be/YToXXfrIu6w"
            ],
            [
                "git",
                "https://github.com/bigscience-workshop/promptsource",
                2712
            ]
        ],
        "colab": "https://colab.research.google.com/drive/1xx7SgdLaAu23YFBirXmaQViDr8caowX_",
        "update": 1653815550.845
    },
    {
        "name": "SOAT",
        "description": "StyleGAN of All Trades: Image Manipulation with Only Pretrained StyleGAN",
        "author": [
            [
                "Min Jin Chong",
                "https://mchong6.github.io/"
            ],
            [
                "Hsin-Ying Lee",
                "http://hsinyinglee.com/"
            ],
            [
                "David Forsyth",
                "http://luthuli.cs.uiuc.edu/~daf/"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/mchong6/SOAT",
                380
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2111.01619"
            ],
            [
                "git",
                "https://github.com/justinpinkney/toonify"
            ],
            [
                "git",
                "https://github.com/rosinality/stylegan2-pytorch"
            ],
            [
                "hf",
                "https://huggingface.co/spaces/akhaliq/SOAT"
            ]
        ],
        "colab": "https://colab.research.google.com/github/mchong6/SOAT/blob/master/infinity.ipynb",
        "update": 1636817884.0
    },
    {
        "name": "JoJoGAN",
        "description": "One Shot Face Stylization",
        "author": [
            [
                "Min Jin Chong",
                "https://mchong6.github.io/"
            ],
            [
                "David Forsyth",
                "http://luthuli.cs.uiuc.edu/~daf/"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/mchong6/JoJoGAN",
                1420
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2112.11641"
            ],
            [
                "git",
                "https://github.com/rosinality/stylegan2-pytorch"
            ],
            [
                "git",
                "https://github.com/replicate/cog"
            ],
            [
                "doi",
                "https://doi.org/10.1007/978-3-031-19787-1_8",
                25
            ]
        ],
        "colab": "https://colab.research.google.com/github/mchong6/JoJoGAN/blob/master/stylize.ipynb",
        "update": 1643838580.0
    },
    {
        "name": "GANs N' Roses",
        "description": "Stable, Controllable, Diverse Image to Image Translation",
        "author": [
            [
                "Min Jin Chong",
                "https://mchong6.github.io/"
            ],
            [
                "David Forsyth",
                "http://luthuli.cs.uiuc.edu/~daf/"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/mchong6/GANsNRoses",
                1158
            ],
            [
                "git",
                "https://github.com/rosinality/stylegan2-pytorch"
            ],
            [
                "git",
                "https://github.com/znxlwm/UGATIT-pytorch"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2106.06561"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2007.06600"
            ],
            [
                "yt",
                "https://youtu.be/VNg0NyCGl_4"
            ]
        ],
        "colab": "https://colab.research.google.com/github/mchong6/GANsNRoses/blob/master/inference_colab.ipynb",
        "update": 1624119682.0
    },
    {
        "name": "SwinIR",
        "description": "Image Restoration Using Swin Transformer",
        "author": [
            [
                "Jingyun Liang",
                "https://jingyunliang.github.io/"
            ],
            [
                "Jiezhang Cao",
                "https://github.com/caojiezhang"
            ],
            [
                "Guolei Sun",
                "https://github.com/GuoleiSun"
            ],
            [
                "Kai Zhang",
                "https://cszn.github.io/"
            ],
            [
                "Luc Van Gool",
                "https://scholar.google.com/citations?user=TwMib_QAAAAJ"
            ],
            [
                "Radu Timofte",
                "https://www.informatik.uni-wuerzburg.de/computervision/home/"
            ]
        ],
        "links": [
            [
                "arxiv",
                "https://arxiv.org/abs/2108.10257"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2107.10833"
            ],
            [
                "git",
                "https://github.com/JingyunLiang/SwinIR",
                4500
            ],
            [
                "git",
                "https://github.com/cszn/BSRGAN"
            ],
            [
                "git",
                "https://github.com/microsoft/Swin-Transformer"
            ],
            [
                "git",
                "https://github.com/cszn/KAIR"
            ],
            [
                "doi",
                "https://doi.org/10.1109/ICCVW54120.2021.00210",
                1888
            ]
        ],
        "colab": "https://colab.research.google.com/gist/JingyunLiang/a5e3e54bc9ef8d7bf594f6fee8208533/swinir-demo-on-real-world-image-sr.ipynb",
        "update": 1655495506.0
    },
    {
        "name": "VRT",
        "description": "A Video Restoration Transformer",
        "author": [
            [
                "Jingyun Liang",
                "https://jingyunliang.github.io/"
            ],
            [
                "Jiezhang Cao",
                "https://github.com/caojiezhang"
            ],
            [
                "Yuchen Fan",
                "https://ychfan.github.io/"
            ],
            [
                "Kai Zhang",
                "https://cszn.github.io/"
            ],
            [
                "Yawei Li",
                "https://ofsoundof.github.io/"
            ],
            [
                "Radu Timofte",
                "https://www.informatik.uni-wuerzburg.de/computervision/home/"
            ],
            [
                "Luc Van Gool",
                "https://scholar.google.com/citations?user=TwMib_QAAAAJ"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/JingyunLiang/VRT",
                1386
            ],
            [
                "git",
                "https://github.com/cszn/KAIR"
            ],
            [
                "git",
                "https://github.com/SwinTransformer/Video-Swin-Transformer"
            ],
            [
                "git",
                "https://github.com/open-mmlab/mmediting"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2201.12288"
            ],
            [
                "doi",
                "https://doi.org/10.1109/TIP.2024.3372454",
                40
            ]
        ],
        "colab": "https://colab.research.google.com/gist/JingyunLiang/deb335792768ad9eb73854a8efca4fe0/vrt-demo-on-video-restoration.ipynb",
        "update": 1655314991.0
    },
    {
        "name": "Swin2SR",
        "description": "Novel Swin Transformer V2, to improve SwinIR for image super-resolution, and in particular, the compressed input scenario",
        "author": [
            [
                "Marcos Conde",
                "https://mv-lab.github.io/"
            ],
            [
                "Ui-Jin Choi",
                "https://github.com/Choiuijin1125"
            ],
            [
                "Maxime Burchi",
                "https://scholar.google.com/citations?user=7S_l2eAAAAAJ"
            ],
            [
                "Radu Timofte",
                "https://www.informatik.uni-wuerzburg.de/computervision/home/"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/mv-lab/swin2sr",
                586
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2209.11345"
            ],
            [
                "git",
                "https://github.com/cszn/KAIR/"
            ],
            [
                "kaggle",
                "https://www.kaggle.com/code/jesucristo/super-resolution-demo-swin2sr-official/"
            ],
            [
                "hf",
                "https://huggingface.co/spaces/jjourney1125/swin2sr"
            ],
            [
                "kaggle",
                "https://www.kaggle.com/datasets/jesucristo/super-resolution-benchmarks"
            ],
            [
                "kaggle",
                "https://www.kaggle.com/jinssaa/official-swin2sr-demo-results/"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2108.10257"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2208.11184"
            ],
            [
                "git",
                "https://github.com/mv-lab/AISP"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2111.09883"
            ],
            [
                "git",
                "https://github.com/microsoft/Swin-Transformer"
            ],
            [
                "doi",
                "https://doi.org/10.1007/978-3-031-25063-7_42",
                57
            ]
        ],
        "colab": "https://colab.research.google.com/drive/1paPrt62ydwLv2U2eZqfcFsePI4X4WRR1",
        "update": 1664795206.185
    },
    {
        "name": "XLS-R",
        "description": "Self-supervised Cross-lingual Speech Representation Learning at Scale",
        "author": [
            [
                "Arun Babu",
                "https://github.com/arbabu123"
            ],
            [
                "Changhan Wang",
                "https://www.changhan.me/"
            ],
            [
                "Andros Tjandra",
                "https://github.com/androstj"
            ],
            [
                "Kushal Lakhotia",
                "https://about.me/hikushalhere"
            ],
            [
                "Qiantong Xu",
                "https://github.com/xuqiantong"
            ],
            [
                "Naman Goyal",
                "https://github.com/ngoyal2707"
            ],
            [
                "Kritika Singh",
                "https://scholar.google.com/citations?user=Ltk3SykAAAAJ"
            ],
            [
                "Patrick von Platen",
                "https://github.com/patrickvonplaten"
            ],
            [
                "Yatharth Saraf",
                "https://scholar.google.com/citations?user=KJTtNJwAAAAJ"
            ],
            [
                "Juan Pino",
                "https://scholar.google.com/citations?user=weU_-4IAAAAJ"
            ],
            [
                "Alexei Baevski",
                "https://github.com/alexeib"
            ],
            [
                "Alexis Conneau",
                "https://github.com/aconneau"
            ],
            [
                "Michael Auli",
                "https://github.com/michaelauli"
            ]
        ],
        "links": [
            [
                "arxiv",
                "https://arxiv.org/abs/2111.09296"
            ],
            [
                "blog post",
                "https://huggingface.co/blog/fine-tune-xlsr-wav2vec2"
            ],
            [
                "git",
                "https://github.com/facebookresearch/fairseq/blob/main/examples/wav2vec/xlsr/README.md",
                30632
            ],
            [
                "git",
                "https://github.com/facebookresearch/fairscale"
            ]
        ],
        "colab": "https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/Fine_Tune_XLS_R_on_Common_Voice.ipynb",
        "update": 1652174602.0
    },
    {
        "name": "MTTR",
        "description": "End-to-End Referring Video Object Segmentation with Multimodal Transformers",
        "author": [
            [
                "Adam Botach",
                "https://www.linkedin.com/in/adam-botach"
            ],
            [
                "Evgenii Zheltonozhskii",
                "https://evgeniizh.com/"
            ],
            [
                "Chaim Baskin",
                "https://github.com/chaimbaskin"
            ]
        ],
        "links": [
            [
                "arxiv",
                "https://arxiv.org/abs/2111.14821"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/1907.11692"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2106.13230"
            ],
            [
                "git",
                "https://github.com/mttr2021/MTTR",
                646
            ],
            [
                "git",
                "https://github.com/SwinTransformer/Video-Swin-Transformer"
            ],
            [
                "yt",
                "https://youtu.be/YqlhXgq6hcs"
            ],
            [
                "hf",
                "https://huggingface.co/spaces/MTTR/MTTR-Referring-Video-Object-Segmentation"
            ],
            [
                "doi",
                "https://doi.org/10.1109/CVPR52688.2022.00493",
                71
            ]
        ],
        "colab": "https://colab.research.google.com/drive/12p0jpSx3pJNfZk-y_L44yeHZlhsKVra-",
        "update": 1655757537.613
    },
    {
        "name": "Disco Diffusion",
        "description": "A frankensteinian amalgamation of notebooks, models and techniques for the generation of AI Art and Animations",
        "author": [
            [
                "Max Ingham",
                "https://github.com/somnai-dreams"
            ],
            [
                "Adam Letts",
                "https://linktr.ee/gandamu"
            ],
            [
                "Daniel Russell",
                "https://github.com/russelldc"
            ],
            [
                "Chigozie Nri",
                "https://github.com/chigozienri"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/alembics/disco-diffusion",
                7476
            ],
            [
                "git",
                "https://github.com/openai/guided-diffusion"
            ],
            [
                "yt",
                "https://youtu.be/_DtWfh9oS54"
            ],
            [
                "yt",
                "https://youtu.be/gWxmtdZL8FE"
            ],
            [
                "yt",
                "https://youtu.be/yVJB6oD0_gM"
            ]
        ],
        "colab": "https://colab.research.google.com/github/alembics/disco-diffusion/blob/main/Disco_Diffusion.ipynb",
        "update": 1676116417.0
    },
    {
        "name": "Deep Painterly Harmonization",
        "description": "Algorithm produces significantly better results than photo compositing or global stylization techniques and that it enables creative painterly edits that would be otherwise difficult to achieve",
        "author": [
            [
                "Fujun Luan",
                "https://luanfujun.github.io/"
            ],
            [
                "Sylvain Paris",
                "http://people.csail.mit.edu/sparis/"
            ],
            [
                "Eli Shechtman",
                "https://research.adobe.com/person/eli-shechtman/"
            ],
            [
                "Kavita Bala",
                "https://www.cs.cornell.edu/~kb/"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/luanfujun/deep-painterly-harmonization",
                6073
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/1804.03189"
            ],
            [
                "git",
                "https://github.com/jcjohnson/neural-style"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/1701.08893"
            ],
            [
                "git",
                "https://github.com/torch/torch7"
            ],
            [
                "git",
                "https://github.com/szagoruyko/loadcaffe"
            ]
        ],
        "colab": "https://colab.research.google.com/gist/eyaler/5303782669fb43510d398bd346c6e3e6/deep-painterly-harmonization.ipynb",
        "update": 1727111674.0
    },
    {
        "name": "Text2Mesh",
        "description": "Text-Driven Neural Stylization for Meshes",
        "author": [
            [
                "Oscar Michel",
                "https://ojmichel.github.io/"
            ],
            [
                "Roi Bar-On",
                "https://github.com/roibaron"
            ],
            [
                "Richard Liu",
                "https://github.com/factoryofthesun"
            ],
            [
                "Sagie Benaim",
                "https://sagiebenaim.github.io/"
            ],
            [
                "Rana Hanocka",
                "http://people.cs.uchicago.edu/~ranahanocka/"
            ]
        ],
        "links": [
            [
                "project",
                "https://threedle.github.io/text2mesh/"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2112.03221"
            ],
            [
                "CLIP",
                "https://openai.com/blog/clip/"
            ],
            [
                "git",
                "https://github.com/threedle/text2mesh",
                934
            ],
            [
                "kaggle",
                "https://www.kaggle.com/code/neverix/text2mesh/notebook"
            ]
        ],
        "colab": "https://colab.research.google.com/github/threedle/text2mesh/blob/master/colab_demo.ipynb",
        "update": 1652540240.0
    },
    {
        "name": "GPEN",
        "description": "GAN Prior Embedded Network for Blind Face Restoration in the Wild",
        "author": [
            [
                "Tao Yang",
                "https://cg.cs.tsinghua.edu.cn/people/~tyang/"
            ],
            [
                "Peiran Ren",
                "https://scholar.google.com/citations?&user=x5dEuxsAAAAJ"
            ],
            [
                "Xuansong Xie",
                "https://scholar.google.com/citations?user=M0Ei1zkAAAAJ"
            ],
            [
                "Lei Zhang",
                "http://www4.comp.polyu.edu.hk/~cslzhang/"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/yangxy/GPEN",
                2452
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2105.06070"
            ],
            [
                "demo",
                "https://vision.aliyun.com/experience/detail?spm=a211p3.14020179.J_7524944390.17.66cd4850wVDkUQ&tagName=facebody&children=EnhanceFace"
            ],
            [
                "git",
                "https://github.com/biubug6/Pytorch_Retinaface"
            ],
            [
                "git",
                "https://github.com/rosinality/stylegan2-pytorch"
            ]
        ],
        "colab": "https://colab.research.google.com/github/yangxy/GPEN/blob/main/GPEN.ipynb",
        "update": 1676431243.0
    },
    {
        "name": "Pose with Style",
        "description": "Detail-Preserving Pose-Guided Image Synthesis with Conditional StyleGAN",
        "author": [
            [
                "Badour AlBahar",
                "https://badouralbahar.github.io/"
            ],
            [
                "Jingwan Lu",
                "https://research.adobe.com/person/jingwan-lu/"
            ],
            [
                "Jimei Yang",
                "https://github.com/jimeiyang"
            ],
            [
                "Zhixin Shu",
                "https://zhixinshu.github.io/"
            ],
            [
                "Eli Shechtman",
                "https://research.adobe.com/person/eli-shechtman/"
            ],
            [
                "Jia-Bin Huang",
                "https://jbhuang0604.github.io/"
            ]
        ],
        "links": [
            [
                "project",
                "https://pose-with-style.github.io/"
            ],
            [
                "yt",
                "https://youtu.be/d_ETeAVLilw"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2109.06166"
            ],
            [
                "git",
                "https://github.com/BadourAlBahar/pose-with-style",
                269
            ],
            [
                "git",
                "https://github.com/rosinality/stylegan2-pytorch"
            ]
        ],
        "colab": "https://colab.research.google.com/github/tg-bomze/collection-of-notebooks/blob/master/HomeStylist.ipynb",
        "update": 1642607044.0
    },
    {
        "name": "Make-A-Scene",
        "description": "Scene-Based Text-to-Image Generation with Human Priors",
        "author": [
            [
                "Oran Gafni",
                "https://github.com/ogafni"
            ],
            [
                "Adam Polyak",
                "https://scholar.google.com/citations?user=CP62OTMAAAAJ"
            ],
            [
                "Oron Ashual",
                "https://scholar.google.com/citations?user=CUA9JCkAAAAJ"
            ],
            [
                "Shelly Sheynin",
                "https://github.com/shellysheynin"
            ],
            [
                "Devi Parikh",
                "https://faculty.cc.gatech.edu/~parikh/"
            ],
            [
                "Yaniv Taigman",
                "https://ytaigman.github.io/"
            ]
        ],
        "links": [
            [
                "arxiv",
                "https://arxiv.org/abs/2203.13131"
            ],
            [
                "git",
                "https://github.com/CasualGANPapers/Make-A-Scene",
                334
            ],
            [
                "yt",
                "https://youtu.be/ZM06MjPdoxw"
            ]
        ],
        "colab": "https://colab.research.google.com/drive/1SPyQ-epTsAOAu8BEohUokN4-b5RM_TnE",
        "update": 1660275880.152
    },
    {
        "name": "Score SDE",
        "description": "Score-Based Generative Modeling through Stochastic Differential Equations",
        "author": [
            [
                "Yang Song",
                "https://yang-song.net/"
            ],
            [
                "Jascha Sohl-Dickstein",
                "http://www.sohldickstein.com/"
            ],
            [
                "Diederik Kingma",
                "http://dpkingma.com/"
            ],
            [
                "Abhishek Kumar",
                "https://abhishek.umiacs.io/"
            ],
            [
                "Stefano Ermon",
                "https://cs.stanford.edu/~ermon/"
            ],
            [
                "Ben Poole",
                "https://cs.stanford.edu/~poole/"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/yang-song/score_sde",
                1523
            ],
            [
                "git",
                "https://github.com/yang-song/score_sde_pytorch"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2011.13456"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/1907.05600"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2006.09011"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2006.11239"
            ],
            [
                "git",
                "https://github.com/google/ml_collections"
            ],
            [
                "yt",
                "https://youtu.be/L9ZegT87QK8"
            ]
        ],
        "colab": "https://colab.research.google.com/github/yang-song/score_sde/blob/main/Score_SDE_demo.ipynb",
        "update": 1616097669.0
    },
    {
        "name": "Fourier Feature Networks",
        "description": "Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains",
        "author": [
            [
                "Matthew Tancik",
                "https://www.matthewtancik.com/"
            ],
            [
                "Pratul Srinivasan",
                "https://pratulsrinivasan.github.io/"
            ],
            [
                "Ben Mildenhall",
                "https://bmild.github.io/"
            ],
            [
                "Sara Fridovich-Keil",
                "https://people.eecs.berkeley.edu/~sfk/"
            ],
            [
                "Nithin Raghavan",
                "https://cseweb.ucsd.edu//~n2raghavan/"
            ],
            [
                "Utkarsh Singhal",
                "https://scholar.google.com/citations?user=lvA86MYAAAAJ"
            ],
            [
                "Ravi Ramamoorthi",
                "https://cseweb.ucsd.edu//~ravir/"
            ],
            [
                "Jon Barron",
                "https://jonbarron.info/"
            ],
            [
                "Ren Ng",
                "https://www2.eecs.berkeley.edu/Faculty/Homepages/yirenng.html"
            ]
        ],
        "links": [
            [
                "project",
                "https://bmild.github.io/fourfeat/"
            ],
            [
                "neurips",
                "https://proceedings.neurips.cc/paper/2020/hash/55053683268957697aa39fba6f231c68-Abstract.html"
            ],
            [
                "yt",
                "https://youtu.be/nVA6K6Sn2S4"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/1806.07572"
            ],
            [
                "neurips",
                "https://papers.nips.cc/paper/2007/hash/013a006f03dbc5392effeb8f18fda755-Abstract.html"
            ],
            [
                "git",
                "https://github.com/tancik/fourier-feature-networks",
                1266
            ]
        ],
        "colab": "https://colab.research.google.com/github/tancik/fourier-feature-networks/blob/master/Demo.ipynb",
        "update": 1673985069.0
    },
    {
        "name": "Bringing Old Photo Back to Life",
        "description": "Restoring old photos that suffer from severe degradation through a deep learning approach",
        "author": [
            [
                "Ziyu Wan",
                "http://raywzy.com/"
            ],
            [
                "Bo Zhang",
                "https://bo-zhang.me/"
            ],
            [
                "Dongdong Chen",
                "http://www.dongdongchen.bid/"
            ],
            [
                "Pan Zhang",
                "https://panzhang0212.github.io/"
            ],
            [
                "Dong Chen",
                "http://www.dongchen.pro/"
            ],
            [
                "Jing Liao",
                "https://liaojing.github.io/html/"
            ],
            [
                "Fang Wen",
                "https://www.microsoft.com/en-us/research/people/fangwen/"
            ]
        ],
        "links": [
            [
                "project",
                "http://raywzy.com/Old_Photo/"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2004.09484"
            ],
            [
                "git",
                "https://github.com/microsoft/Bringing-Old-Photos-Back-to-Life",
                15165
            ],
            [
                "demo",
                "https://replicate.com/microsoft/bringing-old-photos-back-to-life"
            ],
            [
                "yt",
                "https://youtu.be/Q5bhszQq9eA"
            ],
            [
                "doi",
                "https://doi.org/10.1109/CVPR42600.2020.00282",
                154
            ]
        ],
        "colab": "https://colab.research.google.com/drive/1NEm6AsybIiC5TwTU_4DqDkQO0nFRB-uA",
        "update": 1626172092.191
    },
    {
        "name": "VQ-Diffusion",
        "description": "Based on a VQ-VAE whose latent space is modeled by a conditional variant of the recently developed Denoising Diffusion Probabilistic Model",
        "author": [
            [
                "Shuyang Gu",
                "https://github.com/cientgu"
            ],
            [
                "Dong Chen",
                "http://www.dongchen.pro/"
            ],
            [
                "Jianmin Bao",
                "https://jianminbao.github.io/"
            ],
            [
                "Fang Wen",
                "https://www.microsoft.com/en-us/research/people/fangwen/"
            ],
            [
                "Bo Zhang",
                "https://bo-zhang.me/"
            ],
            [
                "Dongdong Chen",
                "http://www.dongdongchen.bid/"
            ],
            [
                "Lu Yuan",
                "https://scholar.google.com/citations?&user=k9TsUVsAAAAJ"
            ],
            [
                "Baining Guo",
                "https://scholar.google.com/citations?user=h4kYmRYAAAAJ"
            ],
            [
                "Shuyang Gu",
                "https://github.com/cientgu"
            ],
            [
                "Zhicong Tang",
                "https://github.com/zzctan"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/microsoft/VQ-Diffusion",
                903
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2111.14822"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2205.16007"
            ],
            [
                "git",
                "https://github.com/ehoogeboom/multinomial_diffusion"
            ],
            [
                "git",
                "https://github.com/openai/improved-diffusion"
            ],
            [
                "doi",
                "https://doi.org/10.1109/CVPR52688.2022.01043",
                278
            ]
        ],
        "colab": "https://colab.research.google.com/drive/1Ws0_wK2cnsWEnfB7HtmPT4bjCPElb40C",
        "update": 1656617899.73
    },
    {
        "name": "Visual ChatGPT",
        "description": "Connects ChatGPT and a series of Visual Foundation Models to enable sending and receiving images during chatting",
        "author": [
            [
                "Chenfei Wu",
                "https://github.com/chenfei-wu"
            ],
            [
                "Shengming Yin",
                "https://github.com/shengming-yin"
            ],
            [
                "Weizhen Qi",
                "https://github.com/WeizhenQ"
            ],
            [
                "Xiaodong Wang",
                "https://wang-xiaodong1899.github.io/"
            ],
            [
                "Zecheng Tang",
                "https://github.com/CODINNLG"
            ],
            [
                "Nan Duan",
                "https://nanduan.github.io/"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/microsoft/visual-chatgpt",
                34558
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2303.04671"
            ],
            [
                "git",
                "https://github.com/hwchase17/langchain"
            ],
            [
                "git",
                "https://github.com/lllyasviel/ControlNet"
            ],
            [
                "git",
                "https://github.com/timothybrooks/instruct-pix2pix"
            ],
            [
                "git",
                "https://github.com/timojl/clipseg"
            ],
            [
                "yt",
                "https://youtu.be/0UfXlFUwLms"
            ],
            [
                "yt",
                "https://youtu.be/7YEiEyfPF5U"
            ]
        ],
        "colab": "https://colab.research.google.com/drive/11BtP3h-w0dZjA-X8JsS9_eo8OeGYvxXB",
        "update": 1678846576.322
    },
    {
        "name": "GLIP",
        "description": "Grounded language-image pre-training model for learning object-level, language-aware, and semantic-rich visual representations",
        "author": [
            [
                "Liunian Harold Li",
                "https://liunian-harold-li.github.io/"
            ],
            [
                "Pengchuan Zhang",
                "https://pzzhang.github.io/pzzhang/"
            ],
            [
                "Haotian Zhang",
                "https://haotian-zhang.github.io/"
            ],
            [
                "Jianwei Yang",
                "https://jwyang.github.io/"
            ],
            [
                "Chunyuan Li",
                "https://chunyuan.li/"
            ],
            [
                "Yiwu Zhong",
                "https://pages.cs.wisc.edu/~yiwuzhong/"
            ],
            [
                "Lijuan Wang",
                "https://github.com/LijuanWang"
            ],
            [
                "Lu Yuan",
                "https://scholar.google.com/citations?user=k9TsUVsAAAAJ"
            ],
            [
                "Lei Zhang",
                "https://www.leizhang.org/"
            ],
            [
                "Jenq-Neng Hwang",
                "https://people.ece.uw.edu/hwang/"
            ],
            [
                "Kai-Wei Chang",
                "http://web.cs.ucla.edu/~kwchang/"
            ],
            [
                "Jianfeng Gao",
                "https://www.microsoft.com/en-us/research/people/jfgao/"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/microsoft/GLIP",
                2250
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2112.03857"
            ],
            [
                "git",
                "https://github.com/gligen/GLIGEN"
            ],
            [
                "doi",
                "https://doi.org/10.1109/CVPR52688.2022.01069",
                344
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2206.05836"
            ],
            [
                "hf",
                "https://huggingface.co/harold/GLIP"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2102.01066"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2204.08790"
            ],
            [
                "yt",
                "https://youtu.be/zu1BGQBI4dU"
            ],
            [
                "medium",
                "https://sh-tsang.medium.com/glip-grounded-language-image-pre-training-2be2483295b3"
            ],
            [
                "medium",
                "https://towardsdatascience.com/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa"
            ],
            [
                "blog post",
                "https://www.microsoft.com/en-us/research/project/project-florence-vl/articles/object-detection-in-the-wild-via-grounded-language-image-pre-training/"
            ]
        ],
        "colab": "https://colab.research.google.com/drive/12x7v-_miN7-SRiziK3Cx4ffJzstBJNqb",
        "update": 1659145203.25
    },
    {
        "name": "LIDA",
        "description": "Tool for generating grammar-agnostic visualizations and infographics",
        "author": [
            [
                "Victor Dibia",
                "https://victordibia.com/"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/microsoft/lida",
                2835
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2303.02927"
            ],
            [
                "project",
                "https://microsoft.github.io/lida/"
            ],
            [
                "git",
                "https://github.com/victordibia/llmx"
            ],
            [
                "doi",
                "https://doi.org/10.18653/v1/2023.acl-demo.11",
                18
            ],
            [
                "git",
                "https://github.com/lida-project/lida-streamlit"
            ],
            [
                "yt",
                "https://youtu.be/exYi9W-dhME"
            ],
            [
                "yt",
                "https://youtu.be/U9K1Cu45nMQ"
            ],
            [
                "yt",
                "https://youtu.be/6xcCwlDx6f8"
            ],
            [
                "medium",
                "https://medium.com/@c17hawke/lida-automatically-generate-visualization-and-with-llms-the-future-of-data-visualization-6bc556876b46"
            ]
        ],
        "colab": "https://colab.research.google.com/github/microsoft/lida/blob/main/notebooks/tutorial.ipynb",
        "update": 1707260376.0
    },
    {
        "name": "ICON",
        "description": "Given a set of images, method estimates a detailed 3D surface from each image and then combines these into an animatable avatar",
        "author": [
            [
                "Yuliang Xiu",
                "https://xiuyuliang.cn/"
            ],
            [
                "Jinlong Yang",
                "https://is.mpg.de/~jyang"
            ],
            [
                "Dimitrios Tzionas",
                "https://ps.is.mpg.de/~dtzionas"
            ],
            [
                "Michael Black",
                "https://ps.is.mpg.de/~black"
            ]
        ],
        "links": [
            [
                "project",
                "https://icon.is.tue.mpg.de/"
            ],
            [
                "git",
                "https://github.com/yuliangxiu/icon",
                1607
            ],
            [
                "git",
                "https://github.com/facebookresearch/KeypointNeRF"
            ],
            [
                "git",
                "https://github.com/YadiraF/PIXIE"
            ],
            [
                "git",
                "https://github.com/YuliangXiu/bvh-distance-queries"
            ],
            [
                "git",
                "https://github.com/Project-Splinter/MonoPortDataset"
            ],
            [
                "git",
                "https://github.com/ZhengZerong/PaMIR"
            ],
            [
                "git",
                "https://github.com/Project-Splinter/MonoPort"
            ],
            [
                "git",
                "https://github.com/shunsukesaito/SCANimate"
            ],
            [
                "git",
                "https://github.com/google/aistplusplus_api"
            ],
            [
                "yt",
                "https://youtu.be/hZd6AYin2DE"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2112.09127"
            ],
            [
                "hf",
                "https://huggingface.co/spaces/Yuliang/ICON"
            ],
            [
                "doi",
                "https://doi.org/10.1109/CVPR52688.2022.01294",
                177
            ]
        ],
        "colab": "https://colab.research.google.com/drive/1-AWeWhPvCTBX0KfMtgtMk10uPU05ihoA",
        "update": 1693515135.028
    },
    {
        "name": "ECON",
        "description": "designed for \"Human digitization from a color image\", which combines the best properties of implicit and explicit representations, to infer high-fidelity 3D clothed humans from in-the-wild images, even with loose clothing or in challenging poses",
        "author": [
            [
                "Yuliang Xiu",
                "https://xiuyuliang.cn/"
            ],
            [
                "Jinlong Yang",
                "https://is.mpg.de/~jyang"
            ],
            [
                "Xu Cao",
                "https://xucao-42.github.io/homepage/"
            ],
            [
                "Dimitrios Tzionas",
                "https://ps.is.mpg.de/~dtzionas"
            ],
            [
                "Michael Black",
                "https://ps.is.mpg.de/~black"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/YuliangXiu/ECON",
                1115
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2212.07422"
            ],
            [
                "docker",
                "https://github.com/YuliangXiu/ECON/blob/master/docs/installation-docker.md"
            ],
            [
                "yt",
                "https://youtu.be/sbWZbTf6ZYk"
            ],
            [
                "yt",
                "https://youtu.be/SDVfCeaI4AY"
            ],
            [
                "discord",
                "https://discord.gg/Vqa7KBGRyk"
            ],
            [
                "twitter",
                "https://twitter.com/yuliangxiu"
            ],
            [
                "git",
                "https://github.com/kwan3854/CEB_ECON"
            ],
            [
                "git",
                "https://github.com/xucao-42/bilateral_normal_integration"
            ],
            [
                "git",
                "https://github.com/Project-Splinter/MonoPortDataset"
            ],
            [
                "git",
                "https://github.com/huangyangyi/TeCH"
            ],
            [
                "git",
                "https://github.com/huangyangyi/TeCH"
            ],
            [
                "git",
                "https://github.com/vchoutas/smplx"
            ],
            [
                "git",
                "https://github.com/yfeng95/PIXIE"
            ],
            [
                "yt",
                "https://youtu.be/5PEd_p90kS0"
            ],
            [
                "yt",
                "https://youtu.be/MDFvV7y5Qgk"
            ],
            [
                "doi",
                "https://doi.org/10.1109/CVPR52729.2023.00057",
                66
            ],
            [
                "reddit",
                "https://www.reddit.com/r/StableDiffusion/comments/1451sjr/econ_explicit_clothed_humans_optimized_via_normal/"
            ]
        ],
        "colab": "https://colab.research.google.com/drive/1YRgwoRCZIrSB2e7auEWFyG10Xzjbrbno",
        "update": 1685499567.318
    },
    {
        "name": "PoolFormer",
        "description": "MetaFormer Is Actually What You Need for Vision",
        "author": [
            [
                "Weihao Yu",
                "https://whyu.me/"
            ],
            [
                "Mi Luo",
                "https://luomi97.github.io/"
            ],
            [
                "Pan Zhou",
                "https://panzhous.github.io/"
            ],
            [
                "Chenyang Si",
                "https://github.com/ChenyangSi"
            ],
            [
                "Yichen Zhou",
                "https://dblp.org/pid/55/10422.html"
            ],
            [
                "Xinchao Wang",
                "https://sites.google.com/site/sitexinchaowang/"
            ],
            [
                "Jiashi Feng",
                "https://sites.google.com/site/jshfeng/"
            ],
            [
                "Shuicheng Yan",
                "https://yanshuicheng.ai/"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/sail-sg/poolformer",
                1300
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2111.11418"
            ],
            [
                "hf",
                "https://huggingface.co/spaces/akhaliq/poolformer"
            ],
            [
                "git",
                "https://github.com/rwightman/pytorch-image-models"
            ],
            [
                "git",
                "https://github.com/facebookresearch/fvcore"
            ],
            [
                "git",
                "https://github.com/NVIDIA/apex"
            ],
            [
                "doi",
                "https://doi.org/10.1109/CVPR52688.2022.01055",
                502
            ]
        ],
        "colab": "https://colab.research.google.com/github/sail-sg/poolformer/blob/main/misc/poolformer_demo.ipynb",
        "update": 1717254549.0
    },
    {
        "name": "PTI",
        "description": "Pivotal Tuning Inversion enables employing off-the-shelf latent based semantic editing techniques on real images using StyleGAN",
        "author": [
            [
                "Daniel Roich",
                "https://github.com/danielroich"
            ],
            [
                "Ron Mokady",
                "https://rmokady.github.io/"
            ],
            [
                "Amit Bermano",
                "https://www.cs.tau.ac.il/~amberman/"
            ],
            [
                "Daniel Cohen-Or",
                "https://danielcohenor.com/"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/danielroich/PTI",
                905
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2106.05744"
            ],
            [
                "git",
                "https://github.com/NVlabs/stylegan2-ada-pytorch"
            ],
            [
                "git",
                "https://github.com/richzhang/PerceptualSimilarity"
            ],
            [
                "doi",
                "https://doi.org/10.1145/3544777",
                215
            ]
        ],
        "colab": "https://colab.research.google.com/github/danielroich/PTI/blob/main/notebooks/inference_playground.ipynb",
        "update": 1625123131.0
    },
    {
        "name": "E2FGVI",
        "description": "An End-to-End framework for Flow-Guided Video Inpainting through elaborately designed three trainable modules, namely, flow completion, feature propagation, and content hallucination modules",
        "author": [
            [
                "Zhen Li",
                "https://paper99.github.io/"
            ],
            [
                "Cheng-Ze Lu",
                "https://github.com/LGYoung"
            ],
            [
                "Jianhua Qin",
                "https://scholar.google.com/citations?&user=TAr7TU4AAAAJ"
            ],
            [
                "Chun-Le Guo",
                "https://scholar.google.com/citations?user=RZLYwR0AAAAJ"
            ],
            [
                "Ming-Ming Cheng",
                "https://mmcheng.net/"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/MCG-NKU/E2FGVI",
                1037
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2204.02663"
            ],
            [
                "yt",
                "https://youtu.be/N--qC3T2wc4"
            ],
            [
                "medium",
                "https://medium.com/mlearning-ai/end-to-end-framework-for-flow-guided-video-inpainting-c5e2d8b61d20"
            ],
            [
                "yt",
                "https://youtu.be/3eH3Fm6gOFk"
            ],
            [
                "data",
                "https://competitions.codalab.org/competitions/19544#participate-get-data"
            ],
            [
                "data",
                "https://data.vision.ee.ethz.ch/csergi/share/davis/DAVIS-2017-trainval-480p.zip"
            ],
            [
                "git",
                "https://github.com/researchmm/STTN"
            ],
            [
                "git",
                "https://github.com/microsoft/Focal-Transformer"
            ],
            [
                "git",
                "https://github.com/ruiliu-ai/FuseFormer"
            ],
            [
                "git",
                "https://github.com/phoenix104104/fast_blind_video_consistency#evaluation"
            ],
            [
                "doi",
                "https://doi.org/10.1109/CVPR52688.2022.01704",
                70
            ]
        ],
        "colab": "https://colab.research.google.com/drive/12rwY2gtG8jVWlNx9pjmmM8uGmh5ue18G",
        "update": 1649259011.212
    },
    {
        "name": "Thin-Plate Spline Motion Model",
        "description": "End-to-end unsupervised motion transfer framework",
        "author": [
            [
                "Jian Zhao",
                "https://scholar.google.com/citations?user=OKm5CQYAAAAJ"
            ],
            [
                "Hui Zhang",
                "https://scholar.google.com/citations?user=w3mzCiwAAAAJ"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/yoyo-nb/Thin-Plate-Spline-Motion-Model",
                3483
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2203.14367"
            ],
            [
                "hf",
                "https://huggingface.co/spaces/CVPR/Image-Animation-using-Thin-Plate-Spline-Motion-Model"
            ],
            [
                "git",
                "https://github.com/AliaksandrSiarohin/monkey-net"
            ],
            [
                "git",
                "https://github.com/AliaksandrSiarohin/video-preprocessing"
            ],
            [
                "git",
                "https://github.com/AliaksandrSiarohin/pose-evaluation"
            ],
            [
                "git",
                "https://github.com/TalkUHulk/Image-Animation-Turbo-Boost"
            ],
            [
                "supp",
                "https://cloud.tsinghua.edu.cn/f/f7b8573bb5b04583949f/?dl=1"
            ],
            [
                "doi",
                "https://doi.org/10.1109/CVPR52688.2022.00364",
                75
            ]
        ],
        "colab": "https://colab.research.google.com/drive/1DREfdpnaBhqISg0fuQlAAIwyGVn1loH_",
        "update": 1688753096.148
    },
    {
        "name": "Panini-Net",
        "description": "GAN Prior based Degradation-Aware Feature Interpolation for Face Restoration",
        "author": [
            [
                "Yinhuai Wang",
                "https://github.com/wyhuai"
            ],
            [
                "Yujie Hu",
                "https://villa.jianzhang.tech/people/yujie-hu/"
            ],
            [
                "Jian Zhang",
                "http://jianzhang.tech/"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/jianzhangcs/panini",
                113
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2203.08444"
            ],
            [
                "git",
                "https://github.com/NVlabs/ffhq-dataset"
            ],
            [
                "git",
                "https://github.com/tkarras/progressive_growing_of_gans"
            ],
            [
                "doi",
                "https://doi.org/10.1609/aaai.v36i3.20159",
                10
            ]
        ],
        "colab": "https://colab.research.google.com/github/GeeveGeorge/Panini-Net-Colab/blob/main/PaniniNet_Working.ipynb",
        "update": 1649821769.0
    },
    {
        "name": "ConvMixer",
        "description": "An extremely simple model that is similar in spirit to the ViT and the even-more-basic MLP-Mixer in that it operates directly on patches as input, separates the mixing of spatial and channel dimensions, and maintains equal size and resolution throughout the network",
        "author": [
            [
                "Asher Trockman",
                "http://ashertrockman.com/"
            ],
            [
                "Zico Kolter",
                "http://zicokolter.com/"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/locuslab/convmixer",
                1062
            ],
            [
                "git",
                "https://github.com/locuslab/convmixer-cifar10"
            ],
            [
                "git",
                "https://github.com/rwightman/pytorch-image-models"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2201.09792"
            ],
            [
                "medium",
                "https://medium.com/codex/an-overview-on-convmixer-patches-are-all-you-need-8502a8d87011"
            ],
            [
                "yt",
                "https://youtu.be/Gl0s0GDqN3c?t=990"
            ]
        ],
        "colab": "https://colab.research.google.com/github/locuslab/convmixer/blob/main/pytorch-image-models/notebooks/EffResNetComparison.ipynb",
        "update": 1633473210.0
    },
    {
        "name": "UniFormerV2",
        "description": "Unified Transformer for Efficient Spatiotemporal Representation Learning",
        "author": [
            [
                "Kunchang Li",
                "https://github.com/Andy1621"
            ],
            [
                "Yali Wang",
                "https://scholar.google.com/citations?user=hD948dkAAAAJ"
            ],
            [
                "Yinan He",
                "https://github.com/yinanhe"
            ],
            [
                "Yizhuo Li",
                "http://liyizhuo.com/"
            ],
            [
                "Yi Wang",
                "https://scholar.google.com/citations?user=Xm2M8UwAAAAJ"
            ],
            [
                "Limin Wang",
                "http://wanglimin.github.io/"
            ],
            [
                "Yu Qiao",
                "http://mmlab.siat.ac.cn/yuqiao/index.html"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/OpenGVLab/UniFormerV2",
                295
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2211.09552"
            ],
            [
                "git",
                "https://github.com/innat/UniFormerV2"
            ],
            [
                "hf",
                "https://huggingface.co/spaces/Andy1621/uniformerv2_demo"
            ],
            [
                "git",
                "https://huggingface.co/spaces/Andy1621/uniformerv2_demo"
            ],
            [
                "git",
                "https://github.com/huggingface/pytorch-image-models/blob/main/timm/models/vision_transformer.py"
            ],
            [
                "pwc",
                "https://paperswithcode.com/sota/action-classification-on-activitynet?p=uniformerv2-spatiotemporal-learning-by-arming"
            ],
            [
                "pwc",
                "https://paperswithcode.com/sota/action-recognition-on-hacs?p=uniformerv2-spatiotemporal-learning-by-arming"
            ],
            [
                "pwc",
                "https://paperswithcode.com/sota/action-classification-on-moments-in-time?p=uniformerv2-spatiotemporal-learning-by-arming"
            ],
            [
                "pwc",
                "https://paperswithcode.com/sota/action-recognition-in-videos-on-something-1?p=uniformerv2-spatiotemporal-learning-by-arming"
            ],
            [
                "pwc",
                "https://paperswithcode.com/sota/action-classification-on-kinetics-700?p=uniformerv2-spatiotemporal-learning-by-arming"
            ],
            [
                "git",
                "https://github.com/facebookresearch/SlowFast"
            ],
            [
                "doi",
                "https://doi.org/10.1109/ICCV51070.2023.00157",
                17
            ]
        ],
        "colab": "https://colab.research.google.com/drive/1Z6RzLcno_eLGD_E96mlWoyGieGbKxPQr",
        "update": 1697798359.344
    },
    {
        "name": "EPro-PnP",
        "description": "Generalized End-to-End Probabilistic Perspective-n-Points for Monocular Object Pose Estimation",
        "author": [
            [
                "Hansheng Chen",
                "https://lakonik.github.io/"
            ],
            [
                "Pichao Wang",
                "https://wangpichao.github.io/"
            ],
            [
                "Fan Wang",
                "https://scholar.google.com/citations?user=WCRGTHsAAAAJ"
            ],
            [
                "Wei Tian",
                "https://scholar.google.com/citations?user=aYKQn88AAAAJ"
            ],
            [
                "Lu Xiong",
                "https://ieeexplore.ieee.org/author/37401835800"
            ],
            [
                "Hao Li",
                "https://scholar.google.com/citations?user=pHN-QIwAAAAJ"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/tjiiv-cprg/EPro-PnP",
                1114
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2203.13254"
            ],
            [
                "yt",
                "https://youtu.be/TonBodQ6EUU"
            ],
            [
                "nuScenes",
                "https://www.nuscenes.org/object-detection?externalData=no&mapData=no&modalities=Camera"
            ],
            [
                "git",
                "https://github.com/megvii-research/petr"
            ],
            [
                "git",
                "https://github.com/HuangJunJie2017/BEVDet"
            ],
            [
                "git",
                "https://github.com/fudan-zvg/PolarFormer"
            ],
            [
                "git",
                "https://github.com/zhiqi-li/BEVFormer"
            ],
            [
                "git",
                "https://github.com/open-mmlab/mmdetection3d"
            ],
            [
                "doi",
                "https://doi.org/10.1109/TPAMI.2024.3354997",
                0
            ]
        ],
        "colab": "https://colab.research.google.com/github/tjiiv-cprg/EPro-PnP/blob/main/demo/fit_identity.ipynb",
        "update": 1657630952.0
    },
    {
        "name": "diffsort",
        "description": "Differentiable Sorting Networks",
        "author": [
            [
                "Felix Petersen",
                "http://petersen.ai/"
            ],
            [
                "Christian Borgelt",
                "https://borgelt.net/"
            ],
            [
                "Hilde Kuehne",
                "https://hildekuehne.github.io/"
            ],
            [
                "Oliver Deussen",
                "https://www.cgmi.uni-konstanz.de/personen/prof-dr-oliver-deussen/"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/Felix-Petersen/diffsort",
                107
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2105.04019"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2203.09630"
            ],
            [
                "yt",
                "https://youtu.be/Rl-sFaE1z4M"
            ]
        ],
        "colab": "https://colab.research.google.com/drive/1q0TZFFYB9FlOJYWKt0_7ZaXQT190anhm",
        "update": 1642380675.757
    },
    {
        "name": "StyleSDF",
        "description": "A high resolution, 3D-consistent image and shape generation technique",
        "author": [
            [
                "Roy Or-El",
                "https://homes.cs.washington.edu/~royorel/"
            ],
            [
                "Xuan Luo",
                "https://roxanneluo.github.io/"
            ],
            [
                "Mengyi Shan",
                "https://shanmy.github.io/"
            ],
            [
                "Eli Shechtman",
                "https://research.adobe.com/person/eli-shechtman/"
            ],
            [
                "Jeong Joon Park",
                "https://jjparkcv.github.io/"
            ],
            [
                "Ira Kemelmacher-Shlizerman",
                "https://www.irakemelmacher.com/"
            ]
        ],
        "links": [
            [
                "project",
                "https://stylesdf.github.io/"
            ],
            [
                "hf",
                "https://huggingface.co/spaces/SerdarHelli/StyleSDF-3D"
            ],
            [
                "git",
                "https://github.com/royorel/StyleSDF",
                536
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2112.11427"
            ],
            [
                "git",
                "https://github.com/rosinality/stylegan2-pytorch"
            ],
            [
                "git",
                "https://github.com/yenchenlin/nerf-pytorch"
            ],
            [
                "doi",
                "https://doi.org/10.1109/CVPR52688.2022.01314",
                127
            ]
        ],
        "colab": "https://colab.research.google.com/github/royorel/StyleSDF/blob/main/StyleSDF_demo.ipynb",
        "update": 1646443525.0
    },
    {
        "name": "Musika",
        "description": "Music generation system that can be trained on hundreds of hours of music using a single consumer GPU, and that allows for much faster than real-time generation of music of arbitrary length on a consumer CPU",
        "author": [
            [
                "Marco Pasini",
                "https://github.com/marcoppasini"
            ],
            [
                "Jan Schlüter",
                "https://www.ofai.at/~jan.schlueter/"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/marcoppasini/musika",
                665
            ],
            [
                "project",
                "https://marcoppasini.github.io/musika"
            ],
            [
                "yt",
                "https://youtu.be/QBl8y2Z_i7Y"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2208.08706"
            ],
            [
                "yt",
                "https://youtu.be/0l7OSM-bFvc"
            ],
            [
                "hf",
                "https://huggingface.co/spaces/marcop/musika"
            ],
            [
                "git",
                "https://github.com/hendriks73/tempo-cnn"
            ],
            [
                "data",
                "https://magenta.tensorflow.org/datasets/maestro"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2005.08526"
            ],
            [
                "git",
                "https://github.com/CPJKU/madmom"
            ]
        ],
        "colab": "https://colab.research.google.com/drive/1PowSw3doBURwLE-OTCiWkO8HVbS5paRb",
        "update": 1696846292.266
    },
    {
        "name": "IDE-3D",
        "description": "Interactive Disentangled Editing for High-Resolution 3D-aware Portrait Synthesis",
        "author": [
            [
                "Jingxiang Sun",
                "https://mrtornado24.github.io/"
            ],
            [
                "Xuan Wang",
                "https://xuanwangvc.github.io/"
            ],
            [
                "Yichun Shi",
                "https://seasonsh.github.io/"
            ],
            [
                "Lizhen Wang",
                "https://lizhenwangt.github.io/"
            ],
            [
                "Jue Wang",
                "https://juewang725.github.io/"
            ],
            [
                "Yebin Liu",
                "http://www.liuyebin.com/"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/MrTornado24/IDE-3D",
                483
            ],
            [
                "git",
                "https://arxiv.org/abs/2205.15517"
            ],
            [
                "git",
                "https://github.com/NVlabs/eg3d"
            ],
            [
                "git",
                "https://github.com/NVlabs/ffhq-dataset"
            ],
            [
                "git",
                "https://github.com/NVlabs/stylegan3"
            ],
            [
                "yt",
                "https://youtu.be/Kj5XY_J2Alk"
            ],
            [
                "doi",
                "https://doi.org/10.1145/3550454.3555506",
                69
            ]
        ],
        "colab": "https://colab.research.google.com/github/MrTornado24/IDE-3D/blob/main/inversion/notebooks/inference_playground.ipynb",
        "update": 1662657657.0
    },
    {
        "name": "HybrIK",
        "description": "Hybrid Analytical-Neural Inverse Kinematics Solution for 3D Human Pose and Shape Estimation",
        "author": [
            [
                "Jiefeng Li",
                "https://jeffli.site/"
            ],
            [
                "Chao Xu",
                "https://www.isdas.cn/"
            ],
            [
                "Zhicun Chen",
                "https://github.com/chenzhicun"
            ],
            [
                "Siyuan Bian",
                "https://github.com/biansy000"
            ],
            [
                "Lixin Yang",
                "https://lixiny.github.io/"
            ],
            [
                "Cewu Lu",
                "https://www.mvig.org/"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/Jeff-sjtu/HybrIK",
                1262
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2011.14672"
            ],
            [
                "supp",
                "https://openaccess.thecvf.com/content/CVPR2021/supplemental/Li_HybrIK_A_Hybrid_CVPR_2021_supplemental.zip"
            ],
            [
                "project",
                "https://jeffli.site/HybrIK/"
            ],
            [
                "git",
                "https://github.com/mks0601/3DMPPE_POSENET_RELEASE"
            ],
            [
                "yt",
                "https://youtu.be/tvwnXXH7xIw"
            ],
            [
                "pwc",
                "https://paperswithcode.com/sota/3d-human-pose-estimation-on-3dpw?p=hybrik-a-hybrid-analytical-neural-inverse"
            ],
            [
                "doi",
                "https://doi.org/10.1109/CVPR46437.2021.00339",
                226
            ]
        ],
        "colab": "https://colab.research.google.com/drive/1n41l7I2NxWseuruVQEU8he2XqzSXhu2f",
        "update": 1672557691.19
    },
    {
        "name": "Decision Transformers",
        "description": "An architecture that casts the problem of RL as conditional sequence modeling",
        "author": [
            [
                "Lili Chen",
                "http://www.lilichen.me/"
            ],
            [
                "Kevin Lu",
                "https://kzl.github.io/"
            ],
            [
                "Aravind Rajeswaran",
                "https://aravindr93.github.io/"
            ],
            [
                "Kimin Lee",
                "https://sites.google.com/view/kiminlee"
            ],
            [
                "Aditya Grover",
                "https://aditya-grover.github.io/"
            ],
            [
                "Michael Laskin",
                "https://www.mishalaskin.com/"
            ],
            [
                "Pieter Abbeel",
                "http://people.eecs.berkeley.edu/~pabbeel/"
            ],
            [
                "Aravind Srinivas",
                "https://github.com/aravindsrinivas"
            ],
            [
                "Igor Mordatch",
                "https://scholar.google.com/citations?user=Vzr1RukAAAAJ"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/kzl/decision-transformer",
                2418
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2106.01345"
            ],
            [
                "yt",
                "https://youtu.be/k08N5a0gG0A"
            ],
            [
                "hf",
                "https://huggingface.co/models?other=gym-continous-control"
            ],
            [
                "wiki",
                "https://en.wikipedia.org/wiki/Autoregressive_model"
            ],
            [
                "hf",
                "https://huggingface.co/edbeeching/decision-transformer-gym-hopper-expert"
            ],
            [
                "project",
                "https://sites.google.com/berkeley.edu/decision-transformer"
            ],
            [
                "yt",
                "https://youtu.be/-buULmf7dec"
            ],
            [
                "yt",
                "https://youtu.be/83QN9S-0I84"
            ],
            [
                "yt",
                "https://youtu.be/w4Bw8WYL8Ps"
            ],
            [
                "hf",
                "https://huggingface.co/docs/transformers/model_doc/decision_transformer"
            ]
        ],
        "colab": "https://colab.research.google.com/drive/1K3UuajwoPY1MzRKNkONNRS3gS5DxZ-qF",
        "update": 1662467667.684
    },
    {
        "name": "PyMAF",
        "description": "Pyramidal Mesh Alignment Feedback loop in regression network for well-aligned body mesh recovery and extend it for the recovery of expressive full-body models",
        "author": [
            [
                "Hongwen Zhang",
                "https://hongwenzhang.github.io/"
            ],
            [
                "Yating Tian",
                "https://github.com/tinatiansjz"
            ],
            [
                "Yuxiang Zhang",
                "https://zhangyux15.github.io/"
            ],
            [
                "Mengcheng Li",
                "https://github.com/Dw1010"
            ],
            [
                "Liang An",
                "https://anl13.github.io/"
            ],
            [
                "Zhenan Sun",
                "http://www.cbsr.ia.ac.cn/users/znsun/"
            ],
            [
                "Yebin Liu",
                "https://www.liuyebin.com/"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/HongwenZhang/PyMAF",
                598
            ],
            [
                "project",
                "https://www.liuyebin.com/pymaf-x/"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2207.06400"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2103.16507"
            ],
            [
                "git",
                "https://github.com/facebookresearch/eft"
            ],
            [
                "git",
                "https://github.com/HongwenZhang/DaNet-DensePose2SMPL"
            ],
            [
                "git",
                "https://github.com/facebookresearch/DensePose"
            ],
            [
                "git",
                "https://github.com/Microsoft/human-pose-estimation.pytorch"
            ],
            [
                "yt",
                "https://youtu.be/yqEmznSKjYI"
            ],
            [
                "yt",
                "https://youtu.be/ylOB0wCeV34"
            ]
        ],
        "colab": "https://colab.research.google.com/drive/11RXLsH9BdoSCwY6G-IX7KgqDxVoImu6K",
        "update": 1665070340.2
    },
    {
        "name": "PyMAF-X",
        "description": "Кegression-based approach to recovering parametric full-body models from monocular images",
        "author": [
            [
                "Hongwen Zhang",
                "https://hongwenzhang.github.io/"
            ],
            [
                "Yating Tian",
                "https://github.com/tinatiansjz"
            ],
            [
                "Yuxiang Zhang",
                "https://zhangyux15.github.io/"
            ],
            [
                "Mengcheng Li",
                "https://github.com/Dw1010"
            ],
            [
                "Liang An",
                "https://anl13.github.io/"
            ],
            [
                "Zhenan Sun",
                "http://www.cbsr.ia.ac.cn/users/znsun/"
            ],
            [
                "Yebin Liu",
                "https://www.liuyebin.com/"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/HongwenZhang/PyMAF-X",
                229
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2207.06400"
            ],
            [
                "project",
                "https://www.liuyebin.com/pymaf-x/"
            ],
            [
                "git",
                "https://github.com/HongwenZhang/DaNet-DensePose2SMPL"
            ],
            [
                "git",
                "https://github.com/facebookresearch/DensePose"
            ],
            [
                "git",
                "https://github.com/Microsoft/human-pose-estimation.pytorch"
            ],
            [
                "git",
                "https://github.com/microsoft/MeshGraphormer"
            ],
            [
                "git",
                "https://github.com/leoxiaobin/deep-high-resolution-net.pytorch"
            ],
            [
                "doi",
                "https://doi.org/10.1109/TPAMI.2023.3271691",
                41
            ],
            [
                "yt",
                "https://youtu.be/ylOB0wCeV34"
            ]
        ],
        "colab": "https://colab.research.google.com/drive/13Iytx1Hb0ZryEwbJdpXBW9ggDxs2Y-tL",
        "update": 1676367370.46
    },
    {
        "name": "AudioLDM",
        "description": "Text-to-audio system that is built on a latent space to learn the continuous audio representations from contrastive language-audio pretraining latents",
        "author": [
            [
                "Haohe Liu",
                "https://haoheliu.github.io/"
            ],
            [
                "Zehua Chen",
                "https://github.com/zehuachenImperial"
            ],
            [
                "Yi Yuan",
                "https://www.surrey.ac.uk/people/yi-yuan"
            ],
            [
                "Xinhao Mei",
                "https://xinhaomei.github.io/"
            ],
            [
                "Xubo Liu",
                "https://liuxubo717.github.io/"
            ],
            [
                "Danilo Mandic",
                "https://www.imperial.ac.uk/people/d.mandic"
            ],
            [
                "Wenwu Wang",
                "http://personal.ee.surrey.ac.uk/Personal/W.Wang/"
            ],
            [
                "Mark Plumbley",
                "https://www.surrey.ac.uk/people/mark-plumbley"
            ]
        ],
        "links": [
            [
                "project",
                "https://audioldm.github.io/"
            ],
            [
                "git",
                "https://github.com/haoheliu/AudioLDM",
                2473
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2301.12503"
            ],
            [
                "git",
                "https://github.com/LAION-AI/CLAP"
            ],
            [
                "git",
                "https://github.com/CompVis/stable-diffusion"
            ],
            [
                "git",
                "https://github.com/toshas/torch-fidelity"
            ],
            [
                "yt",
                "https://youtu.be/_0VTltNYhao"
            ]
        ],
        "colab": "https://colab.research.google.com/github/olaviinha/NeuralTextToAudio/blob/main/AudioLDM_pub.ipynb",
        "update": 1701530373.0
    },
    {
        "name": "AudioSep",
        "description": "Foundation model for open-domain audio source separation with natural language queries",
        "author": [
            [
                "Xubo Liu",
                "https://liuxubo717.github.io/"
            ],
            [
                "Qiuqiang Kong",
                "https://qiuqiangkong.github.io/"
            ],
            [
                "Yan Zhao",
                "https://cliffzhao.github.io/"
            ],
            [
                "Haohe Liu",
                "https://haoheliu.github.io/"
            ],
            [
                "Yi Yuan",
                "https://www.surrey.ac.uk/people/yi-yuan"
            ],
            [
                "Yuzhuo Liu",
                "https://github.com/redrabbit94"
            ],
            [
                "Rui Xia",
                "https://scholar.google.co.uk/citations?user=26oErxwAAAAJ"
            ],
            [
                "Yuxuan Wang",
                "https://scholar.google.com/citations?user=3RaOfJkAAAAJ"
            ],
            [
                "Mark Plumbley",
                "https://www.surrey.ac.uk/people/mark-plumbley"
            ],
            [
                "Wenwu Wang",
                "http://personal.ee.surrey.ac.uk/Personal/W.Wang/"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/Audio-AGI/AudioSep",
                1640
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2308.05037"
            ],
            [
                "project",
                "https://audio-agi.github.io/Separate-Anything-You-Describe/"
            ]
        ],
        "colab": "https://colab.research.google.com/github/Audio-AGI/AudioSep/blob/main/AudioSep_Colab.ipynb",
        "update": 1710515016.0
    },
    {
        "name": "TabPFN",
        "description": "Neural network that learned to do tabular data prediction",
        "author": [
            [
                "Noah Hollmann",
                "https://github.com/noahho"
            ],
            [
                "Samuel Müller",
                "https://scholar.google.com/citations?user=pevYEjAAAAAJ"
            ],
            [
                "Katharina Eggensperger",
                "https://github.com/KEggensperger"
            ],
            [
                "Frank Hutter",
                "https://ml.informatik.uni-freiburg.de/profile/hutter/"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/automl/TabPFN",
                1231
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2207.01848"
            ],
            [
                "yt",
                "https://youtu.be/BGTO5N5-ack"
            ],
            [
                "blog post",
                "https://www.automl.org/tabpfn-a-transformer-that-solves-small-tabular-classification-problems-in-a-second/"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2106.11189"
            ],
            [
                "twitter",
                "https://twitter.com/tunguz/status/1578730907711655937"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2106.01342"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2106.03253"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2106.11189"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2112.10510"
            ]
        ],
        "colab": "https://colab.research.google.com/drive/194mCs6SEPEW6C0rcP7xWzcEtt1RBc8jJ",
        "update": 1701258516.46
    },
    {
        "name": "VALL-E X",
        "description": "Cross-lingual neural codec language model for cross-lingual speech synthesis",
        "author": [
            [
                "Ziqiang Zhang",
                "https://github.com/onisac-K"
            ],
            [
                "Long Zhou",
                "https://long-zhou.github.io/"
            ],
            [
                "Chengyi Wang",
                "https://cywang97.github.io/"
            ],
            [
                "Sanyuan Chen",
                "https://sanyuan-chen.github.io/"
            ],
            [
                "Yu Wu",
                "https://www.microsoft.com/en-us/research/people/yuwu1/"
            ],
            [
                "Shujie Liu",
                "https://www.microsoft.com/en-us/research/people/shujliu/"
            ],
            [
                "Zhuo Chen",
                "https://www.microsoft.com/en-us/research/people/zhuc/"
            ],
            [
                "Yanqing Liu",
                "https://scholar.google.com/citations?user=dIJFz4UAAAAJ"
            ],
            [
                "Huaming Wang",
                "https://scholar.google.com/citations?user=aJDLg5IAAAAJ"
            ],
            [
                "Jinyu Li",
                "https://www.microsoft.com/en-us/research/people/jinyli/"
            ],
            [
                "Lei He",
                "https://scholar.google.com/citations?user=EKl9yY8AAAAJ"
            ],
            [
                "Sheng Zhao",
                "https://scholar.google.com/citations?user=689bIIwAAAAJ"
            ],
            [
                "Furu Wei",
                "https://www.microsoft.com/en-us/research/people/fuwei/"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/Plachtaa/VALL-E-X",
                7712
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2303.03926"
            ],
            [
                "project",
                "https://www.microsoft.com/en-us/research/project/vall-e-x"
            ],
            [
                "discord",
                "https://discord.gg/qCBRmAnTxg"
            ],
            [
                "hf",
                "https://huggingface.co/Plachta/VALL-E-X"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2301.02111"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2209.03143"
            ],
            [
                "demo",
                "https://plachtaa.github.io/"
            ],
            [
                "git",
                "https://github.com/lifeiteng/vall-e"
            ],
            [
                "yt",
                "https://youtu.be/7qgfoVFQmvk"
            ],
            [
                "medium",
                "https://medium.com/syncedreview/speak-a-foreign-language-in-your-own-voice-1dafa42f78d9"
            ]
        ],
        "colab": "https://colab.research.google.com/drive/1yyD_sz531QntLKowMHo-XxorsFBCfKul",
        "update": 1705628231.536
    },
    {
        "name": "NAFNet",
        "description": "Nonlinear Activation Free Network for Image Restoration",
        "author": [
            [
                "Liangyu Chen",
                "https://github.com/mayorx"
            ],
            [
                "Xiaojie Chu",
                "https://github.com/chuxiaojie"
            ],
            [
                "Xiangyu Zhang",
                "https://scholar.google.com/citations?user=yuB-cfoAAAAJ"
            ],
            [
                "Jian Sun",
                "http://www.jiansun.org/"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/megvii-research/NAFNet",
                2273
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2204.04676"
            ],
            [
                "doi",
                "https://doi.org/10.1007/978-3-031-20071-7_2",
                395
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2204.08714"
            ],
            [
                "pwc",
                "https://paperswithcode.com/sota/image-deblurring-on-gopro?p=simple-baselines-for-image-restoration"
            ],
            [
                "pwc",
                "https://paperswithcode.com/sota/image-denoising-on-sidd?p=simple-baselines-for-image-restoration"
            ]
        ],
        "colab": "https://colab.research.google.com/drive/1dkO5AyktmBoWwxBwoKFUurIDn0m4qDXT",
        "update": 1650000908.624
    },
    {
        "name": "ViDT+",
        "description": "An Extendable, Efficient and Effective Transformer-based Object Detector",
        "author": [
            [
                "Hwanjun Song",
                "https://songhwanjun.github.io/"
            ],
            [
                "Deqing Sun",
                "https://deqings.github.io/"
            ],
            [
                "Sanghyuk Chun",
                "https://sanghyukchun.github.io/home/"
            ],
            [
                "Varun Jampani",
                "https://varunjampani.github.io/"
            ],
            [
                "Dongyoon Han",
                "https://sites.google.com/site/dyhan0920/"
            ],
            [
                "Byeongho Heo",
                "https://sites.google.com/view/byeongho-heo/home"
            ],
            [
                "Wonjae Kim",
                "https://wonjae.kim/"
            ],
            [
                "Ming-Hsuan Yang",
                "http://faculty.ucmerced.edu/mhyang/"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/naver-ai/vidt/tree/vidt-plus",
                307
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2204.07962"
            ],
            [
                "git",
                "https://github.com/fundamentalvision/Deformable-DETR"
            ],
            [
                "git",
                "https://github.com/EherSenaw/ViDT_colab"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2110.03921"
            ]
        ],
        "colab": "https://colab.research.google.com/github/EherSenaw/ViDT_colab/blob/main/vidt_colab.ipynb",
        "update": 1650439562.0
    },
    {
        "name": "DiffCSE",
        "description": "Unsupervised contrastive learning framework for learning sentence embeddings",
        "author": [
            [
                "Yung-Sung Chuang",
                "https://people.csail.mit.edu/yungsung/"
            ],
            [
                "Rumen Dangovski",
                "http://super-ms.mit.edu/rumen.html"
            ],
            [
                "Hongyin Luo",
                "https://luohongyin.github.io/"
            ],
            [
                "Yang Zhang",
                "https://mitibmwatsonailab.mit.edu/people/yang-zhang/"
            ],
            [
                "Shiyu Chang",
                "https://code-terminator.github.io/"
            ],
            [
                "Marin Soljačić",
                "http://www.mit.edu/~soljacic/marin.html"
            ],
            [
                "Shang-Wen Li",
                "https://swdanielli.github.io/"
            ],
            [
                "Scott Wen-tau Yih",
                "https://scottyih.org/"
            ],
            [
                "Yoon Kim",
                "https://people.csail.mit.edu/yoonkim/"
            ],
            [
                "James Glass",
                "http://groups.csail.mit.edu/sls/people/glass.shtml"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/voidism/diffcse",
                291
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2204.10298"
            ],
            [
                "hf",
                "https://huggingface.co/voidism"
            ],
            [
                "twitter",
                "https://twitter.com/YungSungChuang/status/1517518077902000129"
            ],
            [
                "git",
                "https://github.com/princeton-nlp/SimCSE"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2104.08821"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2111.00899"
            ]
        ],
        "colab": "https://colab.research.google.com/github/voidism/DiffCSE/blob/master/diffcse_evaluation.ipynb",
        "update": 1650762346.0
    },
    {
        "name": "EVA3D",
        "description": "High-quality unconditional 3D human generative model that only requires 2D image collections for training",
        "author": [
            [
                "Fangzhou Hong",
                "https://hongfz16.github.io/"
            ],
            [
                "Zhaoxi Chen",
                "https://frozenburning.github.io/"
            ],
            [
                "Yushi Lan",
                "https://github.com/NIRVANALAN"
            ],
            [
                "Liang Pan",
                "https://github.com/paul007pl"
            ],
            [
                "Ziwei Liu",
                "https://liuziwei7.github.io/"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/hongfz16/EVA3D",
                590
            ],
            [
                "project",
                "https://hongfz16.github.io/projects/EVA3D.html"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2210.04888"
            ],
            [
                "yt",
                "https://youtu.be/JNV0FJ0aDWM"
            ],
            [
                "yt",
                "https://youtu.be/M-kyvzTQrBI"
            ]
        ],
        "colab": "https://colab.research.google.com/github/hongfz16/EVA3D/blob/main/notebook/EVA3D_Demo.ipynb",
        "update": 1680783711.0
    },
    {
        "name": "Text2Video-Zero",
        "description": "Text-to-Image Diffusion Models are Zero-Shot Video Generators",
        "author": [
            [
                "Levon Khachatryan",
                "https://github.com/lev1khachatryan"
            ],
            [
                "Andranik Movsisyan",
                "https://github.com/19and99"
            ],
            [
                "Vahram Tadevosyan",
                "https://www.linkedin.com/in/vtadevosian"
            ],
            [
                "Roberto Henschel",
                "https://github.com/rob-hen"
            ],
            [
                "Zhangyang Wang",
                "https://www.ece.utexas.edu/people/faculty/atlas-wang"
            ],
            [
                "Shant Navasardyan",
                "https://scholar.google.com/citations?user=VJSh59sAAAAJ"
            ],
            [
                "Humphrey Shi",
                "https://www.humphreyshi.com/"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/Picsart-AI-Research/Text2Video-Zero",
                4076
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2303.13439"
            ],
            [
                "project",
                "https://text2video-zero.github.io/"
            ],
            [
                "video",
                "https://www.dropbox.com/s/uv90mi2z598olsq/Text2Video-Zero.MP4"
            ],
            [
                "git",
                "https://github.com/dbolya/tomesd"
            ],
            [
                "hf",
                "https://huggingface.co/docs/diffusers/api/pipelines/text_to_video_zero"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/1907.01341"
            ],
            [
                "git",
                "https://github.com/JiauZhang/Text2Video-Zero"
            ],
            [
                "git",
                "https://github.com/camenduru/text2video-zero-colab"
            ],
            [
                "git",
                "https://github.com/SHI-Labs/Text2Video-Zero-sd-webui"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2303.17604"
            ],
            [
                "yt",
                "https://youtu.be/beeDJJz-Q0A"
            ],
            [
                "yt",
                "https://youtu.be/97-1GYPtz0M"
            ],
            [
                "doi",
                "https://doi.org/10.1109/ICCV51070.2023.01462",
                113
            ]
        ],
        "colab": "https://colab.research.google.com/github/camenduru/text2video-zero-colab/blob/main/text2video_all.ipynb",
        "update": 1681215461.0
    },
    {
        "name": "FAB",
        "description": "Flow AIS Bootstrap uses AIS to generate samples in regions where the flow is a poor approximation of the target, facilitating the discovery of new modes",
        "author": [
            [
                "Laurence Midgley",
                "https://lollcat.github.io/laurence-midgley/"
            ],
            [
                "Vincent Stimper",
                "https://is.mpg.de/person/vstimper"
            ],
            [
                "Gregor N. C. Simm",
                "https://www.gncs.me/"
            ],
            [
                "Bernhard Schölkopf",
                "https://scholar.google.com/citations?user=DZ-fHPgAAAAJ"
            ],
            [
                "José Miguel Hernández-Lobato",
                "https://jmhl.org/"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/lollcat/fab-torch",
                51
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2208.01893"
            ],
            [
                "git",
                "https://github.com/lollcat/fab-jax-old"
            ],
            [
                "git",
                "https://github.com/deepmind/annealed_flow_transport"
            ],
            [
                "yt",
                "https://youtu.be/xQQXvOWu9nE"
            ]
        ],
        "colab": "https://colab.research.google.com/github/lollcat/fab-torch/blob/master/experiments/gmm/fab_gmm.ipynb",
        "update": 1682752991.0
    },
    {
        "name": "Tune-A-Video",
        "description": "One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation",
        "author": [
            [
                "Jay Zhangjie Wu",
                "https://zhangjiewu.github.io/"
            ],
            [
                "Yixiao Ge",
                "https://geyixiao.com/"
            ],
            [
                "Xintao Wang",
                "https://xinntao.github.io/"
            ],
            [
                "Stan Weixian Lei",
                "https://github.com/StanLei52"
            ],
            [
                "Yuchao Gu",
                "https://ycgu.site/"
            ],
            [
                "Yufei Shi",
                "https://scholar.google.com/citations?user=rpnlkwEAAAAJ"
            ],
            [
                "Wynne Hsu",
                "https://www.comp.nus.edu.sg/~whsu/"
            ],
            [
                "Ying Shan",
                "https://scholar.google.com/citations?user=4oXBp9UAAAAJ"
            ],
            [
                "Xiaohu Qie",
                "https://scholar.google.com/citations?user=mk-F69UAAAAJ"
            ],
            [
                "Mike Zheng Shou",
                "https://sites.google.com/view/showlab"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/showlab/Tune-A-Video",
                4267
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2212.11565"
            ],
            [
                "project",
                "https://tuneavideo.github.io/"
            ],
            [
                "hf",
                "https://huggingface.co/Tune-A-Video-library"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2112.10752"
            ],
            [
                "hf",
                "https://huggingface.co/stabilityai/stable-diffusion-2-1"
            ],
            [
                "hf",
                "https://huggingface.co/sd-dreambooth-library"
            ],
            [
                "yt",
                "https://youtu.be/uzF6CTtjn-g"
            ],
            [
                "yt",
                "https://youtu.be/uUlp1_ExsGQ"
            ],
            [
                "doi",
                "https://doi.org/10.1109/ICCV51070.2023.00701",
                161
            ]
        ],
        "colab": "https://colab.research.google.com/github/showlab/Tune-A-Video/blob/main/notebooks/Tune-A-Video.ipynb",
        "update": 1677116129.0
    },
    {
        "name": "CodeFormer",
        "description": "Transformer-based prediction network to model global composition and context of the low-quality faces for code prediction, enabling the discovery of natural faces that closely approximate the target faces even when the inputs are severely degraded",
        "author": [
            [
                "Shangchen Zhou",
                "https://shangchenzhou.com/"
            ],
            [
                "Kelvin Chan",
                "https://ckkelvinchan.github.io/"
            ],
            [
                "Chongyi Li",
                "https://li-chongyi.github.io/"
            ],
            [
                "Chen Change Loy",
                "https://www.mmlab-ntu.com/person/ccloy/"
            ]
        ],
        "links": [
            [
                "project",
                "https://shangchenzhou.com/projects/CodeFormer/"
            ],
            [
                "git",
                "https://github.com/sczhou/CodeFormer",
                15972
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2206.11253"
            ],
            [
                "yt",
                "https://youtu.be/d3VDpkXlueI"
            ],
            [
                "neurips",
                "https://proceedings.neurips.cc/paper_files/paper/2022/hash/c573258c38d0a3919d8c1364053c45df-Abstract-Conference.html"
            ],
            [
                "git",
                "https://github.com/samb-t/unleashing-transformers"
            ],
            [
                "git",
                "https://github.com/deepcam-cn/yolov5-face"
            ],
            [
                "git",
                "https://github.com/xinntao/facexlib"
            ],
            [
                "yt",
                "https://youtu.be/PtwWu-FugbA"
            ],
            [
                "yt",
                "https://youtu.be/ORtYP8NW4T0"
            ],
            [
                "yt",
                "https://youtu.be/xc5lKOKBCcg"
            ]
        ],
        "colab": "https://colab.research.google.com/drive/1m52PNveE4PBhYrecj34cnpEeiHcC5LTb",
        "update": 1682039402.39
    },
    {
        "name": "Fast Segment Anything",
        "description": "CNN Segment Anything Model trained using only 2% of the SA-1B dataset published by SAM authors",
        "author": [
            [
                "Xu Zhao",
                "https://scholar.google.com/citations?user=F0cYEyAAAAAJ"
            ],
            [
                "Wenchao Ding",
                "https://github.com/berry-ding"
            ],
            [
                "Yongqi An",
                "https://github.com/an-yongqi"
            ],
            [
                "Yinglong Du",
                "https://github.com/YinglongDu"
            ],
            [
                "Tao Yu",
                "https://github.com/tianjinren"
            ],
            [
                "Min Li",
                "https://github.com/limin2021"
            ],
            [
                "Ming Tang",
                "https://www.researchgate.net/profile/Ming-Tang-2"
            ],
            [
                "Jinqiao Wang",
                "https://scholar.google.com/citations?user=7_BkyxEAAAAJ"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/CASIA-IVA-Lab/FastSAM",
                7547
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2306.12156"
            ],
            [
                "git",
                "https://github.com/ChuRuaNh0/FastSam_Awsome_TensorRT"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2112.10003"
            ],
            [
                "yt",
                "https://youtu.be/yHNPyqazYYU"
            ],
            [
                "yt",
                "https://youtu.be/SslzS0AsiAw"
            ],
            [
                "yt",
                "https://www.youtube.com/live/qvqkjP1wCDE"
            ],
            [
                "medium",
                "https://medium.com/@mahimairaja/so-what-exactly-is-fastsam-the-ultimate-guide-ddae21d3b486"
            ]
        ],
        "colab": "https://colab.research.google.com/drive/1oX14f6IneGGw612WgVlAiy91UHwFAvr9",
        "update": 1725936135.776
    },
    {
        "name": "Grounding DINO",
        "description": "Marrying DINO with Grounded Pre-Training for Open-Set Object Detection",
        "author": [
            [
                "Shilong Liu",
                "https://github.com/SlongLiu"
            ],
            [
                "Zhaoyang Zeng",
                "https://scholar.google.com/citations?user=U_cvvUwAAAAJ"
            ],
            [
                "Tianhe Ren",
                "https://rentainhe.github.io/"
            ],
            [
                "Feng Li",
                "https://scholar.google.com/citations?user=ybRe9GcAAAAJ"
            ],
            [
                "Hao Zhang",
                "https://scholar.google.com/citations?user=B8hPxMQAAAAJ"
            ],
            [
                "Jie Yang",
                "https://yangjie-cv.github.io/"
            ],
            [
                "Chunyuan Li",
                "https://scholar.google.com/citations?user=Zd7WmXUAAAAJ"
            ],
            [
                "Jianwei Yang",
                "https://jwyang.github.io/"
            ],
            [
                "Hang Su",
                "https://www.suhangss.me/"
            ],
            [
                "Jun Zhu",
                "https://scholar.google.com/citations?user=axsP38wAAAAJ"
            ],
            [
                "Lei Zhang",
                "https://www.leizhang.org/"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/IDEA-Research/GroundingDINO",
                6925
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2303.05499"
            ],
            [
                "git",
                "https://github.com/IDEA-Research/DINO"
            ],
            [
                "yt",
                "https://youtu.be/wxWDt5UiwY8"
            ],
            [
                "yt",
                "https://youtu.be/cMa77r3YrDk"
            ],
            [
                "yt",
                "https://youtu.be/C4NqaRBz_Kw"
            ],
            [
                "yt",
                "https://youtu.be/oEQYStnF2l8"
            ],
            [
                "git",
                "https://github.com/UX-Decoder/Semantic-SAM"
            ],
            [
                "git",
                "https://github.com/OptimalScale/DetGPT"
            ],
            [
                "git",
                "https://github.com/IDEA-Research/OpenSeeD"
            ],
            [
                "git",
                "https://github.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once"
            ],
            [
                "git",
                "https://github.com/microsoft/X-Decoder/tree/xgpt"
            ],
            [
                "git",
                "https://github.com/IDEA-Research/detrex"
            ],
            [
                "pwc",
                "https://paperswithcode.com/sota/zero-shot-object-detection-on-mscoco?p=grounding-dino-marrying-dino-with-grounded"
            ],
            [
                "pwc",
                "https://paperswithcode.com/sota/zero-shot-object-detection-on-odinw?p=grounding-dino-marrying-dino-with-grounded"
            ],
            [
                "pwc",
                "https://paperswithcode.com/sota/object-detection-on-coco-minival?p=grounding-dino-marrying-dino-with-grounded"
            ],
            [
                "pwc",
                "https://paperswithcode.com/sota/object-detection-on-coco?p=grounding-dino-marrying-dino-with-grounded"
            ]
        ],
        "colab": "https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/zero-shot-object-detection-with-grounding-dino.ipynb",
        "update": 1687953828.0
    },
    {
        "name": "Recognize Anything & Tag2Text",
        "description": "Vision language pre-training framework, which introduces image tagging into vision-language models to guide the learning of visual-linguistic features",
        "author": [
            [
                "Xinyu Huang",
                "https://xinyu1205.github.io/"
            ],
            [
                "Youcai Zhang",
                "https://github.com/Coler1994"
            ],
            [
                "Jinyu Ma",
                "https://github.com/majinyu666"
            ],
            [
                "Zhaoyang Li",
                "https://github.com/ZhaoyangLi-nju"
            ],
            [
                "Yanchun Xie",
                "https://scholar.google.com/citations?user=T0xk9-wAAAAJ"
            ],
            [
                "Yuzhuo Qin",
                "https://scholar.google.com/citations?user=5ZG65AkAAAAJ"
            ],
            [
                "Tong Luo",
                "https://ieeexplore.ieee.org/author/37089387319"
            ],
            [
                "Yaqian Li",
                "https://openreview.net/profile?id=~Yaqian_Li1"
            ],
            [
                "Yandong Guo",
                "http://www.lsl.zone/"
            ],
            [
                "Yandong Guo",
                "https://scholar.google.com/citations?user=fWDoWsQAAAAJ"
            ],
            [
                "Lei Zhang",
                "https://www.leizhang.org/"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/xinyu1205/recognize-anything",
                2943
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2306.03514"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2303.05657"
            ],
            [
                "project",
                "https://recognize-anything.github.io/"
            ],
            [
                "git",
                "https://github.com/OpenGVLab/Ask-Anything"
            ],
            [
                "git",
                "https://github.com/positive666/Prompt-Can-Anything"
            ],
            [
                "project",
                "https://recognize-anything.github.io/"
            ],
            [
                "medium",
                "https://artgor.medium.com/paper-review-recognize-anything-a-strong-image-tagging-model-9e5e1c6dd0af"
            ]
        ],
        "colab": "https://colab.research.google.com/github/mhd-medfa/recognize-anything/blob/main/recognize_anything_demo.ipynb",
        "update": 1688918689.0
    },
    {
        "name": "DreamGaussian",
        "description": "Algorithm to convert 3D Gaussians into textured meshes and apply a fine-tuning stage to refine the details",
        "author": [
            [
                "Jiaxiang Tang",
                "https://me.kiui.moe/"
            ],
            [
                "Jiawei Ren",
                "https://jiawei-ren.github.io/"
            ],
            [
                "Hang Zhou",
                "https://hangz-nju-cuhk.github.io/"
            ],
            [
                "Ziwei Liu",
                "https://liuziwei7.github.io/"
            ],
            [
                "Gang Zeng",
                "http://www.cis.pku.edu.cn/info/1177/1378.htm"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/dreamgaussian/dreamgaussian",
                3984
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2309.16653"
            ],
            [
                "project",
                "https://dreamgaussian.github.io/"
            ],
            [
                "git",
                "https://github.com/graphdeco-inria/diff-gaussian-rasterization"
            ],
            [
                "git",
                "https://github.com/NVlabs/nvdiffrast"
            ],
            [
                "git",
                "https://github.com/hoffstadt/DearPyGui"
            ]
        ],
        "colab": "https://colab.research.google.com/drive/1sLpYmmLS209-e5eHgcuqdryFRRO6ZhFS",
        "update": 1696428246.492
    },
    {
        "name": "Gaussian Splatting",
        "description": "State-of-the-art visual quality while maintaining competitive training times and importantly allow high-quality real-time (≥ 100 fps) novel-view synthesis at 1080p resolution",
        "author": [
            [
                "Bernhard Kerbl",
                "https://www.cg.tuwien.ac.at/staff/BernhardKerbl"
            ],
            [
                "Georgios Kopanas",
                "https://grgkopanas.github.io/"
            ],
            [
                "Thomas Leimkühler",
                "https://people.mpi-inf.mpg.de/~tleimkue/"
            ],
            [
                "George Drettakis",
                "http://www-sop.inria.fr/members/George.Drettakis/"
            ]
        ],
        "links": [
            [
                "arxiv",
                "https://arxiv.org/abs/2308.04079"
            ],
            [
                "git",
                "https://github.com/graphdeco-inria/gaussian-splatting",
                14941
            ],
            [
                "yt",
                "https://youtu.be/T_kXY43VZnk"
            ],
            [
                "project",
                "https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/"
            ],
            [
                "hf",
                "https://huggingface.co/camenduru/gaussian-splatting"
            ],
            [
                "yt",
                "https://youtu.be/UXtuigy_wYc"
            ],
            [
                "yt",
                "https://youtu.be/HVv_IQKlafQ"
            ],
            [
                "yt",
                "https://youtu.be/w43KV79LsFw"
            ],
            [
                "yt",
                "https://youtu.be/TLK3TDDcJFU"
            ],
            [
                "yt",
                "https://youtu.be/kShNYOuDnlI"
            ],
            [
                "yt",
                "https://youtu.be/juRMRej2d5c"
            ],
            [
                "doi",
                "https://doi.org/10.1145/3592433",
                614
            ],
            [
                "medium",
                "https://medium.com/axinc-ai/3d-gaussian-splatting-real-time-rendering-of-photorealistic-scenes-f7f1a47f060"
            ],
            [
                "reddit",
                "https://www.reddit.com/r/singularity/comments/163jeqa/3d_gaussian_splatting_for_realtime_radiance_field/"
            ]
        ],
        "colab": "https://colab.research.google.com/github/camenduru/gaussian-splatting-colab/blob/main/gaussian_splatting_colab.ipynb",
        "update": 1702947208.0
    },
    {
        "name": "Qwen-VL",
        "description": "Set of large-scale vision-language models designed to perceive and understand both text and images",
        "author": [
            [
                "Jinze Bai",
                "https://github.com/jinze1994"
            ],
            [
                "Shuai Bai",
                "https://github.com/ShuaiBai623"
            ],
            [
                "Shusheng Yang",
                "https://shushengyang.com/"
            ],
            [
                "Shijie Wang",
                "https://scholar.google.com/citations?user=DuAqyTwAAAAJ"
            ],
            [
                "Sinan Tan",
                "https://www.tinytangent.com/"
            ],
            [
                "Peng Wang",
                "https://scholar.google.com/citations?user=7fjqA0YAAAAJ"
            ],
            [
                "Junyang Lin",
                "https://justinlin610.github.io/"
            ],
            [
                "Chang Zhou",
                "https://scholar.google.com/citations?user=QeSoG3sAAAAJ"
            ],
            [
                "Jingren Zhou",
                "http://www.cs.columbia.edu/~jrzhou/"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/QwenLM/Qwen-VL",
                5137
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2308.12966"
            ],
            [
                "discord",
                "https://discord.gg/z3GAxXZ9Ce"
            ],
            [
                "demo",
                "https://modelscope.cn/studios/qwen/Qwen-VL-Chat-Demo/summary"
            ],
            [
                "git",
                "https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Evaluation"
            ],
            [
                "hf",
                "https://huggingface.co/spaces/AILab-CVC/SEED-Bench_Leaderboard"
            ],
            [
                "git",
                "https://github.com/OFA-Sys/TouchStone"
            ],
            [
                "git",
                "https://github.com/PanQiWei/AutoGPTQ"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2106.09685"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2305.14314"
            ],
            [
                "hf",
                "https://huggingface.co/Qwen/Qwen-VL"
            ],
            [
                "yt",
                "https://youtu.be/ElrSJDg23Po"
            ],
            [
                "yt",
                "https://youtu.be/E3MS8GfGWj4"
            ],
            [
                "yt",
                "https://youtu.be/ju09YaO7BGA"
            ]
        ],
        "colab": "https://colab.research.google.com/github/camenduru/Qwen-VL-Chat-colab/blob/main/Qwen_VL_Chat_colab.ipynb",
        "update": 1700810100.0
    },
    {
        "name": "LLaVA",
        "description": "Large Language and Vision Assistant, an end-to-end trained large multimodal model that connects a vision encoder and LLM for general-purpose visual and language understanding",
        "author": [
            [
                "Haotian Liu",
                "https://hliu.cc/"
            ],
            [
                "Chunyuan Li",
                "https://chunyuan.li/"
            ],
            [
                "Qingyang Wu",
                "https://qywu.github.io/"
            ],
            [
                "Yong Jae Lee",
                "https://pages.cs.wisc.edu/~yongjaelee/"
            ],
            [
                "Yuheng Li",
                "https://yuheng-li.github.io/"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/haotian-liu/LLaVA",
                20560
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2304.08485"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2310.03744"
            ],
            [
                "project",
                "https://llava-vl.github.io/"
            ],
            [
                "demo",
                "https://llava.hliu.cc/"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2306.00890"
            ],
            [
                "git",
                "https://github.com/ggerganov/llama.cpp/pull/3436"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2309.09958"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2306.14895"
            ],
            [
                "yt",
                "https://youtu.be/mkI7EPD1vp8"
            ],
            [
                "git",
                "https://github.com/microsoft/LLaVA-Med"
            ],
            [
                "hf",
                "https://huggingface.co/datasets/liuhaotian/LLaVA-Pretrain"
            ],
            [
                "git",
                "https://github.com/lm-sys/FastChat"
            ],
            [
                "git",
                "https://github.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once"
            ],
            [
                "git",
                "https://github.com/Luodian/Otter"
            ],
            [
                "git",
                "https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM"
            ],
            [
                "yt",
                "https://youtu.be/kx1VpI6JzsY"
            ],
            [
                "yt",
                "https://youtu.be/RxBSmbdJ1I8"
            ],
            [
                "yt",
                "https://youtu.be/mdYycY4lsuE"
            ],
            [
                "yt",
                "https://youtu.be/t7I46dxfmWs"
            ],
            [
                "yt",
                "https://youtu.be/KRAQkJC-XJU"
            ],
            [
                "medium",
                "https://xthemadgenius.medium.com/how-to-use-llava-large-language-and-vision-assistant-732c666b5ed0"
            ],
            [
                "hf",
                "https://huggingface.co/liuhaotian/LLaVA-Pretrained-Projectors"
            ]
        ],
        "colab": "https://colab.research.google.com/github/camenduru/LLaVA-colab/blob/main/LLaVA_13b_4bit_vanilla_colab.ipynb",
        "update": 1703273204.0
    },
    {
        "name": "Show-1",
        "description": "Hybrid model, dubbed as Show-1, which marries pixel-based and latent-based VDMs for text-to-video generation",
        "author": [
            [
                "David Junhao Zhang",
                "https://junhaozhang98.github.io/"
            ],
            [
                "Jay Zhangjie Wu",
                "https://zhangjiewu.github.io/"
            ],
            [
                "Jiawei Liu",
                "https://jia-wei-liu.github.io/"
            ],
            [
                "Rui Zhao",
                "https://ruizhaocv.github.io/"
            ],
            [
                "Lingmin Ran",
                "https://siacorplab.nus.edu.sg/people/ran-lingmin/"
            ],
            [
                "Yuchao Gu",
                "https://ycgu.site/"
            ],
            [
                "Difei Gao",
                "https://scholar.google.com/citations?user=No9OsocAAAAJ"
            ],
            [
                "Mike Zheng Shou",
                "https://sites.google.com/view/showlab/home"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/showlab/Show-1",
                1107
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2309.15818"
            ],
            [
                "hf",
                "https://huggingface.co/showlab/show-1-base"
            ],
            [
                "hf",
                "https://huggingface.co/showlab/show-1-interpolation"
            ],
            [
                "hf",
                "https://huggingface.co/showlab/show-1-sr1"
            ],
            [
                "hf",
                "https://huggingface.co/showlab/show-1-sr2"
            ],
            [
                "hf",
                "https://huggingface.co/damo-vilab/modelscope-damo-text-to-video-synthesis"
            ],
            [
                "hf",
                "https://huggingface.co/cerspense/zeroscope_v2_576w"
            ],
            [
                "project",
                "https://showlab.github.io/Show-1/"
            ]
        ],
        "colab": "https://colab.research.google.com/github/camenduru/Show-1-colab/blob/main/Show_1_steps_colab.ipynb",
        "update": 1697397354.0
    },
    {
        "name": "DiffBIR",
        "description": "Towards Blind Image Restoration with Generative Diffusion Prior",
        "author": [
            [
                "Xinqi Lin",
                "https://github.com/0x3f3f3f3fun"
            ],
            [
                "Jingwen He",
                "https://github.com/hejingwenhejingwen"
            ],
            [
                "Ziyan Chen",
                "https://github.com/ziyannchen"
            ],
            [
                "Zhaoyang Lyu",
                "https://zhaoyanglyu.github.io/"
            ],
            [
                "Ben Fei",
                "https://scholar.google.com/citations?user=skQROj8AAAAJ"
            ],
            [
                "Bo Dai",
                "http://daibo.info/"
            ],
            [
                "Wanli Ouyang",
                "https://wlouyang.github.io/"
            ],
            [
                "Yu Qiao",
                "https://mmlab.siat.ac.cn/yuqiao"
            ],
            [
                "Chao Dong",
                "http://xpixel.group/2010/01/20/chaodong.html"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/XPixelGroup/DiffBIR",
                3418
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2308.15070"
            ],
            [
                "project",
                "https://0x3f3f3f3fun.github.io/projects/diffbir/"
            ],
            [
                "git",
                "https://github.com/albarji/mixture-of-diffusers"
            ],
            [
                "hf",
                "https://huggingface.co/stabilityai/stable-diffusion-2-1-base"
            ],
            [
                "yt",
                "https://youtu.be/rGnrpxWjBOg"
            ],
            [
                "yt",
                "https://youtu.be/MIRiJGuGqsg"
            ]
        ],
        "colab": "https://colab.research.google.com/github/camenduru/DiffBIR-colab/blob/main/DiffBIR_colab.ipynb",
        "update": 1702902393.0
    },
    {
        "name": "SadTalker",
        "description": "Generates 3D motion coefficients of the 3DMM from audio and implicitly modulates a novel 3D-aware face render for talking head generation",
        "author": [
            [
                "Wenxuan Zhang",
                "https://github.com/Winfredy"
            ],
            [
                "Xiaodong Cun",
                "https://vinthony.github.io/academic/"
            ],
            [
                "Xuan Wang",
                "https://xuanwangvc.github.io/"
            ],
            [
                "Yong Zhang",
                "https://yzhang2016.github.io/"
            ],
            [
                "Xi Shen",
                "https://xishen0220.github.io/"
            ],
            [
                "Yu Guo",
                "https://yuguo-xjtu.github.io/"
            ],
            [
                "Ying Shan",
                "https://scholar.google.com/citations?user=4oXBp9UAAAAJ"
            ],
            [
                "Fei Wang",
                "http://gr.xjtu.edu.cn/zh/web/feynmanw"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/OpenTalker/SadTalker",
                12048
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2211.12194"
            ],
            [
                "project",
                "https://sadtalker.github.io/"
            ],
            [
                "discord",
                "https://discord.gg/rrayYqZ4tf"
            ],
            [
                "git",
                "https://github.com/OpenTalker/DPE"
            ],
            [
                "git",
                "https://github.com/zhanglonghao1992/One-Shot_Free-View_Neural_Talking_Head_Synthesis"
            ],
            [
                "git",
                "https://github.com/RenYurui/PIRender"
            ],
            [
                "git",
                "https://github.com/microsoft/Deep3DFaceReconstruction"
            ],
            [
                "git",
                "https://github.com/xinntao/facexlib"
            ],
            [
                "git",
                "https://github.com/Zz-ww/SadTalker-Video-Lip-Sync"
            ],
            [
                "git",
                "https://github.com/FeiiYin/SPI"
            ],
            [
                "yt",
                "https://youtu.be/AoIzJWnQw1M"
            ],
            [
                "yt",
                "https://youtu.be/fDgQcDL-qOc"
            ],
            [
                "yt",
                "https://youtu.be/BkSnM9cxkcM"
            ],
            [
                "yt",
                "https://youtu.be/7u0FYVPQ5rc"
            ],
            [
                "doi",
                "https://doi.org/10.1109/CVPR52729.2023.00836",
                87
            ]
        ],
        "colab": "https://colab.research.google.com/github/OpenTalker/SadTalker/blob/main/quick_demo.ipynb",
        "update": 1696954221.0
    },
    {
        "name": "VideoReTalking",
        "description": "System to edit the faces of a real-world talking head video according to input audio, producing a high-quality and lip-syncing output video even with a different emotion",
        "author": [
            [
                "Kun Cheng",
                "https://github.com/kunncheng"
            ],
            [
                "Xiaodong Cun",
                "https://vinthony.github.io/"
            ],
            [
                "Yong Zhang",
                "https://yzhang2016.github.io/"
            ],
            [
                "Menghan Xia",
                "https://menghanxia.github.io/"
            ],
            [
                "Fei Yin",
                "https://feiiyin.github.io/"
            ],
            [
                "Mingrui Zhu",
                "https://web.xidian.edu.cn/mrzhu/en/index.html"
            ],
            [
                "Xuan Wang",
                "https://xuanwangvc.github.io/"
            ],
            [
                "Jue Wang",
                "https://juewang725.github.io/"
            ],
            [
                "Nannan Wang",
                "https://web.xidian.edu.cn/nnwang/en/index.html"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/OpenTalker/video-retalking",
                6703
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2211.14758"
            ],
            [
                "doi",
                "https://doi.org/10.1145/3550469.3555399",
                37
            ],
            [
                "git",
                "https://github.com/donydchen/ganimation_replicate"
            ],
            [
                "git",
                "https://github.com/RenYurui/PIRender"
            ],
            [
                "git",
                "https://github.com/OpenTalker/StyleHEAT"
            ],
            [
                "git",
                "https://github.com/FeiiYin/SPI"
            ],
            [
                "project",
                "https://opentalker.github.io/video-retalking/"
            ],
            [
                "yt",
                "https://youtu.be/pttsTrQ-fko"
            ],
            [
                "yt",
                "https://youtu.be/2Lkw8AmmRn0"
            ],
            [
                "yt",
                "https://youtu.be/RJ8YK_K4Ne0"
            ],
            [
                "reddit",
                "https://www.reddit.com/r/StableDiffusion/comments/178krha/videoretalking/"
            ],
            [
                "medium",
                "https://xthemadgenius.medium.com/making-videos-talk-right-syncing-lips-with-sound-using-videoretalking-611428084bbc"
            ]
        ],
        "colab": "https://colab.research.google.com/github/vinthony/video-retalking/blob/main/quick_demo.ipynb",
        "update": 1679203156.0
    },
    {
        "name": "T2M-GPT",
        "description": "Conditional generative framework based on Vector Quantised-Variational AutoEncoder and Generative Pre-trained Transformer for human motion generation from textural descriptions",
        "author": [
            [
                "Jianrong Zhang",
                "https://github.com/Jiro-zhang"
            ],
            [
                "Yangsong Zhang",
                "https://github.com/Mael-zys"
            ],
            [
                "Xiaodong Cun",
                "https://vinthony.github.io/academic/"
            ],
            [
                "Shaoli Huang",
                "https://shaoli-huang.github.io/"
            ],
            [
                "Yong Zhang",
                "https://yzhang2016.github.io/"
            ],
            [
                "Hongwei Zhao",
                "https://teachers.jlu.edu.cn/zhaohongwei/en/index.htm"
            ],
            [
                "Hongtao Lu",
                "https://www.cs.sjtu.edu.cn/en/PeopleDetail.aspx?id=156"
            ],
            [
                "Xi Shen",
                "https://xishen0220.github.io/"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/Mael-zys/T2M-GPT",
                609
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2301.06052"
            ],
            [
                "project",
                "https://mael-zys.github.io/T2M-GPT/"
            ],
            [
                "hf",
                "https://huggingface.co/vumichien/T2M-GPT"
            ],
            [
                "hf",
                "https://huggingface.co/spaces/vumichien/generate_human_motion"
            ],
            [
                "git",
                "https://github.com/EricGuo5513/HumanML3D"
            ],
            [
                "git",
                "https://github.com/EricGuo5513/text-to-motion"
            ],
            [
                "git",
                "https://github.com/GuyTevet/motion-diffusion-model"
            ],
            [
                "git",
                "https://github.com/EricGuo5513/TM2T"
            ],
            [
                "medium",
                "https://medium.com/@kaveh.kamali/t2m-gpt-pioneering-human-motion-generation-from-textual-descriptions-48dc62b5cd7a"
            ],
            [
                "yt",
                "https://youtu.be/09K2cx9P0_0"
            ],
            [
                "doi",
                "https://doi.org/10.1109/CVPR52729.2023.01415",
                68
            ]
        ],
        "colab": "https://colab.research.google.com/drive/1Vy69w2q2d-Hg19F-KibqG0FRdpSj3L4O",
        "update": 1732435708.161
    },
    {
        "name": "Würstchen",
        "description": "Architecture for text-to-image synthesis that combines competitive performance with unprecedented cost-effectiveness for large-scale text-to-image diffusion models",
        "author": [
            [
                "Pablo Pernias",
                "https://github.com/pabloppp"
            ],
            [
                "Dominic Rampas",
                "https://github.com/dome272"
            ],
            [
                "Mats Richter",
                "https://scholar.google.com/citations?user=xtlV5SAAAAAJ"
            ],
            [
                "Christopher Pal",
                "https://www.polymtl.ca/expertises/pal-christopher-j"
            ],
            [
                "Marc Aubreville",
                "https://lme.tf.fau.de/person/aubreville/"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/dome272/wuerstchen",
                531
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2306.00637"
            ],
            [
                "hf",
                "https://huggingface.co/blog/wuerstchen"
            ],
            [
                "reddit",
                "https://www.reddit.com/r/StableDiffusion/comments/16hsklt/w%C3%BCrstchen_is_here_a_game_changing_fastest/"
            ],
            [
                "yt",
                "https://youtu.be/ogJsCPqgFMk"
            ]
        ],
        "colab": "https://colab.research.google.com/github/dome272/Wuerstchen/blob/main/w%C3%BCrstchen-stage-C.ipynb",
        "update": 1712395198.0
    },
    {
        "name": "DeepLabCut",
        "description": "Efficient method for markerless pose estimation based on transfer learning with deep neural networks that achieves excellent results with minimal training data",
        "author": [
            [
                "Alexander Mathis",
                "https://github.com/AlexEMG"
            ],
            [
                "Pranav Mamidanna",
                "https://pranavm19.github.io/"
            ],
            [
                "Kevin Cury",
                "https://kevincury.com/"
            ],
            [
                "Taiga Abe",
                "https://cellistigs.github.io/"
            ],
            [
                "Venkatesh Murthy",
                "https://github.com/venkateshnmurthy"
            ],
            [
                "Mackenzie Mathis",
                "https://github.com/MMathisLab"
            ],
            [
                "Matthias Bethge",
                "https://bethgelab.org/"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/DeepLabCut/DeepLabCut",
                4713
            ],
            [
                "doi",
                "https://doi.org/10.1038/s41593-018-0209-y",
                3276
            ],
            [
                "website",
                "https://www.deeplabcut.org/"
            ],
            [
                "forum",
                "https://forum.image.sc/tag/deeplabcut"
            ],
            [
                "docker",
                "https://hub.docker.com/r/deeplabcut/deeplabcut"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/1605.03170"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/1804.03142"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/1909.11229"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2009.00564"
            ],
            [
                "git",
                "https://github.com/DeepLabCut/DLCutils"
            ],
            [
                "twitter",
                "https://twitter.com/DeepLabCut"
            ],
            [
                "medium",
                "https://medium.com/@cziscience/how-open-source-software-contributors-are-accelerating-biomedicine-1a5f50f6846a"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/1909.13868"
            ],
            [
                "git",
                "https://github.com/DeepLabCut/DeepLabCut-Workshop-Materials"
            ],
            [
                "doi",
                "https://doi.org/10.1038/s41596-019-0176-0"
            ],
            [
                "doi",
                "https://doi.org/10.1109/WACV48630.2021.00190"
            ],
            [
                "doi",
                "https://doi.org/10.1038/s41592-022-01443-0"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/1909.13868"
            ],
            [
                "yt",
                "https://www.youtube.com/@deeplabcut7702"
            ],
            [
                "yt",
                "https://youtu.be/uWZu3rnj-kQ"
            ],
            [
                "yt",
                "https://youtu.be/Teb5r2TNAYs"
            ]
        ],
        "colab": "https://colab.research.google.com/github/DeepLabCut/DeepLabCut/blob/master/examples/COLAB/COLAB_maDLC_TrainNetwork_VideoAnalysis.ipynb",
        "update": 1717580576.0
    },
    {
        "name": "DeepCache",
        "description": "Training-free paradigm that accelerates diffusion models from the perspective of model architecture",
        "author": [
            [
                "Xinyin Ma",
                "https://horseee.github.io/"
            ],
            [
                "Gongfan Fang",
                "https://fangggf.github.io/"
            ],
            [
                "Xinchao Wang",
                "https://sites.google.com/site/sitexinchaowang/"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/horseee/DeepCache",
                811
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2312.00858"
            ],
            [
                "project",
                "https://horseee.github.io/Diffusion_DeepCache/"
            ],
            [
                "hf",
                "https://huggingface.co/docs/diffusers/v0.24.0/en/api/pipelines/stable_diffusion/text2img#diffusers.StableDiffusionPipeline"
            ],
            [
                "reddit",
                "https://www.reddit.com/r/StableDiffusion/comments/18b40hh/deepcache_accelerating_diffusion_models_for_free/"
            ]
        ],
        "colab": "https://colab.research.google.com/github/camenduru/DeepCache-colab/blob/main/DeepCache_colab.ipynb",
        "update": 1702904632.0
    },
    {
        "name": "PASD",
        "description": "Pixel-aware stable diffusion network to achieve robust Real-ISR as well as personalized stylization",
        "author": [
            [
                "Tao Yang",
                "https://cg.cs.tsinghua.edu.cn/people/~tyang"
            ],
            [
                "Peiran Ren",
                "http://renpr.org/"
            ],
            [
                "Xuansong Xie",
                "https://github.com/xungie"
            ],
            [
                "Lei Zhang",
                "https://www4.comp.polyu.edu.hk/~cslzhang"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/yangxy/PASD",
                903
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2308.14469"
            ],
            [
                "hf",
                "https://huggingface.co/runwayml/stable-diffusion-v1-5"
            ],
            [
                "git",
                "https://github.com/pkuliyi2015/multidiffusion-upscaler-for-automatic1111"
            ],
            [
                "hf",
                "https://huggingface.co/nitrosocke/mo-di-diffusion"
            ],
            [
                "reddit",
                "https://www.reddit.com/r/StableDiffusion/comments/18qxe5q/pixelaware_stable_diffusion_for_realistic_image/"
            ]
        ],
        "colab": "https://colab.research.google.com/drive/1lZ_-rSGcmreLCiRniVT973x6JLjFiC-b",
        "update": 1705088431.844
    },
    {
        "name": "Concept Sliders",
        "description": "Plug-and-play low rank adaptors applied on top of pretrained models",
        "author": [
            [
                "Rohit Gandikota",
                "https://rohitgandikota.github.io/"
            ],
            [
                "Joanna Materzyńska",
                "https://joaanna.github.io/"
            ],
            [
                "Tingrui Zhou",
                "https://www.p1at.dev/"
            ],
            [
                "Antonio Torralba",
                "https://groups.csail.mit.edu/vision/torralbalab/"
            ],
            [
                "David Bau",
                "https://baulab.info/"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/rohitgandikota/sliders",
                977
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2311.12092"
            ],
            [
                "project",
                "https://sliders.baulab.info/"
            ],
            [
                "neurips",
                "https://proceedings.neurips.cc/paper/2020/hash/49856ed476ad01fcff881d57e161d73f-Abstract.html"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2207.12598"
            ],
            [
                "medium",
                "https://medium.com/@furkangozukara/concept-sliders-lora-adaptors-for-precise-control-in-diffusion-models-b7f6b36fabee"
            ],
            [
                "reddit",
                "https://www.reddit.com/r/StableDiffusion/comments/180zon7/concept_sliders_lora_adaptors_for_precise_control/"
            ]
        ],
        "colab": "https://colab.research.google.com/github/rohitgandikota/sliders/blob/main/demo_concept_sliders.ipynb",
        "update": 1700975403.0
    },
    {
        "name": "MagicAnimate",
        "description": "Diffusion-based framework that aims at enhancing temporal consistency, preserving reference image faithfully, and improving animation fidelity",
        "author": [
            [
                "Zhongcong Xu",
                "https://scholar.google.com/citations?user=-4iADzMAAAAJ"
            ],
            [
                "Jianfeng Zhang",
                "http://jeff95.me/"
            ],
            [
                "Jun Hao Liew",
                "https://scholar.google.com/citations?user=8gm-CYYAAAAJ"
            ],
            [
                "Hanshu Yan",
                "https://hanshuyan.github.io/"
            ],
            [
                "Jiawei Liu",
                "https://jia-wei-liu.github.io/"
            ],
            [
                "Chenxu Zhang",
                "https://zhangchenxu528.github.io/"
            ],
            [
                "Jiashi Feng",
                "https://sites.google.com/site/jshfeng/home"
            ],
            [
                "Mike Shou",
                "https://sites.google.com/view/showlab"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/magic-research/magic-animate",
                10511
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2311.16498"
            ],
            [
                "project",
                "https://showlab.github.io/magicanimate/"
            ],
            [
                "hf",
                "https://huggingface.co/zcxu-eric/MagicAnimate"
            ],
            [
                "hf",
                "https://huggingface.co/runwayml/stable-diffusion-v1-5"
            ],
            [
                "hf",
                "https://huggingface.co/stabilityai/sd-vae-ft-mse"
            ],
            [
                "yt",
                "https://youtu.be/td27SyA9M80"
            ],
            [
                "yt",
                "https://youtu.be/1pATjLFvNtY"
            ],
            [
                "yt",
                "https://youtu.be/HeXknItbMM8"
            ],
            [
                "website",
                "https://www.magicanimate.org/"
            ],
            [
                "medium",
                "https://medium.com/@AIWorldBlog/revolutionizing-image-animation-with-magicanimate-technology-78cc94151915"
            ]
        ],
        "colab": "https://colab.research.google.com/github/camenduru/MagicAnimate-colab/blob/main/MagicAnimate_colab.ipynb",
        "update": 1702902441.0
    },
    {
        "name": "f-BRS",
        "description": "Feature backpropagating refinement scheme that solves an optimization problem with respect to auxiliary variables instead of the network inputs, and requires running forward and backward pass just for a small part of a network",
        "author": [
            [
                "Konstantin Sofiiuk",
                "https://github.com/ksofiyuk"
            ],
            [
                "Ilia Petrov",
                "https://virtualhumans.mpi-inf.mpg.de/people/Petrov.html"
            ],
            [
                "Olga Barinova",
                "https://github.com/OlgaBarinova"
            ],
            [
                "Anton Konushin",
                "https://scholar.google.com/citations?user=ZT_k-wMAAAAJ"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/SamsungLabs/fbrs_interactive_segmentation",
                583
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2001.10331"
            ],
            [
                "yt",
                "https://youtu.be/ArcZ5xtyMCk"
            ],
            [
                "git",
                "https://github.com/HRNet/HRNet-Image-Classification"
            ],
            [
                "doi",
                "https://doi.org/10.1109/CVPR42600.2020.00865",
                140
            ],
            [
                "yt",
                "https://youtu.be/xg-5J9gLuXA"
            ]
        ],
        "colab": "https://colab.research.google.com/github/SamsungLabs/fbrs_interactive_segmentation/blob/master/notebooks/colab_test_any_model.ipynb",
        "update": 1611580713.0
    },
    {
        "name": "RITM",
        "description": "Simple feedforward model for click-based interactive segmentation that employs the segmentation masks from previous steps",
        "author": [
            [
                "Konstantin Sofiiuk",
                "https://github.com/ksofiyuk"
            ],
            [
                "Ilia Petrov",
                "https://virtualhumans.mpi-inf.mpg.de/people/Petrov.html"
            ],
            [
                "Anton Konushin",
                "https://scholar.google.com/citations?user=ZT_k-wMAAAAJ"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/SamsungLabs/ritm_interactive_segmentation",
                637
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2102.06583"
            ],
            [
                "doi",
                "https://doi.org/10.1109/ICIP46576.2022.9897365",
                105
            ],
            [
                "pwc",
                "https://paperswithcode.com/sota/interactive-segmentation-on-grabcut?p=reviving-iterative-training-with-mask"
            ],
            [
                "pwc",
                "https://paperswithcode.com/sota/interactive-segmentation-on-berkeley?p=reviving-iterative-training-with-mask"
            ],
            [
                "git",
                "https://github.com/HRNet/HRNet-Image-Classification"
            ]
        ],
        "colab": "https://colab.research.google.com/github/SamsungLabs/ritm_interactive_segmentation/blob/master/notebooks/colab_test_any_model.ipynb",
        "update": 1613235447.0
    },
    {
        "name": "HandRefiner",
        "description": "Refining Malformed Hands in Generated Images by Diffusion-based Conditional Inpainting",
        "author": [
            [
                "Wenquan Lu",
                "https://github.com/wenquanlu"
            ],
            [
                "Yufei Xu",
                "https://scholar.google.com/citations?user=hlYWxX8AAAAJ"
            ],
            [
                "Jing Zhang",
                "https://scholar.google.com/citations?user=9jH5v74AAAAJ"
            ],
            [
                "Chaoyue Wang",
                "https://wang-chaoyue.github.io/"
            ],
            [
                "Dacheng Tao",
                "https://scholar.google.com/citations?user=RwlJNLcAAAAJ"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/wenquanlu/HandRefiner",
                761
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2311.17957"
            ],
            [
                "git",
                "https://github.com/Fannovel16/comfyui_controlnet_aux"
            ],
            [
                "git",
                "https://github.com/Mikubill/sd-webui-controlnet"
            ],
            [
                "git",
                "https://github.com/microsoft/MeshGraphormer"
            ],
            [
                "yt",
                "https://youtu.be/Tt-Fyn1RA6c"
            ],
            [
                "reddit",
                "https://www.reddit.com/r/StableDiffusion/comments/1881z4v/handrefiner_refining_malformed_hands_in_generated/"
            ]
        ],
        "colab": "https://colab.research.google.com/github/camenduru/HandRefiner-colab/blob/main/HandRefiner_colab.ipynb",
        "update": 1704752195.0
    },
    {
        "name": "DDColor",
        "description": "End-to-end method with dual decoders for image colorization",
        "author": [
            [
                "Xiaoyang Kang",
                "https://piddnad.github.io/xiaoyangkang"
            ],
            [
                "Tao Yang",
                "https://cg.cs.tsinghua.edu.cn/people/~tyang/"
            ],
            [
                "Wenqi Ouyang",
                "https://vicky0522.github.io/Wenqi-Ouyang/"
            ],
            [
                "Peiran Ren",
                "https://scholar.google.com/citations?user=x5dEuxsAAAAJ"
            ],
            [
                "Lingzhi Li",
                "https://lingzhili.com/"
            ],
            [
                "Xuansong Xie",
                "https://github.com/xungie"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/piddnad/DDColor",
                1151
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2212.11613"
            ],
            [
                "git",
                "https://github.com/jixiaozhong/ColorFormer"
            ],
            [
                "git",
                "https://github.com/KIMGEONUNG/BigColor"
            ]
        ],
        "colab": "https://colab.research.google.com/github/camenduru/DDColor-colab/blob/main/DDColor_colab.ipynb",
        "update": 1705313888.0
    },
    {
        "name": "Background Matting V2",
        "description": "Real-time, high-resolution background replacement technique which operates at 30fps in 4K resolution, and 60fps for HD on a modern GPU",
        "author": [
            [
                "Shanchuan Lin",
                "https://github.com/PeterL1n"
            ],
            [
                "Andrey Ryabtsev",
                "https://github.com/andreyryabtsev"
            ],
            [
                "Soumyadip Sengupta",
                "https://github.com/senguptaumd"
            ],
            [
                "Brian Curless",
                "https://homes.cs.washington.edu/~curless/"
            ],
            [
                "Steve Seitz",
                "https://www.smseitz.com/"
            ],
            [
                "Ira Kemelmacher-Shlizerman",
                "https://www.irakemelmacher.com/"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/PeterL1n/BackgroundMattingV2",
                6879
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2012.07810"
            ],
            [
                "doi",
                "https://doi.org/10.1109/CVPR46437.2021.00865",
                144
            ],
            [
                "project",
                "https://grail.cs.washington.edu/projects/background-matting-v2/"
            ],
            [
                "yt",
                "https://youtu.be/oMfPTeYDF9g"
            ],
            [
                "git",
                "https://github.com/senguptaumd/Background-Matting"
            ],
            [
                "git",
                "https://github.com/andreyryabtsev/BGMv2-webcam-plugin-linux"
            ],
            [
                "yt",
                "https://youtu.be/b7ps21MVyTA"
            ]
        ],
        "colab": "https://colab.research.google.com/drive/1cTxFq1YuoJ5QPqaTcnskwlHDolnjBkB9",
        "update": 1703199600.0
    },
    {
        "name": "RVM",
        "description": "Robust, real-time, high-resolution human video matting method that achieves new state-of-the-art performance",
        "author": [
            [
                "Shanchuan Lin",
                "https://github.com/PeterL1n"
            ],
            [
                "Linjie Yang",
                "https://sites.google.com/site/linjieyang89"
            ],
            [
                "Imran Saleemi",
                "https://github.com/imran-saleemi"
            ],
            [
                "Soumyadip Sengupta",
                "https://github.com/senguptaumd"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/PeterL1n/RobustVideoMatting",
                8641
            ],
            [
                "project",
                "https://peterl1n.github.io/RobustVideoMatting"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2108.11515"
            ],
            [
                "yt",
                "https://youtu.be/Jvzltozpbpk"
            ],
            [
                "yt",
                "https://youtu.be/Ay-mGCEYEzM"
            ],
            [
                "doi",
                "https://doi.org/10.1109/WACV51458.2022.00319",
                88
            ],
            [
                "yt",
                "https://youtu.be/VL-0K6HjhvQ"
            ],
            [
                "yt",
                "https://youtu.be/Jhuf6M_VrBI"
            ],
            [
                "yt",
                "https://youtu.be/_oN9yyRi3HY"
            ],
            [
                "reddit",
                "https://www.reddit.com/r/MachineLearning/comments/pdbpmg/r_robust_highresolution_video_matting_with/"
            ]
        ],
        "colab": "https://colab.research.google.com/drive/10z-pNKRnVNsp0Lq9tH1J_XPZ7CBC_uHm",
        "update": 1637742544.201
    },
    {
        "name": "CogView",
        "description": "Mastering Text-to-Image Generation via Transformers",
        "author": [
            [
                "Ming Ding",
                "https://scholar.google.com/citations?user=Va50YzkAAAAJ"
            ],
            [
                "Zhuoyi Yang",
                "https://scholar.google.com/citations?user=tgAt-gEAAAAJ"
            ],
            [
                "Wenyi Hong",
                "https://github.com/wenyihong"
            ],
            [
                "Wendi Zheng",
                "https://github.com/minkowski0125"
            ],
            [
                "Chang Zhou",
                "https://scholar.google.com/citations?user=QeSoG3sAAAAJ"
            ],
            [
                "Junyang Lin",
                "https://justinlin610.github.io/"
            ],
            [
                "Xu Zou",
                "http://xuzou.cn/"
            ],
            [
                "Zhou Shao",
                "https://www.researchgate.net/profile/Shao_Zhou4"
            ],
            [
                "Hongxia Yang",
                "https://sites.google.com/site/hystatistics/home"
            ],
            [
                "Jie Tang",
                "https://keg.cs.tsinghua.edu.cn/jietang/"
            ]
        ],
        "links": [
            [
                "neurips",
                "https://proceedings.neurips.cc/paper/2021/hash/a4d92e2cd541fca87e4620aba658316d-Abstract.html"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2105.13290"
            ],
            [
                "git",
                "https://github.com/THUDM/CogView",
                1728
            ],
            [
                "demo",
                "https://thudm.github.io/CogView/index.html"
            ],
            [
                "git",
                "https://github.com/NVIDIA/apex"
            ],
            [
                "git",
                "https://github.com/Sleepychord/cogdata"
            ],
            [
                "yt",
                "https://youtu.be/Cw1r8ACIj8U"
            ],
            [
                "medium",
                "https://towardsdatascience.com/cogview-image-generation-and-language-modelling-at-scale-8d358a0686d2"
            ],
            [
                "reddit",
                "https://www.reddit.com/r/MachineLearning/comments/nmxsd8/r_cogview_mastering_texttoimage_generation_via/"
            ]
        ],
        "colab": "https://colab.research.google.com/drive/1Bi2TnSUp2vNiSUhamsNuC4HqkZ2J4WwZ",
        "update": 1624289917.456
    },
    {
        "name": "AQLM",
        "description": "Extreme Compression of Large Language Models via Additive Quantization",
        "author": [
            [
                "Vage Egiazarian",
                "https://github.com/Vahe1994"
            ],
            [
                "Andrei Panferov",
                "https://blog.panferov.org/"
            ],
            [
                "Denis Kuznedelev",
                "https://github.com/Godofnothing"
            ],
            [
                "Elias Frantar",
                "https://efrantar.github.io/"
            ],
            [
                "Artem Babenko",
                "https://scholar.google.com/citations?user=2Kv3JP0AAAAJ"
            ],
            [
                "Dan Alistarh",
                "https://github.com/dalistarh"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/Vahe1994/AQLM",
                1177
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2401.06118"
            ],
            [
                "hf",
                "https://huggingface.co/docs/datasets/main/en/cache#cache-directory"
            ],
            [
                "hf",
                "https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T-Sample"
            ],
            [
                "hf",
                "https://huggingface.co/datasets/Vahe1994/AQLM"
            ],
            [
                "reddit",
                "https://www.reddit.com/r/LearningMachines/comments/1atvrnl/240106118_extreme_compression_of_large_language/"
            ],
            [
                "yt",
                "https://youtu.be/Qx8PNk4OkUA"
            ],
            [
                "yt",
                "https://youtu.be/hAHBKAXO-88"
            ]
        ],
        "colab": "https://colab.research.google.com/github/Vahe1994/AQLM/blob/main/notebooks/colab_example.ipynb",
        "update": 1709894341.0
    },
    {
        "name": "Multi-LoRA Composition",
        "description": "LoRA Switch and LoRA Composite, approaches that aim to surpass traditional techniques in terms of accuracy and image quality, especially in complex compositions",
        "author": [
            [
                "Ming Zhong",
                "https://maszhongming.github.io/"
            ],
            [
                "Yelong Shen",
                "https://scholar.google.com/citations?user=S6OFEFEAAAAJ"
            ],
            [
                "Shuohang Wang",
                "https://www.microsoft.com/en-us/research/people/shuowa/"
            ],
            [
                "Yadong Lu",
                "https://adamlu123.github.io/"
            ],
            [
                "Yizhu Jiao",
                "https://yzjiao.github.io/"
            ],
            [
                "Siru Ouyang",
                "https://ozyyshr.github.io/"
            ],
            [
                "Donghan Yu",
                "https://plusross.github.io/"
            ],
            [
                "Jiawei Han",
                "https://hanj.cs.illinois.edu/"
            ],
            [
                "Weizhu Chen",
                "https://www.microsoft.com/en-us/research/people/wzchen/"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/maszhongming/Multi-LoRA-Composition",
                450
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2402.16843"
            ],
            [
                "website",
                "https://maszhongming.github.io/Multi-LoRA-Composition/"
            ],
            [
                "twitter",
                "https://x.com/MingZhong_/status/1762347881812443575?s=20"
            ],
            [
                "medium",
                "https://medium.com/@letscodeai/multi-lora-composition-for-image-generation-f2706528c590"
            ],
            [
                "reddit",
                "https://www.reddit.com/r/ninjasaid13/comments/1b13q8s/multilora_composition_for_image_generation/"
            ]
        ],
        "colab": "https://colab.research.google.com/drive/1eSTj6qGOtSY5NaazwwN3meXOzEZxgaZq",
        "update": 1709477822.515
    },
    {
        "name": "VIBE",
        "description": "Video Inference for Body Pose and Shape Estimation, which makes use of an existing large-scale motion capture dataset together with unpaired, in-the-wild, 2D keypoint annotations",
        "author": [
            [
                "Muhammed Kocabas",
                "https://ps.is.mpg.de/person/mkocabas"
            ],
            [
                "Nikos Athanasiou",
                "https://github.com/athn-nik"
            ],
            [
                "Michael Black",
                "https://ps.is.mpg.de/person/black"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/mkocabas/VIBE",
                2906
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/1912.05656"
            ],
            [
                "pwc",
                "https://paperswithcode.com/sota/3d-human-pose-estimation-on-3dpw?p=vibe-video-inference-for-human-body-pose-and"
            ],
            [
                "yt",
                "https://youtu.be/3qhs5IRJ1LI"
            ],
            [
                "yt",
                "https://youtu.be/w1biKeiQThY"
            ],
            [
                "git",
                "https://github.com/carlosedubarreto/vibe_win_install"
            ],
            [
                "git",
                "https://github.com/vchoutas/smplx"
            ],
            [
                "git",
                "https://github.com/akanazawa/human_dynamics"
            ],
            [
                "git",
                "https://github.com/MandyMo/pytorch_HMR"
            ],
            [
                "git",
                "https://github.com/soulslicer/STAF/tree/staf"
            ],
            [
                "doi",
                "https://doi.org/10.1109/CVPR42600.2020.00530",
                636
            ],
            [
                "yt",
                "https://youtu.be/rIr-nX63dUA"
            ],
            [
                "yt",
                "https://youtu.be/fW0sIZfQcIs"
            ],
            [
                "yt",
                "https://youtu.be/8Qt0wA16kTo"
            ],
            [
                "yt",
                "https://youtu.be/xyo5gl5GLEI"
            ],
            [
                "yt",
                "https://youtu.be/XNzgUhxKC38"
            ],
            [
                "yt",
                "https://youtu.be/hErK0MamTY4"
            ],
            [
                "yt",
                "https://youtu.be/Gfmm8uMfMq0"
            ]
        ],
        "colab": "https://colab.research.google.com/drive/1dFfwxZ52MN86FA6uFNypMEdFShd2euQA",
        "update": 1608710427.317
    },
    {
        "name": "PixArt-Σ",
        "description": "Weak-to-Strong Training of Diffusion Transformer for 4K Text-to-Image Generation",
        "author": [
            [
                "Junsong Chen",
                "https://lawrence-cj.github.io/"
            ],
            [
                "Chongjian Ge",
                "https://chongjiange.github.io/"
            ],
            [
                "Enze Xie",
                "https://xieenze.github.io/"
            ],
            [
                "Yue Wu",
                "https://yuewuhkust.github.io/"
            ],
            [
                "Lewei Yao",
                "https://scholar.google.com/citations?user=hqDyTg8AAAAJ"
            ],
            [
                "Xiaozhe Ren",
                "https://scholar.google.com/citations?user=3t2j87YAAAAJ"
            ],
            [
                "Zhongdao Wang",
                "https://zhongdao.github.io/"
            ],
            [
                "Ping Luo",
                "http://luoping.me/"
            ],
            [
                "Huchuan Lu",
                "https://scholar.google.com/citations?user=D3nE0agAAAAJ"
            ],
            [
                "Zhenguo Li",
                "https://scholar.google.com/citations?user=XboZC1AAAAAJ"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/PixArt-alpha/PixArt-sigma",
                1701
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2403.04692"
            ],
            [
                "discord",
                "https://discord.gg/rde6eaE5Ta"
            ],
            [
                "project",
                "https://pixart-alpha.github.io/PixArt-sigma-project/"
            ],
            [
                "reddit",
                "https://www.reddit.com/r/PixArtSigma/"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2310.00426"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2401.05252"
            ],
            [
                "hf",
                "https://huggingface.co/spaces/PixArt-alpha/PixArt-alpha"
            ],
            [
                "hf",
                "https://huggingface.co/spaces/PixArt-alpha/PixArt-LCM"
            ]
        ],
        "colab": "https://colab.research.google.com/drive/1jZ5UZXk7tcpTfVwnX33dDuefNMcnW9ME",
        "update": 1699337689.896
    },
    {
        "name": "AlphaPose",
        "description": "Whole-Body Regional Multi-Person Pose Estimation and Tracking in Real-Time",
        "author": [
            [
                "Hao-Shu Fang",
                "https://fang-haoshu.github.io/"
            ],
            [
                "Jiefeng Li",
                "https://jeffli.site/"
            ],
            [
                "Hongyang Tang",
                "https://github.com/tang-hy"
            ],
            [
                "Chao Xu",
                "https://www.isdas.cn/"
            ],
            [
                "Haoyi Zhu",
                "https://www.haoyizhu.site/"
            ],
            [
                "Yuliang Xiu",
                "https://xiuyuliang.cn/"
            ],
            [
                "Yong-Lu Li",
                "https://dirtyharrylyl.github.io/"
            ],
            [
                "Cewu Lu",
                "https://scholar.google.com/citations?user=QZVQEWAAAAAJ"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/MVIG-SJTU/AlphaPose",
                8068
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2211.03375"
            ],
            [
                "project",
                "https://www.mvig.org/research/alphapose.html"
            ],
            [
                "git",
                "https://github.com/tycoer/AlphaPose_jittor"
            ],
            [
                "git",
                "https://github.com/Fang-Haoshu/Halpe-FullBody"
            ],
            [
                "yt",
                "https://youtu.be/uze6chg-YeU"
            ],
            [
                "yt",
                "https://youtu.be/Z2WPd59pRi8"
            ],
            [
                "yt",
                "https://youtu.be/qW4lb9tnA3I"
            ],
            [
                "yt",
                "https://youtu.be/_qtNzylm1XI"
            ],
            [
                "doi",
                "https://doi.org/10.1109/TPAMI.2022.3222784",
                181
            ]
        ],
        "colab": "https://colab.research.google.com/drive/1_3Wxi4H3QGVC28snL3rHIoeMAwI2otMR",
        "update": 1673110466.93
    },
    {
        "name": "SPIN",
        "description": "Learning to Reconstruct 3D Human Pose and Shape via Model-fitting in the Loop",
        "author": [
            [
                "Nikos Kolotouros",
                "https://www.nikoskolot.com/"
            ],
            [
                "Georgios Pavlakos",
                "https://geopavlakos.github.io/"
            ],
            [
                "Michael Black",
                "https://ps.is.mpg.de/~black"
            ],
            [
                "Kostas Daniilidis",
                "https://www.cis.upenn.edu/~kostas/"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/nkolot/SPIN",
                827
            ],
            [
                "project",
                "https://www.nikoskolot.com/projects/spin/"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/1909.12828"
            ],
            [
                "docker",
                "https://hub.docker.com/r/chaneyk/spin"
            ],
            [
                "git",
                "https://github.com/vchoutas/smplify-x"
            ],
            [
                "git",
                "https://github.com/CMU-Perceptual-Computing-Lab/openpose"
            ],
            [
                "doi",
                "https://doi.org/10.1109/ICCV.2019.00234",
                687
            ]
        ],
        "colab": "https://colab.research.google.com/drive/1uH2JtavOtDrFl6RsipyIncCSr19GWW4x",
        "update": 1724228059.29
    },
    {
        "name": "StoryDiffusion",
        "description": "Way of self-attention calculation, termed Consistent Self-Attention, that significantly boosts the consistency between the generated images and augments prevalent pretrained diffusion-based text-to-image models in a zero-shot manner",
        "author": [
            [
                "Yupeng Zhou",
                "https://mmcheng.net/zyp/"
            ],
            [
                "Daquan Zhou",
                "https://github.com/zhoudaquan"
            ],
            [
                "Ming-Ming Cheng",
                "https://mmcheng.net/cmm/"
            ],
            [
                "Jiashi Feng",
                "https://sites.google.com/site/jshfeng/?pli=1"
            ],
            [
                "Qibin Hou",
                "https://houqb.github.io/"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/HVision-NKU/StoryDiffusion",
                6012
            ],
            [
                "project",
                "https://storydiffusion.github.io/"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2405.01434"
            ],
            [
                "yt",
                "https://youtu.be/jZWRENqCl6I"
            ],
            [
                "yt",
                "https://youtu.be/GeNyP4VY9rE"
            ],
            [
                "medium",
                "https://youtu.be/GeNyP4VY9rE?si=qW1jcW_GbKutmKQv"
            ],
            [
                "reddit",
                "https://www.reddit.com/r/StoryDiffusion/"
            ]
        ],
        "colab": "https://colab.research.google.com/github/HVision-NKU/StoryDiffusion/blob/main/Comic_Generation.ipynb",
        "update": 1714858924.0
    },
    {
        "name": "PuLID",
        "description": "Pure and Lightning ID customization, a tuning-free ID customization method for text-to-image generation",
        "author": [
            [
                "Zinan Guo",
                "https://github.com/guozinan126"
            ],
            [
                "Yanze Wu",
                "https://tothebeginning.github.io/"
            ],
            [
                "Zhuowei Chen",
                "https://scholar.google.com/citations?user=ow1jGJkAAAAJ"
            ],
            [
                "Lang Chen",
                "https://scholar.google.com/citations?user=h5xex20AAAAJ"
            ],
            [
                "Qian He",
                "https://scholar.google.com/citations?user=9rWWCgUAAAAJ"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/ToTheBeginning/PuLID",
                2738
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2404.16022"
            ],
            [
                "git",
                "https://github.com/cubiq/PuLID_ComfyUI"
            ],
            [
                "git",
                "https://github.com/ZHO-ZHO-ZHO/ComfyUI-PuLID-ZHO"
            ],
            [
                "git",
                "https://github.com/Mikubill/sd-webui-controlnet/pull/2838"
            ],
            [
                "reddit",
                "https://www.reddit.com/r/comfyui/comments/1cnv269/pulid_pure_and_lightning_id_customization_via/"
            ]
        ],
        "colab": "https://colab.research.google.com/github/camenduru/PuLID-jupyter/blob/main/PuLID_jupyter.ipynb",
        "update": 1731191503.0
    },
    {
        "name": "CAPE",
        "description": "Learning to Dress 3D People in Generative Clothing",
        "author": [
            [
                "Qianli Ma",
                "https://qianlim.github.io/"
            ],
            [
                "Jinlong Yang",
                "https://scholar.google.com/citations?user=HGt39SUAAAAJ"
            ],
            [
                "Anurag Ranjan",
                "https://anuragranj.github.io/"
            ],
            [
                "Sergi Pujades",
                "https://github.com/pujades"
            ],
            [
                "Gerard Pons-Moll",
                "https://virtualhumans.mpi-inf.mpg.de/"
            ],
            [
                "Siyu Tang",
                "https://scholar.google.com/citations?user=BUDh_4wAAAAJ"
            ],
            [
                "Michael Black",
                "https://ps.is.mpg.de/~black"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/qianlim/CAPE",
                314
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/1907.13615"
            ],
            [
                "doi",
                "https://doi.org/10.1109/CVPR42600.2020.00650",
                222
            ],
            [
                "yt",
                "https://youtu.be/e4W-hPFNwDE"
            ],
            [
                "yt",
                "https://youtu.be/NOEA-Rtq6vM"
            ],
            [
                "data",
                "https://cape.is.tue.mpg.de/dataset"
            ],
            [
                "project",
                "https://cape.is.tue.mpg.de/"
            ],
            [
                "git",
                "https://github.com/MPI-IS/mesh"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/1807.10267"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2004.02658"
            ],
            [
                "git",
                "https://github.com/vchoutas/smplx"
            ],
            [
                "git",
                "https://github.com/anuragranj/coma"
            ],
            [
                "medium",
                "https://medium.com/@mahyarfardinfar/learning-to-dress-3d-people-in-generative-clothing-486eb90136ff"
            ]
        ],
        "colab": "https://colab.research.google.com/drive/1DCNo2OyyTNi1xDG-7j32FZQ9sBA6i9Ys",
        "update": 1596645736.414
    },
    {
        "name": "SCALE",
        "description": "Modeling Clothed Humans with a Surface Codec of Articulated Local Elements",
        "author": [
            [
                "Qianli Ma",
                "https://qianlim.github.io/"
            ],
            [
                "Shunsuke Saito",
                "https://shunsukesaito.github.io/"
            ],
            [
                "Jinlong Yang",
                "https://is.mpg.de/~jyang"
            ],
            [
                "Siyu Tang",
                "https://scholar.google.com/citations?user=BUDh_4wAAAAJ"
            ],
            [
                "Michael Black",
                "https://ps.is.mpg.de/~black"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/qianlim/SCALE",
                148
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2104.07660"
            ],
            [
                "yt",
                "https://youtu.be/-EvWqFCUb7U"
            ],
            [
                "project",
                "https://qianlim.github.io/SCALE.html"
            ],
            [
                "poster",
                "https://ps.is.tuebingen.mpg.de/uploads_file/attachment/attachment/650/SCALE_poster_CVPR_final_compressed.pdf"
            ],
            [
                "git",
                "https://github.com/krrish94/chamferdist"
            ],
            [
                "data",
                "https://cape.is.tue.mpg.de/"
            ],
            [
                "doi",
                "https://doi.org/10.1109/CVPR46437.2021.01582",
                62
            ],
            [
                "git",
                "https://github.com/shunsukesaito/SCANimate"
            ],
            [
                "yt",
                "https://youtu.be/v4rWCxJJzhc"
            ]
        ],
        "colab": "https://colab.research.google.com/drive/1lp6r-A-s1kBorIvg6rLD4Ja3o6JOvu3G",
        "update": 1624699970.138
    },
    {
        "name": "FollowYourPose",
        "description": "Two-stage training scheme that can utilize image pose pair and pose-free video datasets and the pre-trained text-to-image model to obtain the pose-controllable character videos",
        "author": [
            [
                "Yue Ma",
                "https://mayuelala.github.io/"
            ],
            [
                "Yingqing He",
                "https://yingqinghe.github.io/"
            ],
            [
                "Xiaodong Cun",
                "https://vinthony.github.io/academic/"
            ],
            [
                "Xintao Wang",
                "https://xinntao.github.io/"
            ],
            [
                "Siran Chen",
                "https://github.com/Sranc3"
            ],
            [
                "Ying Shan",
                "https://scholar.google.com/citations?user=4oXBp9UAAAAJ"
            ],
            [
                "Xiu Li",
                "https://scholar.google.com/citations?user=Xrh1OIUAAAAJ"
            ],
            [
                "Qifeng Chen",
                "https://cqf.io/"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/mayuelala/FollowYourPose",
                1268
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2304.01186"
            ],
            [
                "project",
                "https://follow-your-pose.github.io/"
            ],
            [
                "hf",
                "https://huggingface.co/YueMafighting/FollowYourPose_v1/tree/main"
            ],
            [
                "git",
                "https://github.com/bryandlee/Tune-A-Video"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2112.10752"
            ],
            [
                "hf",
                "https://huggingface.co/CompVis/stable-diffusion-v1-4"
            ],
            [
                "twitter",
                "https://github.com/mayuelala"
            ],
            [
                "doi",
                "https://doi.org/10.1609/aaai.v38i5.28206",
                21
            ],
            [
                "video",
                "https://underline.io/lecture/91712-follow-your-pose-pose-guided-text-to-video-generation-using-pose-free-videos"
            ]
        ],
        "colab": "https://colab.research.google.com/github/mayuelala/FollowYourPose/blob/main/quick_demo.ipynb",
        "update": 1680857650.0
    },
    {
        "name": "ZeST",
        "description": "Method for zero-shot material transfer to an object in the input image given a material exemplar image",
        "author": [
            [
                "Ta-Ying Cheng",
                "https://ttchengab.github.io/"
            ],
            [
                "Prafull Sharma",
                "https://prafullsharma.net/"
            ],
            [
                "Andrew Markham",
                "https://www.cs.ox.ac.uk/people/andrew.markham/"
            ],
            [
                "Niki Trigoni",
                "https://www.cs.ox.ac.uk/people/niki.trigoni/"
            ],
            [
                "Varun Jampani",
                "https://varunjampani.github.io/"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/ttchengab/zest_code",
                370
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2404.06425"
            ],
            [
                "project",
                "https://ttchengab.github.io/zest/"
            ],
            [
                "yt",
                "https://youtu.be/atG1VvgeG_g"
            ],
            [
                "hf",
                "https://huggingface.co/h94/IP-Adapter"
            ],
            [
                "hf",
                "https://github.com/intel-isl/DPT/releases/download/1_0/dpt_hybrid-midas-501f0c75.pt"
            ],
            [
                "medium",
                "https://xthemadgenius.medium.com/zest-unlocks-material-magic-in-single-image-transfers-05f7ff7ee483"
            ],
            [
                "git",
                "https://github.com/kealiu/ComfyUI-ZeroShot-MTrans"
            ],
            [
                "reddit",
                "https://www.reddit.com/r/learnmachinelearning/comments/1c0wpjd/zest_zeroshot_material_transfer_from_a_single/"
            ]
        ],
        "colab": "https://colab.research.google.com/github/camenduru/zest-jupyter/blob/main/zest_jupyter.ipynb",
        "update": 1713271256.0
    },
    {
        "name": "VoiceCraft",
        "description": "token infilling neural codec language model, that achieves state-of-the-art performance on both speech editing and zero-shot text-to-speech on audiobooks, internet videos, and podcasts",
        "author": [
            [
                "Puyuan Peng",
                "https://jasonppy.github.io/"
            ],
            [
                "Po-Yao Huang",
                "https://berniebear.github.io/"
            ],
            [
                "Shang-Wen Li",
                "https://swdanielli.github.io/"
            ],
            [
                "Abdelrahman Mohamed",
                "https://www.cs.toronto.edu/~asamir/"
            ],
            [
                "David Harwath",
                "https://www.cs.utexas.edu/~harwath/"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/jasonppy/VoiceCraft",
                7679
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2403.16973"
            ],
            [
                "hf",
                "https://huggingface.co/pyp1/VoiceCraft"
            ],
            [
                "yt",
                "https://youtu.be/eikybOi8iwU"
            ],
            [
                "git",
                "https://github.com/lifeiteng/vall-e"
            ],
            [
                "yt",
                "https://youtu.be/PJ2qSjycLcw"
            ],
            [
                "yt",
                "https://youtu.be/JxRrHpq-hys"
            ],
            [
                "project",
                "https://jasonppy.github.io/VoiceCraft_web/"
            ],
            [
                "reddit",
                "https://www.reddit.com/r/LocalLLaMA/comments/1bmxfk3/voicecraft_zeroshot_speech_editing_and/"
            ]
        ],
        "colab": "https://colab.research.google.com/github/jasonppy/VoiceCraft/blob/master/voicecraft-gradio-colab.ipynb",
        "update": 1713717489.0
    },
    {
        "name": "InstantMesh",
        "description": "Feed-forward framework for instant 3D mesh generation from a single image, featuring state-of-the-art generation quality and significant training scalability",
        "author": [
            [
                "Jiale Xu",
                "https://github.com/bluestyle97"
            ],
            [
                "Weihao Cheng",
                "https://www.cheng.website/"
            ],
            [
                "Yiming Gao",
                "https://scholar.google.com/citations?user=uRCc-McAAAAJ"
            ],
            [
                "Xintao Wang",
                "https://xinntao.github.io/"
            ],
            [
                "Shenghua Gao",
                "https://scholar.google.com/citations?user=fe-1v0MAAAAJ"
            ],
            [
                "Ying Shan",
                "https://scholar.google.com/citations?user=4oXBp9UAAAAJ"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/TencentARC/InstantMesh",
                3434
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2404.07191"
            ],
            [
                "hf",
                "https://huggingface.co/TencentARC/InstantMesh"
            ],
            [
                "git",
                "https://github.com/danielgatis/rembg"
            ],
            [
                "git",
                "https://github.com/3DTopia/OpenLRM"
            ],
            [
                "git",
                "https://github.com/nv-tlabs/FlexiCubes"
            ],
            [
                "reddit",
                "https://www.reddit.com/r/StableDiffusion/comments/1c5hs3e/instantmesh_efficient_3d_mesh_generation_from_a/"
            ],
            [
                "yt",
                "https://youtu.be/BvngSJOStvQ"
            ]
        ],
        "colab": "https://colab.research.google.com/github/camenduru/InstantMesh-jupyter/blob/main/InstantMesh_jupyter.ipynb",
        "update": 1713261103.0
    },
    {
        "name": "HMR",
        "description": "End-to-end framework for reconstructing a full 3D mesh of a human body from a single RGB image",
        "author": [
            [
                "Angjoo Kanazawa",
                "https://people.eecs.berkeley.edu/~kanazawa/"
            ],
            [
                "Michael Black",
                "https://ps.is.mpg.de/person/black"
            ],
            [
                "David Jacobs",
                "https://www.cs.umd.edu/~djacobs/"
            ],
            [
                "Jitendra Malik",
                "https://people.eecs.berkeley.edu/~malik/"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/akanazawa/hmr",
                1554
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/1712.06584"
            ],
            [
                "project",
                "https://akanazawa.github.io/hmr/"
            ],
            [
                "doi",
                "https://doi.org/10.1109/CVPR.2018.00744",
                1202
            ],
            [
                "yt",
                "https://youtu.be/bmMV9aJKa-c"
            ],
            [
                "git",
                "https://github.com/mattloper/chumpy"
            ],
            [
                "git",
                "https://github.com/CMU-Perceptual-Computing-Lab/openpose"
            ],
            [
                "docker",
                "https://hub.docker.com/r/dawars/hmr/"
            ],
            [
                "git",
                "https://github.com/MandyMo/pytorch_HMR"
            ],
            [
                "git",
                "https://github.com/layumi/hmr"
            ],
            [
                "git",
                "https://github.com/russoale/hmr2.0"
            ]
        ],
        "colab": "https://colab.research.google.com/github/Dene33/video_to_bvh/blob/master/video_to_bvh.ipynb",
        "update": 1552680803.0
    },
    {
        "name": "Zero123++",
        "description": "Image-conditioned diffusion model for generating 3D-consistent multi-view images from a single input view",
        "author": [
            [
                "Ruoxi Shi",
                "https://rshi.top/"
            ],
            [
                "Hansheng Chen",
                "https://lakonik.github.io/"
            ],
            [
                "Zhuoyang Zhang",
                "https://github.com/zhuoyang20"
            ],
            [
                "Minghua Liu",
                "https://cseweb.ucsd.edu/~mil070/"
            ],
            [
                "Chao Xu",
                "https://chaoxu.xyz/"
            ],
            [
                "Xinyue Wei",
                "https://sarahweiii.github.io/"
            ],
            [
                "Linghao Chen",
                "https://ootts.github.io/"
            ],
            [
                "Chong Zeng",
                "https://www.chong-zeng.com/"
            ],
            [
                "Hao Su",
                "https://cseweb.ucsd.edu/~haosu/"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/SUDO-AI-3D/zero123plus",
                1782
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2310.15110"
            ],
            [
                "hf",
                "https://huggingface.co/spaces/sudo-ai/zero123plus-demo-space"
            ],
            [
                "hf",
                "https://huggingface.co/spaces/ysharma/Zero123PlusDemo"
            ],
            [
                "reddit",
                "https://www.reddit.com/r/StableDiffusion/comments/17f4c6p/zero123_a_single_image_to_consistent_multiview/"
            ],
            [
                "medium",
                "https://xthemadgenius.medium.com/zero123-your-guide-to-single-view-to-multi-view-3d-image-transformation-b4346b0e6615"
            ],
            [
                "yt",
                "https://youtu.be/V9AR-81pAgk"
            ],
            [
                "git",
                "https://github.com/One-2-3-45/One-2-3-45"
            ],
            [
                "git",
                "https://github.com/cvlab-columbia/zero123"
            ]
        ],
        "colab": "https://colab.research.google.com/drive/1_5ECnTOosRuAsm2tUp0zvBG0DppL-F3V",
        "update": 1698284045.894
    },
    {
        "name": "FateZero",
        "description": "Zero-shot text-based editing method on real-world videos without per-prompt training or use-specific mask",
        "author": [
            [
                "Chenyang Qi",
                "https://chenyangqiqi.github.io/"
            ],
            [
                "Xiaodong Cun",
                "https://vinthony.github.io/academic/"
            ],
            [
                "Yong Zhang",
                "https://yzhang2016.github.io/"
            ],
            [
                "Chenyang Lei",
                "https://chenyanglei.github.io/"
            ],
            [
                "Xintao Wang",
                "https://xinntao.github.io/"
            ],
            [
                "Ying Shan",
                "https://scholar.google.com/citations?user=4oXBp9UAAAAJ"
            ],
            [
                "Qifeng Chen",
                "https://cqf.io/"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/ChenyangQiQi/FateZero",
                1117
            ],
            [
                "project",
                "https://fate-zero-edit.github.io/"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2303.09535"
            ],
            [
                "hf",
                "https://huggingface.co/spaces/chenyangqi/FateZero"
            ],
            [
                "git",
                "https://github.com/bryandlee/Tune-A-Video"
            ],
            [
                "video",
                "https://hkustconnect-my.sharepoint.com/personal/cqiaa_connect_ust_hk/_layouts/15/stream.aspx?id=%2Fpersonal%2Fcqiaa%5Fconnect%5Fust%5Fhk%2FDocuments%2Fdiffusion%2Fweb%5Fvideo%2Emp4&ga=1&referrer=StreamWebApp%2EWeb&referrerScenario=AddressBarCopied%2Eview%2E9b85614a%2D5af9%2D4485%2Dbcb1%2Db39f90e8d381"
            ],
            [
                "git",
                "https://github.com/google/prompt-to-prompt"
            ],
            [
                "hf",
                "https://huggingface.co/chenyangqi/jeep_tuned_200"
            ],
            [
                "doi",
                "https://doi.org/10.1109/ICCV51070.2023.01460",
                63
            ],
            [
                "reddit",
                "https://www.reddit.com/r/MachineLearning/comments/11uzioo/r_fatezero_fusing_attentions_for_zeroshot/"
            ]
        ],
        "colab": "https://colab.research.google.com/github/ChenyangQiQi/FateZero/blob/main/colab_fatezero.ipynb",
        "update": 1691879916.0
    },
    {
        "name": "LivePortrait",
        "description": "Video-driven portrait animation framework with a focus on better generalization, controllability, and efficiency for practical usage",
        "author": [
            [
                "Jianzhu Guo",
                "https://guojianzhu.com/"
            ],
            [
                "Dingyun Zhang",
                "https://github.com/DingyunZhang"
            ],
            [
                "Xiaoqiang Liu",
                "https://github.com/Liu-lxq"
            ],
            [
                "Zhizhou Zhong",
                "https://scholar.google.com/citations?user=t88nyvsAAAAJ"
            ],
            [
                "Yuan Zhang",
                "https://scholar.google.com/citations?user=_8k1ubAAAAAJ"
            ],
            [
                "Pengfei Wan",
                "https://scholar.google.com/citations?user=P6MraaYAAAAJ"
            ],
            [
                "Di Zhang",
                "https://openreview.net/profile?id=~Di_ZHANG3"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/KwaiVGI/LivePortrait",
                13192
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2407.03168"
            ],
            [
                "project",
                "https://liveportrait.github.io/"
            ],
            [
                "hf",
                "https://huggingface.co/spaces/KwaiVGI/LivePortrait"
            ],
            [
                "git",
                "https://github.com/kijai/ComfyUI-LivePortraitKJ"
            ],
            [
                "git",
                "https://github.com/shadowcz007/comfyui-liveportrait"
            ],
            [
                "yt",
                "https://youtu.be/uyjSTAOY7yI"
            ],
            [
                "yt",
                "https://youtu.be/8-IcDDmiUMM"
            ],
            [
                "yt",
                "https://youtu.be/aFcS31OWMjE"
            ],
            [
                "git",
                "https://github.com/zhanglonghao1992/One-Shot_Free-View_Neural_Talking_Head_Synthesis"
            ],
            [
                "git",
                "https://github.com/NVlabs/SPADE"
            ],
            [
                "git",
                "https://github.com/deepinsight/insightface"
            ],
            [
                "reddit",
                "https://www.reddit.com/r/StableDiffusion/comments/1dvepjx/liveportrait_efficient_portrait_animation_with/"
            ],
            [
                "yt",
                "https://youtu.be/bRHf2oQwgG4"
            ],
            [
                "yt",
                "https://youtu.be/FPtpNrmuwXk"
            ],
            [
                "yt",
                "https://youtu.be/wG7oPp01COg"
            ]
        ],
        "colab": "https://colab.research.google.com/github/camenduru/LivePortrait-jupyter/blob/main/LivePortrait_jupyter.ipynb",
        "update": 1720640954.0
    },
    {
        "name": "ROMP",
        "description": "Monocular, One-stage, Regression of Multiple 3D People",
        "author": [
            [
                "Yu Sun",
                "https://www.yusun.work/"
            ],
            [
                "Qian Bao",
                "https://github.com/for-code0216"
            ],
            [
                "Wu Liu",
                "https://faculty.ustc.edu.cn/liuwu"
            ],
            [
                "Yili Fu",
                "https://ieeexplore.ieee.org/author/37286601800"
            ],
            [
                "Michael Black",
                "https://ps.is.mpg.de/~black"
            ],
            [
                "Tao Mei",
                "https://taomei.me/"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/Arthur151/ROMP",
                1359
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2008.12272"
            ],
            [
                "yt",
                "https://youtu.be/hunBPJxnyBU"
            ],
            [
                "yt",
                "https://youtu.be/Q62fj_6AxRI"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2112.08274"
            ],
            [
                "arxiv",
                "http://arxiv.org/abs/2306.02850"
            ],
            [
                "yt",
                "https://youtu.be/l8aLHDXWQRw"
            ],
            [
                "git",
                "https://github.com/Arthur151/Relative_Human"
            ],
            [
                "git",
                "https://github.com/Arthur151/DynaCam"
            ],
            [
                "git",
                "https://github.com/yanchxx/MoPA"
            ],
            [
                "doi",
                "https://doi.org/10.1109/ICCV48922.2021.01099",
                159
            ]
        ],
        "colab": "https://colab.research.google.com/drive/1oz9E6uIbj4udOPZvA1Zi9pFx0SWH_UXg",
        "update": 1644565085.103
    },
    {
        "name": "RealBasicVSR",
        "description": "Investigating Tradeoffs in Real-World Video Super-Resolution",
        "author": [
            [
                "Kelvin Chan",
                "https://ckkelvinchan.github.io/"
            ],
            [
                "Shangchen Zhou",
                "https://shangchenzhou.com/"
            ],
            [
                "Xiangyu Xu",
                "https://xuxy09.github.io/"
            ],
            [
                "Chen Change Loy",
                "https://www.mmlab-ntu.com/person/ccloy/"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/ckkelvinchan/RealBasicVSR",
                925
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2111.12704"
            ],
            [
                "hf",
                "https://huggingface.co/spaces/akhaliq/RealBasicVSR"
            ],
            [
                "doi",
                "https://doi.org/10.1109/CVPR52688.2022.00587",
                52
            ],
            [
                "reddit",
                "https://www.reddit.com/r/MachineLearning/comments/tc8p70/rp_investigating_tradeoffs_in_realworld_video/"
            ]
        ],
        "colab": "https://colab.research.google.com/drive/1JzWRUR34hpKvtCHm84IGx6nv35LCv20J",
        "update": 1640437209.976
    },
    {
        "name": "BasicVSR++",
        "description": "Redesign BasicVSR by proposing second-order grid propagation and flow-guided deformable alignment",
        "author": [
            [
                "Kelvin Chan",
                "https://ckkelvinchan.github.io/"
            ],
            [
                "Shangchen Zhou",
                "https://shangchenzhou.com/"
            ],
            [
                "Xiangyu Xu",
                "https://xuxy09.github.io/"
            ],
            [
                "Chen Change Loy",
                "https://www.mmlab-ntu.com/person/ccloy/"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/ckkelvinchan/BasicVSR_PlusPlus",
                602
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2104.13371"
            ],
            [
                "project",
                "https://ckkelvinchan.github.io/projects/BasicVSR++/"
            ],
            [
                "git",
                "https://github.com/ckkelvinchan/BasicVSR-IconVSR"
            ],
            [
                "git",
                "https://github.com/ckkelvinchan/offset-fidelity-loss"
            ],
            [
                "doi",
                "https://doi.org/10.1109/CVPR52688.2022.00588",
                258
            ],
            [
                "yt",
                "https://youtu.be/iIDml09CUc4"
            ]
        ],
        "colab": "https://colab.research.google.com/drive/1I0kZMM0DQyb4ueHZw5si8fMnRCJ_eUX3",
        "update": 1650253886.73
    },
    {
        "name": "BiRefNet",
        "description": "Bilateral reference framework for high-resolution dichotomous image segmentation",
        "author": [
            [
                "Peng Zheng",
                "https://zhengpeng7.github.io/about/"
            ],
            [
                "Dehong Gao",
                "https://teacher.nwpu.edu.cn/dehonggao"
            ],
            [
                "Deng-Ping Fan",
                "https://dengpingfan.github.io/"
            ],
            [
                "Li Liu",
                "https://scholar.google.com/citations?user=9cMQrVsAAAAJ"
            ],
            [
                "Jorma Laaksonen",
                "https://scholar.google.com/citations?user=qQP6WXIAAAAJ"
            ],
            [
                "Wanli Ouyang",
                "https://wlouyang.github.io/"
            ],
            [
                "Nicu Sebe",
                "https://disi.unitn.it/~sebe/"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/ZhengPeng7/BiRefNet",
                1439
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2401.03407"
            ],
            [
                "project",
                "https://www.birefnet.top/"
            ],
            [
                "hf",
                "https://huggingface.co/spaces/ZhengPeng7/BiRefNet_demo"
            ],
            [
                "hf",
                "https://huggingface.co/ZhengPeng7/BiRefNet"
            ],
            [
                "pwc",
                "https://paperswithcode.com/sota/dichotomous-image-segmentation-on-dis-te1?p=bilateral-reference-for-high-resolution"
            ],
            [
                "pwc",
                "https://paperswithcode.com/sota/camouflaged-object-segmentation-on-cod?p=bilateral-reference-for-high-resolution"
            ],
            [
                "pwc",
                "https://paperswithcode.com/sota/rgb-salient-object-detection-on-davis-s?p=bilateral-reference-for-high-resolution"
            ],
            [
                "git",
                "https://github.com/Kazuhito00/BiRefNet-ONNX-Sample"
            ],
            [
                "git",
                "https://github.com/ZHO-ZHO-ZHO/ComfyUI-BiRefNet-ZHO"
            ],
            [
                "git",
                "https://github.com/viperyl/ComfyUI-BiRefNet"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2302.14485"
            ],
            [
                "discord",
                "https://discord.gg/d9NN5sgFrq"
            ],
            [
                "doi",
                "https://doi.org/10.26599/AIR.2024.9150038",
                0
            ]
        ],
        "colab": "https://colab.research.google.com/drive/1B6aKZ3ekcvKMkSBn0N5mCASLUYMp0whK",
        "update": 1724420190.23
    },
    {
        "name": "DifFace",
        "description": "Method that is capable of coping with unseen and complex degradations more gracefully without complicated loss designs",
        "author": [
            [
                "Zongsheng Yue",
                "https://zsyoaoa.github.io/"
            ],
            [
                "Chen Change Loy",
                "https://www.mmlab-ntu.com/person/ccloy/"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/zsyOAOA/DifFace",
                654
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2212.06512"
            ],
            [
                "hf",
                "https://huggingface.co/spaces/OAOA/DifFace"
            ],
            [
                "doi",
                "https://doi.org/10.1109/TPAMI.2024.3432651",
                4
            ],
            [
                "git",
                "https://github.com/NVlabs/ffhq-dataset"
            ],
            [
                "git",
                "https://github.com/openai/improved-diffusion"
            ],
            [
                "git",
                "https://github.com/deepcam-cn/yolov5-face"
            ],
            [
                "git",
                "https://github.com/xinntao/facexlib"
            ]
        ],
        "colab": "https://colab.research.google.com/drive/1BNtoPPRuJwNDvqfwDOOmD9XJyF05Zh4m",
        "update": 1728142128.404
    },
    {
        "name": "CodeTalker",
        "description": "Cast speech-driven facial animation as a code query task in a finite proxy space of the learned codebook, which effectively promotes the vividness of the generated motions by reducing the cross-modal mapping uncertainty",
        "author": [
            [
                "Jinbo Xing",
                "https://doubiiu.github.io/"
            ],
            [
                "Menghan Xia",
                "https://menghanxia.github.io/"
            ],
            [
                "Yuechen Zhang",
                "https://julianjuaner.github.io/"
            ],
            [
                "Xiaodong Cun",
                "https://vinthony.github.io/academic/"
            ],
            [
                "Jue Wang",
                "https://juewang725.github.io/"
            ],
            [
                "Tien-Tsin Wong",
                "https://ttwong12.github.io/myself.html"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/Doubiiu/CodeTalker",
                538
            ],
            [
                "project",
                "https://doubiiu.github.io/projects/codetalker/"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2301.02379"
            ],
            [
                "git",
                "https://github.com/MPI-IS/mesh"
            ],
            [
                "git",
                "https://github.com/TimoBolkart/voca/tree/master/template"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2303.09797"
            ],
            [
                "git",
                "https://github.com/EvelynFan/FaceFormer"
            ],
            [
                "git",
                "https://github.com/RenYurui/PIRender"
            ],
            [
                "git",
                "https://github.com/OpenTalker/StyleHEAT"
            ],
            [
                "git",
                "https://github.com/Meta-Portrait/MetaPortrait"
            ],
            [
                "yt",
                "https://youtu.be/J2RngmuYrG4"
            ],
            [
                "doi",
                "https://doi.org/10.1109/CVPR52729.2023.01229",
                59
            ]
        ],
        "colab": "https://colab.research.google.com/github/Doubiiu/CodeTalker/blob/main/demo.ipynb",
        "update": 1686903885.0
    },
    {
        "name": "Score Jacobian Chaining",
        "description": "Apply chain rule on the learned gradients, and back-propagate the score of a diffusion model through the Jacobian of a differentiable renderer, which we instantiate to be a voxel radiance field",
        "author": [
            [
                "Haochen Wang",
                "https://whc.is/"
            ],
            [
                "Xiaodan Du",
                "https://xiaodan.io/"
            ],
            [
                "Jiahao Li",
                "https://jiahao.ai/"
            ],
            [
                "Raymond Yeh",
                "https://raymond-yeh.com/"
            ],
            [
                "Greg Shakhnarovich",
                "https://home.ttic.edu/~gregory/"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/pals-ttic/sjc",
                508
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2212.00774"
            ],
            [
                "hf",
                "https://huggingface.co/spaces/MirageML/sjc"
            ],
            [
                "project",
                "https://pals.ttic.edu/p/score-jacobian-chaining"
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2206.00364"
            ],
            [
                "doi",
                "https://doi.org/10.1109/CVPR52729.2023.01214",
                151
            ],
            [
                "reddit",
                "https://www.reddit.com/r/StableDiffusion/comments/zac8z4/score_jacobian_chaining_lifting_pretrained_2d/"
            ],
            [
                "yt",
                "https://youtu.be/MmDSLc6CjoI"
            ],
            [
                "yt",
                "https://youtu.be/1oeruRLKoiU"
            ]
        ],
        "colab": "https://colab.research.google.com/drive/1zixo66UYGl70VOPy053o7IV_YkQt5lCZ",
        "update": 1670264379.865
    },
    {
        "name": "FBA Matting",
        "description": "Low-cost modification to alpha matting networks to also predict the foreground and background colours",
        "author": [
            [
                "Marco Forte",
                "https://github.com/MarcoForte"
            ],
            [
                "François Pitié",
                "https://francois.pitie.net/"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/MarcoForte/FBA_Matting",
                468
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2003.07711"
            ],
            [
                "hf",
                "https://huggingface.co/spaces/leonelhs/FBA-Matting"
            ],
            [
                "pwc",
                "https://paperswithcode.com/sota/image-matting-on-composition-1k?p=f-b-alpha-matting"
            ],
            [
                "git",
                "https://github.com/MarcoForte/closed-form-matting"
            ]
        ],
        "colab": "https://colab.research.google.com/drive/1Ut2szLBTxPejGHt_GYUkua21yUVWseOE",
        "update": 1584577040.529
    },
    {
        "name": "MAGIC",
        "description": "Training-free framework, iMAge-Guided text generatIon with CLIP, for plugging in visual controls in the generation process and enabling LMs to perform multimodal tasks in a zero-shot manner",
        "author": [
            [
                "Yixuan Su",
                "https://yxuansu.github.io/"
            ],
            [
                "Tian Lan",
                "https://github.com/gmftbyGMFTBY"
            ],
            [
                "Yahui Liu",
                "https://yhlleo.github.io/"
            ],
            [
                "Fangyu Liu",
                "https://fangyuliu.me/about"
            ],
            [
                "Dani Yogatama",
                "https://dyogatama.github.io/"
            ],
            [
                "Yan Wang",
                "https://libertywing.github.io/yanwang.github.io/"
            ],
            [
                "Lingpeng Kong",
                "https://www.cs.cmu.edu/~lingpenk/"
            ],
            [
                "Nigel Collier",
                "https://sites.google.com/site/nhcollier/"
            ]
        ],
        "links": [
            [
                "git",
                "https://github.com/yxuansu/magic",
                255
            ],
            [
                "arxiv",
                "https://arxiv.org/abs/2205.02655"
            ]
        ],
        "colab": "https://colab.research.google.com/drive/1NDVkKpanbsaUwecHoRp_2kIpMztOFW25",
        "update": 1651504432.142
    }
]